<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 19:11:47 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>HB_REPORT(8) Pacemaker documentation HB_REPORT(8)</p>

<p style="margin-top: 1em">NAME <br>
hb_report - create report for CRM based clusters
(Pacemaker)</p>

<p style="margin-top: 1em">SYNOPSIS <br>
hb_report -f {time|&quot;cts:&quot;testnum} [-t time] [-u
user] [-l file] [-n nodes] [-E files] [-p patt] [-L patt]
[-e prog] [-MSDCZAQVsvhd] [dest]</p>

<p style="margin-top: 1em">DESCRIPTION <br>
The hb_report(1) is a utility to collect all information
(logs, configuration files, system information, etc)
relevant to Pacemaker (CRM) over the given period of
time.</p>

<p style="margin-top: 1em">OPTIONS <br>
dest <br>
The report name. It can also contain a path where to put the
report tarball. If left out, the tarball is created in the
current directory named &quot;hb_report-current_date&quot;,
for <br>
instance hb_report-Wed-03-Mar-2010.</p>

<p style="margin-top: 1em">-d <br>
Don&acirc;t create the compressed tar, but leave the result
in a directory.</p>

<p style="margin-top: 1em">-f { time |
&quot;cts:&quot;testnum } <br>
The start time from which to collect logs. The time is in
the format as used by the Date::Parse perl module. For cts
tests, specify the &quot;cts:&quot; string followed by the
test <br>
number. This option is required.</p>

<p style="margin-top: 1em">-t time <br>
The end time to which to collect logs. Defaults to now.</p>

<p style="margin-top: 1em">-n nodes <br>
A list of space separated hostnames (cluster members).
hb_report may try to find out the set of nodes by itself,
but if it runs on the loghost which, as it is usually the
<br>
case, does not belong to the cluster, that may be difficult.
Also, OpenAIS doesn&acirc;t contain a list of nodes and if
Pacemaker is not running, there is no way to find it out
<br>
automatically. This option is cumulative (i.e. use -n
&quot;a b&quot; or -n a -n b).</p>

<p style="margin-top: 1em">-l file <br>
Log file location. If, for whatever reason, hb_report cannot
find the log files, you can specify its absolute path.</p>

<p style="margin-top: 1em">-E files <br>
Extra log files to collect. This option is cumulative. By
default, /var/log/messages are collected along with the
cluster logs.</p>

<p style="margin-top: 1em">-M <br>
Don&acirc;t collect extra log files, but only the file
containing messages from the cluster subsystems.</p>

<p style="margin-top: 1em">-L patt <br>
A list of regular expressions to match in log files for
analysis. This option is additive (default: &quot;CRIT:
ERROR:&quot;).</p>

<p style="margin-top: 1em">-p patt <br>
Additional patterns to match parameter name which contain
sensitive information. This option is additive (default:
&quot;passw.*&quot;).</p>

<p style="margin-top: 1em">-Q <br>
Quick run. Gathering some system information can be
expensive. With this option, such operations are skipped and
thus information collecting sped up. The operations <br>
considered I/O or CPU intensive: verifying installed
packages content, sanitizing files for sensitive
information, and producing dot files from PE inputs.</p>

<p style="margin-top: 1em">-A <br>
This is an OpenAIS cluster. hb_report has some heuristics to
find the cluster stack, but that is not always reliable. By
default, hb_report assumes that it is run on a <br>
Heartbeat cluster.</p>

<p style="margin-top: 1em">-u user <br>
The ssh user. hb_report will try to login to other nodes
without specifying a user, then as &quot;root&quot;, and
finally as &quot;hacluster&quot;. If you have another user
for administration <br>
over ssh, please use this option.</p>

<p style="margin-top: 1em">-X ssh-options <br>
Extra ssh options. These will be added to every ssh
invocation. Alternatively, use $HOME/.ssh/config to setup
desired ssh connection options.</p>

<p style="margin-top: 1em">-S <br>
Single node operation. Run hb_report only on this node and
don&acirc;t try to start slave collectors on other members
of the cluster. Under normal circumstances this option is
not <br>
needed. Use if ssh(1) does not work to other nodes.</p>

<p style="margin-top: 1em">-Z <br>
If the destination directory exist, remove it instead of
exiting (this is default for CTS).</p>

<p style="margin-top: 1em">-V <br>
Print the version including the last repository
changeset.</p>

<p style="margin-top: 1em">-v <br>
Increase verbosity. Normally used to debug unexpected
behaviour.</p>

<p style="margin-top: 1em">-h <br>
Show usage and some examples.</p>

<p style="margin-top: 1em">-D (obsolete) <br>
Don&acirc;t invoke editor to fill the description text
file.</p>

<p style="margin-top: 1em">-e prog (obsolete) <br>
Your favourite text editor. Defaults to $EDITOR, vim, vi,
emacs, or nano, whichever is found first.</p>

<p style="margin-top: 1em">-C (obsolete) <br>
Remove the destination directory once the report has been
put in a tarball.</p>

<p style="margin-top: 1em">EXAMPLES <br>
Last night during the backup there were several warnings
encountered (logserver is the log host):</p>

<p style="margin-top: 1em">logserver# hb_report -f 3:00 -t
4:00 -n &quot;node1 node2&quot; report</p>

<p style="margin-top: 1em">collects everything from all
nodes from 3am to 4am last night. The files are compressed
to a tarball report.tar.bz2.</p>

<p style="margin-top: 1em">Just found a problem during
testing:</p>

<p style="margin-top: 1em"># note the current time <br>
node1# date <br>
Fri Sep 11 18:51:40 CEST 2009 <br>
node1# /etc/init.d/heartbeat start <br>
node1# nasty-command-that-breaks-things <br>
node1# sleep 120 #wait for the cluster to settle <br>
node1# hb_report -f 18:51 hb1</p>

<p style="margin-top: 1em"># if hb_report can&rsquo;t
figure out that this is corosync <br>
node1# hb_report -f 18:51 -A hb1</p>

<p style="margin-top: 1em"># if hb_report can&rsquo;t
figure out the cluster members <br>
node1# hb_report -f 18:51 -n &quot;node1 node2&quot; hb1</p>

<p style="margin-top: 1em">The files are compressed to a
tarball hb1.tar.bz2.</p>

<p style="margin-top: 1em">INTERPRETING RESULTS <br>
The compressed tar archive is the final product of
hb_report. This is one example of its content, for a CTS
test case on a three node OpenAIS cluster:</p>

<p style="margin-top: 1em">$ ls -RF 001-Restart</p>

<p style="margin-top: 1em">001-Restart: <br>
analysis.txt events.txt logd.cf s390vm13/ s390vm16/ <br>
description.txt ha-log.txt openais.conf s390vm14/</p>

<p style="margin-top: 1em">001-Restart/s390vm13: <br>
STOPPED crm_verify.txt hb_uuid.txt openais.conf@ sysinfo.txt
<br>
cib.txt dlm_dump.txt logd.cf@ pengine/ sysstats.txt <br>
cib.xml events.txt messages permissions.txt</p>

<p style="margin-top: 1em">001-Restart/s390vm13/pengine:
<br>
pe-input-738.bz2 pe-input-740.bz2 pe-warn-450.bz2 <br>
pe-input-739.bz2 pe-warn-449.bz2 pe-warn-451.bz2</p>

<p style="margin-top: 1em">001-Restart/s390vm14: <br>
STOPPED crm_verify.txt hb_uuid.txt openais.conf@
sysstats.txt <br>
cib.txt dlm_dump.txt logd.cf@ permissions.txt <br>
cib.xml events.txt messages sysinfo.txt</p>

<p style="margin-top: 1em">001-Restart/s390vm16: <br>
STOPPED crm_verify.txt hb_uuid.txt messages sysinfo.txt <br>
cib.txt dlm_dump.txt hostcache openais.conf@ sysstats.txt
<br>
cib.xml events.txt logd.cf@ permissions.txt</p>

<p style="margin-top: 1em">The top directory contains
information which pertains to the cluster or event as a
whole. Files with exactly the same content on all nodes will
also be at the top, with per-node <br>
links created (as it is in this example the case with
openais.conf and logd.cf).</p>

<p style="margin-top: 1em">The cluster log files are named
ha-log.txt regardless of the actual log file name on the
system. If it is found on the loghost, then it is placed in
the top directory. If not, <br>
the top directory ha-log.txt contains all nodes logs merged
and sorted by time. Files named messages are excerpts of
/var/log/messages from nodes.</p>

<p style="margin-top: 1em">Most files are copied verbatim
or they contain output of a command. For instance, cib.xml
is a copy of the CIB found in
/var/lib/heartbeat/crm/cib.xml. crm_verify.txt is output
<br>
of the crm_verify(8) program.</p>

<p style="margin-top: 1em">Some files are result of a more
involved processing:</p>

<p style="margin-top: 1em">analysis.txt <br>
A set of log messages matching user defined patterns (may be
provided with the -L option).</p>

<p style="margin-top: 1em">events.txt <br>
A set of log messages matching event patterns. It should
provide information about major cluster motions without
unnecessary details. These patterns are devised by the <br>
cluster experts. Currently, the patterns cover membership
and quorum changes, resource starts and stops, fencing
(stonith) actions, and cluster starts and stops. events.txt
<br>
is always generated for each node. In case the central
cluster log was found, also combined for all nodes.</p>

<p style="margin-top: 1em">permissions.txt <br>
One of the more common problem causes are file and directory
permissions. hb_report looks for a set of predefined
directories and checks their permissions. Any issues are
<br>
reported here.</p>

<p style="margin-top: 1em">backtraces.txt <br>
gdb generated backtrace information for cores dumped within
the specified period.</p>

<p style="margin-top: 1em">sysinfo.txt <br>
Various release information about the platform, kernel,
operating system, packages, and anything else deemed to be
relevant. The static part of the system.</p>

<p style="margin-top: 1em">sysstats.txt <br>
Output of various system commands such as ps(1), uptime(1),
netstat(8), and ifconfig(8). The dynamic part of the
system.</p>

<p style="margin-top: 1em">description.txt should contain a
user supplied description of the problem, but since it is
very seldom used, it will be dropped from the future
releases.</p>

<p style="margin-top: 1em">PREREQUISITES <br>
ssh <br>
It is not strictly required, but you won&acirc;t regret
having a password-less ssh. It is not too difficult to setup
and will save you a lot of time. If you can&acirc;t have it,
for <br>
example because your security policy does not allow such a
thing, or you just prefer menial work, then you will have to
resort to the semi-manual semi-automated report <br>
generation. See below for instructions.</p>

<p style="margin-top: 1em">If you need to supply a password
for your passphrase/login, then always use the -u
option.</p>

<p style="margin-top: 1em">For extra ssh(1) options, if
you&acirc;re too lazy to setup $HOME/.ssh/config, use the -X
option. Do not forget to put the options in quotes.</p>

<p style="margin-top: 1em">sudo <br>
If the ssh user (as specified with the -u option) is other
than root, then hb_report uses sudo to collect the
information which is readable only by the root user. In that
<br>
case it is required to setup the sudoers file properly. The
user (or group to which the user belongs) should have the
following line:</p>

<p style="margin-top: 1em">&lt;user&gt; ALL = NOPASSWD:
/usr/sbin/hb_report</p>

<p style="margin-top: 1em">See the sudoers(5) man page for
more details.</p>

<p style="margin-top: 1em">Times <br>
In order to find files and messages in the given period and
to parse the -f and -t options, hb_report uses perl and one
of the Date::Parse or Date::Manip perl modules. Note <br>
that you need only one of these. Furthermore, on nodes which
have no logs and where you don&acirc;t run hb_report
directly, no date parsing is necessary. In other words, if
you run <br>
this on a loghost then you don&acirc;t need these perl
modules on the cluster nodes.</p>

<p style="margin-top: 1em">On rpm based distributions, you
can find Date::Parse in perl-TimeDate and on Debian and its
derivatives in libtimedate-perl.</p>

<p style="margin-top: 1em">Core dumps <br>
To backtrace core dumps gdb is needed and the packages with
the debugging info. The debug info packages may be installed
at the time the report is created. Let&acirc;s hope that
<br>
you will need this really seldom.</p>

<p style="margin-top: 1em">TIMES <br>
Specifying times can at times be a nuisance. That is why we
have chosen to use one of the perl modules&acirc;they do
allow certain freedom when talking dates. You can either
read the <br>
instructions at the Date::Parse examples page. or just rely
on common sense and try stuff like:</p>

<p style="margin-top: 1em">3:00 (today at 3am) <br>
15:00 (today at 3pm) <br>
2007/9/1 2pm (September 1st at 2pm) <br>
Tue Sep 15 20:46:27 CEST 2009 (September 15th etc)</p>

<p style="margin-top: 1em">hb_report will (probably)
complain if it can&acirc;t figure out what do you mean.</p>

<p style="margin-top: 1em">Try to delimit the event as
close as possible in order to reduce the size of the report,
but still leaving a minute or two around for good
measure.</p>

<p style="margin-top: 1em">-f is not optional. And
don&acirc;t forget to quote dates when they contain
spaces.</p>

<p style="margin-top: 1em">SHOULD I SEND ALL THIS TO THE
REST OF INTERNET? <br>
By default, the sensitive data in CIB and PE files is not
mangled by hb_report because that makes PE input files
mostly useless. If you still have no other option but to
send the <br>
report to a public mailing list and do not want the
sensitive data to be included, use the -s option. Without
this option, hb_report will issue a warning if it finds
information <br>
which should not be exposed. By default, parameters matching
passw.* are considered sensitive. Use the -p option to
specify additional regular expressions to match variable
names <br>
which may contain information you don&acirc;t want to leak.
For example:</p>

<p style="margin-top: 1em"># hb_report -f 18:00 -p
&quot;user.*&quot; -p &quot;secret.*&quot;
/var/tmp/report</p>

<p style="margin-top: 1em">Heartbeat&acirc;s ha.cf is
always sanitized. Logs and other files are not filtered.</p>

<p style="margin-top: 1em">LOGS <br>
It may be tricky to find syslog logs. The scheme used is to
log a unique message on all nodes and then look it up in the
usual syslog locations. This procedure is not foolproof,
<br>
in particular if the syslog files are in a non-standard
directory. We look in /var/log /var/logs /var/syslog
/var/adm /var/log/ha /var/log/cluster. In case we
can&acirc;t find the <br>
logs, please supply their location:</p>

<p style="margin-top: 1em"># hb_report -f 5pm -l
/var/log/cluster1/ha-log -S /tmp/report_node1</p>

<p style="margin-top: 1em">If you have different log
locations on different nodes, well, perhaps you&acirc;d like
to make them the same and make life easier for
everybody.</p>

<p style="margin-top: 1em">Files starting with
&quot;ha-&quot; are preferred. In case syslog sends messages
to more than one file, if one of them is named ha-log or
ha-debug those will be favoured over syslog or <br>
messages.</p>

<p style="margin-top: 1em">hb_report supports also archived
logs in case the period specified extends that far in the
past. The archives must reside in the same directory as the
current log and their names <br>
must be prefixed with the name of the current log
(syslog-1.gz or messages-20090105.bz2).</p>

<p style="margin-top: 1em">If there is no separate log for
the cluster, possibly unrelated messages from other programs
are included. We don&acirc;t filter logs, but just pick a
segment for the period you <br>
specified.</p>

<p style="margin-top: 1em">MANUAL REPORT COLLECTION <br>
So, your ssh doesn&acirc;t work. In that case, you will have
to run this procedure on all nodes. Use -S so that hb_report
doesn&acirc;t bother with ssh:</p>

<p style="margin-top: 1em"># hb_report -f 5:20pm -t 5:30pm
-S /tmp/report_node1</p>

<p style="margin-top: 1em">If you also have a log host
which is not in the cluster, then you&acirc;ll have to copy
the log to one of the nodes and tell us where it is:</p>

<p style="margin-top: 1em"># hb_report -f 5:20pm -t 5:30pm
-l /var/tmp/ha-log -S /tmp/report_node1</p>

<p style="margin-top: 1em">OPERATION <br>
hb_report collects files and other information in a fairly
straightforward way. The most complex tasks are discovering
the log file locations (if syslog is used which is the most
<br>
common case) and coordinating the operation on multiple
nodes.</p>

<p style="margin-top: 1em">The instance of hb_report
running on the host where it was invoked is the master
instance. Instances running on other nodes are slave
instances. The master instance communicates <br>
with slave instances by ssh. There are multiple ssh
invocations per run, so it is essential that the ssh works
without password, i.e. with the public key authentication
and <br>
authorized_keys.</p>

<p style="margin-top: 1em">The operation consists of three
phases. Each phase must finish on all nodes before the next
one can commence. The first phase consists of logging unique
messages through syslog <br>
on all nodes. This is the shortest of all phases.</p>

<p style="margin-top: 1em">The second phase is the most
involved. During this phase all local information is
collected, which includes:</p>

<p style="margin-top: 1em">&Acirc;&middot; logs (both
current and archived if the start time is far in the
past)</p>

<p style="margin-top: 1em">&Acirc;&middot; various
configuration files (corosync, heartbeat, logd)</p>

<p style="margin-top: 1em">&Acirc;&middot; the CIB (both as
xml and as represented by the crm shell)</p>

<p style="margin-top: 1em">&Acirc;&middot; pengine inputs
(if this node was the DC at any point in time over the given
period)</p>

<p style="margin-top: 1em">&Acirc;&middot; system
information and status</p>

<p style="margin-top: 1em">&Acirc;&middot; package
information and status</p>

<p style="margin-top: 1em">&Acirc;&middot; dlm lock
information</p>

<p style="margin-top: 1em">&Acirc;&middot; backtraces (if
there were core dumps)</p>

<p style="margin-top: 1em">The third phase is collecting
information from all nodes and analyzing it. The analyzis
consists of the following tasks:</p>

<p style="margin-top: 1em">&Acirc;&middot; identify files
equal on all nodes which may then be moved to the top
directory</p>

<p style="margin-top: 1em">&Acirc;&middot; save log
messages matching user defined patterns (defaults to ERRORs
and CRITical conditions)</p>

<p style="margin-top: 1em">&Acirc;&middot; report if there
were coredumps and by whom</p>

<p style="margin-top: 1em">&Acirc;&middot; report
crm_verify(8) results</p>

<p style="margin-top: 1em">&Acirc;&middot; save log
messages matching major events to events.txt</p>

<p style="margin-top: 1em">&Acirc;&middot; in case logging
is configured without loghost, node logs and events files
are combined using a perl utility</p>

<p style="margin-top: 1em">BUGS <br>
Finding logs may at times be extremely difficult, depending
on how weird the syslog configuration. It would be nice to
ask syslog-ng developers to provide a way to find out the
<br>
log destination based on facility and priority.</p>

<p style="margin-top: 1em">If you think you found a bug,
please rerun with the -v option and attach the output to
bugzilla.</p>

<p style="margin-top: 1em">hb_report can function in a
satisfactory way only if ssh works to all nodes using
authorized_keys (without password).</p>

<p style="margin-top: 1em">There are way too many
options.</p>

<p style="margin-top: 1em">AUTHOR <br>
Written by Dejan Muhamedagic, &lt;dejan@suse.de&gt;</p>

<p style="margin-top: 1em">RESOURCES <br>
Pacemaker: http://clusterlabs.org/</p>

<p style="margin-top: 1em">Heartbeat and other Linux HA
resources: http://linux-ha.org/wiki</p>

<p style="margin-top: 1em">OpenAIS:
http://www.openais.org/</p>

<p style="margin-top: 1em">Corosync:
http://www.corosync.org/</p>

<p style="margin-top: 1em">SEE ALSO <br>
Date::Parse(3)</p>

<p style="margin-top: 1em">COPYING <br>
Copyright (C) 2007-2009 Dejan Muhamedagic. Free use of this
software is granted under the terms of the GNU General
Public License (GPL).</p>

<p style="margin-top: 1em">hb_report 1.2 04/10/2017
HB_REPORT(8)</p>
<hr>
</body>
</html>
