<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 19:14:32 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>PCS(8) System Administration Utilities PCS(8)</p>

<p style="margin-top: 1em">NAME <br>
pcs - pacemaker/corosync configuration system</p>

<p style="margin-top: 1em">SYNOPSIS <br>
pcs [-f file] [-h] [commands]...</p>

<p style="margin-top: 1em">DESCRIPTION <br>
Control and configure pacemaker and corosync.</p>

<p style="margin-top: 1em">OPTIONS <br>
-h, --help <br>
Display usage and exit.</p>

<p style="margin-top: 1em">-f file <br>
Perform actions on file instead of active CIB.</p>

<p style="margin-top: 1em">--debug <br>
Print all network traffic and external commands run.</p>

<p style="margin-top: 1em">--version <br>
Print pcs version information.</p>

<p style="margin-top: 1em">Commands: <br>
cluster <br>
Configure cluster options and nodes.</p>

<p style="margin-top: 1em">resource <br>
Manage cluster resources.</p>

<p style="margin-top: 1em">stonith <br>
Configure fence devices.</p>

<p style="margin-top: 1em">constraint <br>
Set resource constraints.</p>

<p style="margin-top: 1em">property <br>
Set pacemaker properties.</p>

<p style="margin-top: 1em">acl Set pacemaker access control
lists.</p>

<p style="margin-top: 1em">qdevice <br>
Manage quorum device provider on the local host.</p>

<p style="margin-top: 1em">quorum Manage cluster quorum
settings.</p>

<p style="margin-top: 1em">booth Manage booth (cluster
ticket manager).</p>

<p style="margin-top: 1em">status View cluster status.</p>

<p style="margin-top: 1em">config View and manage cluster
configuration.</p>

<p style="margin-top: 1em">pcsd Manage pcs daemon.</p>

<p style="margin-top: 1em">node Manage cluster nodes.</p>

<p style="margin-top: 1em">alert Manage pacemaker
alerts.</p>

<p style="margin-top: 1em">resource <br>
[show [&lt;resource id&gt;] | --full | --groups |
--hide-inactive] <br>
Show all currently configured resources or if a resource is
specified show the options for the configured resource. If
--full is specified, all configured resource <br>
options will be displayed. If --groups is specified, only
show groups (and their resources). If --hide-inactive is
specified, only show active resources.</p>

<p style="margin-top: 1em">list
[&lt;standard|provider|type&gt;] [--nodesc] <br>
Show list of all available resources, optionally filtered by
specified type, standard or provider. If --nodesc is used
then descriptions of resources are not printed.</p>

<p style="margin-top: 1em">describe
&lt;standard:provider:type|type&gt; <br>
Show options for the specified resource.</p>

<p style="margin-top: 1em">create &lt;resource id&gt;
&lt;standard:provider:type|type&gt; [resource options] [op
&lt;operation action&gt; &lt;operation options&gt;
[&lt;operation action&gt; &lt;operation options&gt;]...]
[meta &lt;meta <br>
options&gt;...] [--clone &lt;clone options&gt; | --master
&lt;master options&gt; | --group &lt;group id&gt; [--before
&lt;resource id&gt; | --after &lt;resource id&gt;]]
[--disabled] [--wait[=n]] <br>
Create specified resource. If --clone is used a clone
resource is created. If --master is specified a master/slave
resource is created. If --group is specified the <br>
resource is added to the group named. You can use --before
or --after to specify the position of the added resource
relatively to some resource already existing in the <br>
group. If --disabled is specified the resource is not
started automatically. If --wait is specified, pcs will wait
up to &rsquo;n&rsquo; seconds for the resource to start and
then <br>
return 0 if the resource is started, or 1 if the resource
has not yet started. If &rsquo;n&rsquo; is not specified it
defaults to 60 minutes.</p>

<p style="margin-top: 1em">Example: Create a new resource
called &rsquo;VirtualIP&rsquo; with IP address 192.168.0.99,
netmask of 32, monitored everything 30 seconds, on eth2: pcs
resource create VirtualIP <br>
ocf:heartbeat:IPaddr2 ip=192.168.0.99 cidr_netmask=32
nic=eth2 op monitor interval=30s</p>

<p style="margin-top: 1em">delete &lt;resource id|group
id|master id|clone id&gt; <br>
Deletes the resource, group, master or clone (and all
resources within the group/master/clone).</p>

<p style="margin-top: 1em">enable &lt;resource id&gt;
[--wait[=n]] <br>
Allow the cluster to start the resource. Depending on the
rest of the configuration (constraints, options, failures,
etc), the resource may remain stopped. If --wait is <br>
specified, pcs will wait up to &rsquo;n&rsquo; seconds for
the resource to start and then return 0 if the resource is
started, or 1 if the resource has not yet started. If
&rsquo;n&rsquo; is not <br>
specified it defaults to 60 minutes.</p>

<p style="margin-top: 1em">disable &lt;resource id&gt;
[--wait[=n]] <br>
Attempt to stop the resource if it is running and forbid the
cluster from starting it again. Depending on the rest of the
configuration (constraints, options, failures, <br>
etc), the resource may remain started. If --wait is
specified, pcs will wait up to &rsquo;n&rsquo; seconds for
the resource to stop and then return 0 if the resource is
stopped or 1 <br>
if the resource has not stopped. If &rsquo;n&rsquo; is not
specified it defaults to 60 minutes.</p>

<p style="margin-top: 1em">restart &lt;resource id&gt;
[node] [--wait=n] <br>
Restart the resource specified. If a node is specified and
if the resource is a clone or master/slave it will be
restarted only on the node specified. If --wait is
speci&acirc; <br>
fied, then we will wait up to &rsquo;n&rsquo; seconds for
the resource to be restarted and return 0 if the restart was
successful or 1 if it was not.</p>

<p style="margin-top: 1em">debug-start &lt;resource id&gt;
[--full] <br>
This command will force the specified resource to start on
this node ignoring the cluster recommendations and print the
output from starting the resource. Using --full <br>
will give more detailed output. This is mainly used for
debugging resources that fail to start.</p>

<p style="margin-top: 1em">debug-stop &lt;resource id&gt;
[--full] <br>
This command will force the specified resource to stop on
this node ignoring the cluster recommendations and print the
output from stopping the resource. Using --full <br>
will give more detailed output. This is mainly used for
debugging resources that fail to stop.</p>

<p style="margin-top: 1em">debug-promote &lt;resource
id&gt; [--full] <br>
This command will force the specified resource to be
promoted on this node ignoring the cluster recommendations
and print the output from promoting the resource. Using <br>
--full will give more detailed output. This is mainly used
for debugging resources that fail to promote.</p>

<p style="margin-top: 1em">debug-demote &lt;resource id&gt;
[--full] <br>
This command will force the specified resource to be demoted
on this node ignoring the cluster recommendations and print
the output from demoting the resource. Using <br>
--full will give more detailed output. This is mainly used
for debugging resources that fail to demote.</p>

<p style="margin-top: 1em">debug-monitor &lt;resource
id&gt; [--full] <br>
This command will force the specified resource to be
moniored on this node ignoring the cluster recommendations
and print the output from monitoring the resource. Using
<br>
--full will give more detailed output. This is mainly used
for debugging resources that fail to be monitored.</p>

<p style="margin-top: 1em">move &lt;resource id&gt;
[destination node] [--master] [lifetime=&lt;lifetime&gt;]
[--wait[=n]] <br>
Move the resource off the node it is currently running on by
creating a -INFINITY location constraint to ban the node. If
destination node is specified the resource will <br>
be moved to that node by creating an INFINITY location
constraint to prefer the destination node. If --master is
used the scope of the command is limited to the master <br>
role and you must use the master id (instead of the resource
id). If lifetime is specified then the constraint will
expire after that time, otherwise it defaults to <br>
infinity and the constraint can be cleared manually with
&rsquo;pcs resource clear&rsquo; or &rsquo;pcs constraint
delete&rsquo;. If --wait is specified, pcs will wait up to
&rsquo;n&rsquo; seconds for the <br>
resource to move and then return 0 on success or 1 on error.
If &rsquo;n&rsquo; is not specified it defaults to 60
minutes. If you want the resource to preferably avoid
running on <br>
some nodes but be able to failover to them use &rsquo;pcs
location avoids&rsquo;.</p>

<p style="margin-top: 1em">ban &lt;resource id&gt; [node]
[--master] [lifetime=&lt;lifetime&gt;] [--wait[=n]] <br>
Prevent the resource id specified from running on the node
(or on the current node it is running on if no node is
specified) by creating a -INFINITY location constraint. <br>
If --master is used the scope of the command is limited to
the master role and you must use the master id (instead of
the resource id). If lifetime is specified then the <br>
constraint will expire after that time, otherwise it
defaults to infinity and the constraint can be cleared
manually with &rsquo;pcs resource clear&rsquo; or &rsquo;pcs
constraint delete&rsquo;. <br>
If --wait is specified, pcs will wait up to &rsquo;n&rsquo;
seconds for the resource to move and then return 0 on
success or 1 on error. If &rsquo;n&rsquo; is not specified
it defaults to 60 <br>
minutes. If you want the resource to preferably avoid
running on some nodes but be able to failover to them use
&rsquo;pcs location avoids&rsquo;.</p>

<p style="margin-top: 1em">clear &lt;resource id&gt; [node]
[--master] [--wait[=n]] <br>
Remove constraints created by move and/or ban on the
specified resource (and node if specified). If --master is
used the scope of the command is limited to the master role
<br>
and you must use the master id (instead of the resource id).
If --wait is specified, pcs will wait up to &rsquo;n&rsquo;
seconds for the operation to finish (including starting <br>
and/or moving resources if appropriate) and then return 0 on
success or 1 on error. If &rsquo;n&rsquo; is not specified
it defaults to 60 minutes.</p>

<p style="margin-top: 1em">standards <br>
List available resource agent standards supported by this
installation (OCF, LSB, etc.).</p>

<p style="margin-top: 1em">providers <br>
List available OCF resource agent providers.</p>

<p style="margin-top: 1em">agents [standard[:provider]]
<br>
List available agents optionally filtered by standard and
provider.</p>

<p style="margin-top: 1em">update &lt;resource id&gt;
[resource options] [op [&lt;operation action&gt;
&lt;operation options&gt;]...] [meta &lt;meta
operations&gt;...] [--wait[=n]] <br>
Add/Change options to specified resource, clone or
multi-state resource. If an operation (op) is specified it
will update the first found operation with the same action
<br>
on the specified resource, if no operation with that action
exists then a new operation will be created. (WARNING: all
existing options on the updated operation will be <br>
reset if not specified.) If you want to create multiple
monitor operations you should use the &rsquo;op add&rsquo;
&amp; &rsquo;op remove&rsquo; commands. If --wait is
specified, pcs will wait up <br>
to &rsquo;n&rsquo; seconds for the changes to take effect
and then return 0 if the changes have been processed or 1
otherwise. If &rsquo;n&rsquo; is not specified it defaults
to 60 minutes.</p>

<p style="margin-top: 1em">op add &lt;resource id&gt;
&lt;operation action&gt; [operation properties] <br>
Add operation for specified resource.</p>

<p style="margin-top: 1em">op remove &lt;resource id&gt;
&lt;operation action&gt; [&lt;operation properties&gt;...]
<br>
Remove specified operation (note: you must specify the exact
operation properties to properly remove an existing
operation).</p>

<p style="margin-top: 1em">op remove &lt;operation id&gt;
<br>
Remove the specified operation id.</p>

<p style="margin-top: 1em">op defaults [options] <br>
Set default values for operations, if no options are passed,
lists currently configured defaults.</p>

<p style="margin-top: 1em">meta &lt;resource id | group id
| master id | clone id&gt; &lt;meta options&gt; [--wait[=n]]
<br>
Add specified options to the specified resource, group,
master/slave or clone. Meta options should be in the format
of name=value, options may be removed by setting an <br>
option without a value. If --wait is specified, pcs will
wait up to &rsquo;n&rsquo; seconds for the changes to take
effect and then return 0 if the changes have been processed
or 1 <br>
otherwise. If &rsquo;n&rsquo; is not specified it defaults
to 60 minutes. Example: pcs resource meta TestResource
failure-timeout=50 stickiness=</p>

<p style="margin-top: 1em">group add &lt;group id&gt;
&lt;resource id&gt; [resource id] ... [resource id]
[--before &lt;resource id&gt; | --after &lt;resource id&gt;]
[--wait[=n]] <br>
Add the specified resource to the group, creating the group
if it does not exist. If the resource is present in another
group it is moved to the new group. You can use <br>
--before or --after to specify the position of the added
resources relatively to some resource already existing in
the group. If --wait is specified, pcs will wait up to <br>
&rsquo;n&rsquo; seconds for the operation to finish
(including moving resources if appropriate) and then return
0 on success or 1 on error. If &rsquo;n&rsquo; is not
specified it defaults to 60 <br>
minutes.</p>

<p style="margin-top: 1em">group remove &lt;group id&gt;
&lt;resource id&gt; [resource id] ... [resource id]
[--wait[=n]] <br>
Remove the specified resource(s) from the group, removing
the group if it no resources remain. If --wait is specified,
pcs will wait up to &rsquo;n&rsquo; seconds for the
operation <br>
to finish (including moving resources if appropriate) and
then return 0 on success or 1 on error. If &rsquo;n&rsquo;
is not specified it defaults to 60 minutes.</p>

<p style="margin-top: 1em">ungroup &lt;group id&gt;
[resource id] ... [resource id] [--wait[=n]] <br>
Remove the group (note: this does not remove any resources
from the cluster) or if resources are specified, remove the
specified resources from the group. If --wait is <br>
specified, pcs will wait up to &rsquo;n&rsquo; seconds for
the operation to finish (including moving resources if
appropriate) and the return 0 on success or 1 on error. If
&rsquo;n&rsquo; is <br>
not specified it defaults to 60 minutes.</p>

<p style="margin-top: 1em">clone &lt;resource id | group
id&gt; [clone options]... [--wait[=n]] <br>
Setup up the specified resource or group as a clone. If
--wait is specified, pcs will wait up to &rsquo;n&rsquo;
seconds for the operation to finish (including starting
clone <br>
instances if appropriate) and then return 0 on success or 1
on error. If &rsquo;n&rsquo; is not specified it defaults to
60 minutes.</p>

<p style="margin-top: 1em">unclone &lt;resource id | group
id&gt; [--wait[=n]] <br>
Remove the clone which contains the specified group or
resource (the resource or group will not be removed). If
--wait is specified, pcs will wait up to &rsquo;n&rsquo;
seconds for <br>
the operation to finish (including stopping clone instances
if appropriate) and then return 0 on success or 1 on error.
If &rsquo;n&rsquo; is not specified it defaults to 60
minutes.</p>

<p style="margin-top: 1em">master [&lt;master/slave id&gt;]
&lt;resource id | group id&gt; [options] [--wait[=n]] <br>
Configure a resource or group as a multi-state
(master/slave) resource. If --wait is specified, pcs will
wait up to &rsquo;n&rsquo; seconds for the operation to
finish (including <br>
starting and promoting resource instances if appropriate)
and then return 0 on success or 1 on error. If
&rsquo;n&rsquo; is not specified it defaults to 60 minutes.
Note: to remove <br>
a master you must remove the resource/group it contains.</p>

<p style="margin-top: 1em">manage &lt;resource id&gt; ...
[resource n] <br>
Set resources listed to managed mode (default).</p>

<p style="margin-top: 1em">unmanage &lt;resource id&gt; ...
[resource n] <br>
Set resources listed to unmanaged mode.</p>

<p style="margin-top: 1em">defaults [options] <br>
Set default values for resources, if no options are passed,
lists currently configured defaults.</p>

<p style="margin-top: 1em">cleanup [&lt;resource id&gt;]
[--node &lt;node&gt;] <br>
Cleans up the resource in the lrmd (useful to reset the
resource status and failcount). This tells the cluster to
forget the operation history of a resource and re-detect
<br>
its current state. This can be useful to purge knowledge of
past failures that have since been resolved. If a resource
id is not specified then all resources/stonith <br>
devices will be cleaned up. If a node is not specified then
resources on all nodes will be cleaned up.</p>

<p style="margin-top: 1em">failcount show &lt;resource
id&gt; [node] <br>
Show current failcount for specified resource from all nodes
or only on specified node.</p>

<p style="margin-top: 1em">failcount reset &lt;resource
id&gt; [node] <br>
Reset failcount for specified resource on all nodes or only
on specified node. This tells the cluster to forget how many
times a resource has failed in the past. This may <br>
allow the resource to be started or moved to a more
preferred location.</p>

<p style="margin-top: 1em">relocate dry-run [resource1]
[resource2] ... <br>
The same as &rsquo;relocate run&rsquo; but has no effect on
the cluster.</p>

<p style="margin-top: 1em">relocate run [resource1]
[resource2] ... <br>
Relocate specified resources to their preferred nodes. If no
resources are specified, relocate all resources. This
command calculates the preferred node for each <br>
resource while ignoring resource stickiness. Then it creates
location constraints which will cause the resources to move
to their preferred nodes. Once the resources <br>
have been moved the constraints are deleted automatically.
Note that the preferred node is calculated based on current
cluster status, constraints, location of resources <br>
and other settings and thus it might change over time.</p>

<p style="margin-top: 1em">relocate show <br>
Display current status of resources and their optimal node
ignoring resource stickiness.</p>

<p style="margin-top: 1em">relocate clear <br>
Remove all constraints created by the &rsquo;relocate
run&rsquo; command.</p>

<p style="margin-top: 1em">utilization [&lt;resource id&gt;
[&lt;name&gt;=&lt;value&gt; ...]] <br>
Add specified utilization options to specified resource. If
resource is not specified, shows utilization of all
resources. If utilization options are not specified, shows
<br>
utilization of specified resource. Utilization option should
be in format name=value, value has to be integer. Options
may be removed by setting an option without a value. <br>
Example: pcs resource utilization TestResource cpu=
ram=20</p>

<p style="margin-top: 1em">cluster <br>
auth [node] [...] [-u username] [-p password] [--force]
[--local] <br>
Authenticate pcs to pcsd on nodes specified, or on all nodes
configured in corosync.conf if no nodes are specified
(authorization tokens are stored in ~/.pcs/tokens or <br>
/var/lib/pcsd/tokens for root). By default all nodes are
also authenticated to each other, using --local only
authenticates the local node (and does not authenticate the
<br>
remote nodes with each other). Using --force forces
re-authentication to occur.</p>

<p style="margin-top: 1em">setup [--start
[--wait[=&lt;n&gt;]]] [--local] [--enable] --name
&lt;cluster name&gt; &lt;node1[,node1-altaddr]&gt;
[&lt;node2[,node2-altaddr]&gt;] [...] [--transport udpu|udp]
[--rrpmode active|pas&acirc; <br>
sive] [--addr0 &lt;addr/net&gt; [[[--mcast0 &lt;address&gt;]
[--mcastport0 &lt;port&gt;] [--ttl0 &lt;ttl&gt;]] |
[--broadcast0]] [--addr1 &lt;addr/net&gt; [[[--mcast1
&lt;address&gt;] [--mcastport1 &lt;port&gt;] [--ttl1 <br>
&lt;ttl&gt;]] | [--broadcast1]]]]
[--wait_for_all=&lt;0|1&gt;]
[--auto_tie_breaker=&lt;0|1&gt;]
[--last_man_standing=&lt;0|1&gt;
[--last_man_standing_window=&lt;time in ms&gt;]] [--ipv6]
[--token &lt;timeout&gt;] <br>
[--token_coefficient &lt;timeout&gt;] [--join
&lt;timeout&gt;] [--consensus &lt;timeout&gt;]
[--miss_count_const &lt;count&gt;] [--fail_recv_const
&lt;failures&gt;] <br>
Configure corosync and sync configuration out to listed
nodes. --local will only perform changes on the local node,
--start will also start the cluster on the specified <br>
nodes, --wait will wait up to &rsquo;n&rsquo; seconds for
the nodes to start, --enable will enable corosync and
pacemaker on node startup, --transport allows specification
of corosync <br>
transport (default: udpu; udp for RHEL 6 clusters),
--rrpmode allows you to set the RRP mode of the system.
Currently only &rsquo;passive&rsquo; is supported or tested
(using &rsquo;active&rsquo; <br>
is not recommended). The --wait_for_all, --auto_tie_breaker,
--last_man_standing, --last_man_standing_window options are
all documented in corosync&rsquo;s votequorum(5) man <br>
page. These options are not supported on RHEL 6
clusters.</p>

<p style="margin-top: 1em">--ipv6 will configure corosync
to use ipv6 (instead of ipv4). This option is not supported
on RHEL 6 clusters.</p>

<p style="margin-top: 1em">--token &lt;timeout&gt; sets
time in milliseconds until a token loss is declared after
not receiving a token (default 1000 ms)</p>

<p style="margin-top: 1em">--token_coefficient
&lt;timeout&gt; sets time in milliseconds used for clusters
with at least 3 nodes as a coefficient for real token
timeout calculation (token + (num&acirc; <br>
ber_of_nodes - 2) * token_coefficient) (default 650 ms) This
option is not supported on RHEL 6 clusters.</p>

<p style="margin-top: 1em">--join &lt;timeout&gt; sets time
in milliseconds to wait for join messages (default 50
ms)</p>

<p style="margin-top: 1em">--consensus &lt;timeout&gt; sets
time in milliseconds to wait for consensus to be achieved
before starting a new round of membership configuration
(default 1200 ms)</p>

<p style="margin-top: 1em">--miss_count_const &lt;count&gt;
sets the maximum number of times on receipt of a token a
message is checked for retransmission before a
retransmission occurs (default 5 mes&acirc; <br>
sages)</p>

<p style="margin-top: 1em">--fail_recv_const
&lt;failures&gt; specifies how many rotations of the token
without receiving any messages when messages should be
received may occur before a new configuration <br>
is formed (default 2500 failures)</p>

<p style="margin-top: 1em">Configuring Redundant Ring
Protocol (RRP)</p>

<p style="margin-top: 1em">When using udpu specifying
nodes, specify the ring 0 address first followed by a
&rsquo;,&rsquo; and then the ring 1 address.</p>

<p style="margin-top: 1em">Example: pcs cluster setup
--name cname nodeA-0,nodeA-1 nodeB-0,nodeB-1</p>

<p style="margin-top: 1em">When using udp, using --addr0
and --addr1 will allow you to configure rrp mode for
corosync. It&rsquo;s recommended to use a network (instead
of IP address) for --addr0 and <br>
--addr1 so the same corosync.conf file can be used around
the cluster. --mcast0 defaults to 239.255.1.1 and --mcast1
defaults to 239.255.2.1, --mcastport0/1 default to <br>
5405 and ttl defaults to 1. If --broadcast is specified,
--mcast0/1, --mcastport0/1 &amp; --ttl0/1 are ignored.</p>

<p style="margin-top: 1em">start [--all] [node] [...]
[--wait[=&lt;n&gt;]] <br>
Start corosync &amp; pacemaker on specified node(s), if a
node is not specified then corosync &amp; pacemaker are
started on the local node. If --all is specified then
corosync &amp; <br>
pacemaker are started on all nodes. If --wait is specified,
wait up to &rsquo;n&rsquo; seconds for nodes to start.</p>

<p style="margin-top: 1em">stop [--all] [node] [...] <br>
Stop corosync &amp; pacemaker on specified node(s), if a
node is not specified then corosync &amp; pacemaker are
stopped on the local node. If --all is specified then
corosync &amp; <br>
pacemaker are stopped on all nodes.</p>

<p style="margin-top: 1em">kill Force corosync and
pacemaker daemons to stop on the local node (performs kill
-9). Note that init system (e.g. systemd) can detect that
cluster is not running and start it <br>
again. If you want to stop cluster on a node, run pcs
cluster stop on that node.</p>

<p style="margin-top: 1em">enable [--all] [node] [...] <br>
Configure corosync &amp; pacemaker to run on node boot on
specified node(s), if node is not specified then corosync
&amp; pacemaker are enabled on the local node. If --all is
<br>
specified then corosync &amp; pacemaker are enabled on all
nodes.</p>

<p style="margin-top: 1em">disable [--all] [node] [...]
<br>
Configure corosync &amp; pacemaker to not run on node boot
on specified node(s), if node is not specified then corosync
&amp; pacemaker are disabled on the local node. If --all is
<br>
specified then corosync &amp; pacemaker are disabled on all
nodes. Note: this is the default after installation.</p>

<p style="margin-top: 1em">remote-node add &lt;hostname&gt;
&lt;resource id&gt; [options] <br>
Enables the specified resource as a remote-node resource on
the specified hostname (hostname should be the same as
&rsquo;uname -n&rsquo;).</p>

<p style="margin-top: 1em">remote-node remove
&lt;hostname&gt; <br>
Disables any resources configured to be remote-node resource
on the specified hostname (hostname should be the same as
&rsquo;uname -n&rsquo;).</p>

<p style="margin-top: 1em">status View current cluster
status (an alias of &rsquo;pcs status cluster&rsquo;).</p>

<p style="margin-top: 1em">pcsd-status [node] [...] <br>
Get current status of pcsd on nodes specified, or on all
nodes configured in corosync.conf if no nodes are
specified.</p>

<p style="margin-top: 1em">sync Sync corosync configuration
to all nodes found from current corosync.conf file
(cluster.conf on systems running Corosync 1.x).</p>

<p style="margin-top: 1em">cib [filename]
[scope=&lt;scope&gt; | --config] <br>
Get the raw xml from the CIB (Cluster Information Base). If
a filename is provided, we save the CIB to that file,
otherwise the CIB is printed. Specify scope to get a <br>
specific section of the CIB. Valid values of the scope are:
configuration, nodes, resources, constraints, crm_config,
rsc_defaults, op_defaults, status. --config is the <br>
same as scope=configuration. Do not specify a scope if you
want to edit the saved CIB using pcs (pcs -f
&lt;command&gt;).</p>

<p style="margin-top: 1em">cib-push &lt;filename&gt;
[diff-against=&lt;filename_original&gt; |
scope=&lt;scope&gt; | --config] [--wait[=&lt;n&gt;]] <br>
Push the raw xml from &lt;filename&gt; to the CIB (Cluster
Information Base). You can obtain the CIB by running the
&rsquo;pcs cluster cib&rsquo; command, which is recommended
first step <br>
when you want to perform desired modifications (pcs -f
&lt;command&gt;) for the one-off push. If diff-against is
specified, pcs diffs contents of filename against contents
of <br>
filename_original and pushes the result to the CIB. Specify
scope to push a specific section of the CIB. Valid values of
the scope are: configuration, nodes, resources, <br>
constraints, crm_config, rsc_defaults, op_defaults. --config
is the same as scope=configuration. Use of --config is
recommended. Do not specify a scope if you need to <br>
push the whole CIB or be warned in the case of outdated CIB.
If --wait is specified wait up to &rsquo;n&rsquo; seconds
for changes to be applied. WARNING: the selected scope of
the <br>
CIB will be overwritten by the current content of the
specified file.</p>

<p style="margin-top: 1em">Example: <br>
pcs cluster cib &gt; original.xml <br>
cp original.xml new.xml <br>
pcs -f new.xml constraint location apache prefers node2 <br>
pcs cluster cib-push new.xml diff-against=original.xml</p>

<p style="margin-top: 1em">cib-upgrade <br>
Upgrade the CIB to conform to the latest version of the
document schema.</p>

<p style="margin-top: 1em">edit [scope=&lt;scope&gt; |
--config] <br>
Edit the cib in the editor specified by the $EDITOR
environment variable and push out any changes upon saving.
Specify scope to edit a specific section of the CIB. Valid
<br>
values of the scope are: configuration, nodes, resources,
constraints, crm_config, rsc_defaults, op_defaults. --config
is the same as scope=configuration. Use of --con&acirc; <br>
fig is recommended. Do not specify a scope if you need to
edit the whole CIB or be warned in the case of outdated
CIB.</p>

<p style="margin-top: 1em">node add
&lt;node[,node-altaddr]&gt; [--start [--wait[=&lt;n&gt;]]]
[--enable] [--watchdog=&lt;watchdog-path&gt;] <br>
Add the node to corosync.conf and corosync on all nodes in
the cluster and sync the new corosync.conf to the new node.
If --start is specified also start corosync/pace&acirc; <br>
maker on the new node, if --wait is sepcified wait up to
&rsquo;n&rsquo; seconds for the new node to start. If
--enable is specified enable corosync/pacemaker on new node.
When <br>
using Redundant Ring Protocol (RRP) with udpu transport,
specify the ring 0 address first followed by a
&rsquo;,&rsquo; and then the ring 1 address. Use --watchdog
to specify path to <br>
watchdog on newly added node, when SBD is enabled in
cluster.</p>

<p style="margin-top: 1em">node remove &lt;node&gt; <br>
Shutdown specified node and remove it from pacemaker and
corosync on all other nodes in the cluster.</p>

<p style="margin-top: 1em">uidgid List the current
configured uids and gids of users allowed to connect to
corosync.</p>

<p style="margin-top: 1em">uidgid add [uid=&lt;uid&gt;]
[gid=&lt;gid&gt;] <br>
Add the specified uid and/or gid to the list of users/groups
allowed to connect to corosync.</p>

<p style="margin-top: 1em">uidgid rm [uid=&lt;uid&gt;]
[gid=&lt;gid&gt;] <br>
Remove the specified uid and/or gid from the list of
users/groups allowed to connect to corosync.</p>

<p style="margin-top: 1em">corosync [node] <br>
Get the corosync.conf from the specified node or from the
current node if node not specified.</p>

<p style="margin-top: 1em">reload corosync <br>
Reload the corosync configuration on the current node.</p>

<p style="margin-top: 1em">destroy [--all] <br>
Permanently destroy the cluster on the current node, killing
all corosync/pacemaker processes removing all cib files and
the corosync.conf file. Using --all will attempt <br>
to destroy the cluster on all nodes configure in the
corosync.conf file. WARNING: This command permantly removes
any cluster configuration that has been created. It is <br>
recommended to run &rsquo;pcs cluster stop&rsquo; before
destroying the cluster.</p>

<p style="margin-top: 1em">verify [-V] [filename] <br>
Checks the pacemaker configuration (cib) for syntax and
common conceptual errors. If no filename is specified the
check is performed on the currently running cluster. If <br>
-V is used more verbose output will be printed.</p>

<p style="margin-top: 1em">report [--from &quot;YYYY-M-D
H:M:S&quot; [--to &quot;YYYY-M-D&quot; H:M:S&quot;]] dest
<br>
Create a tarball containing everything needed when reporting
cluster problems. If --from and --to are not used, the
report will include the past 24 hours.</p>

<p style="margin-top: 1em">stonith <br>
[show [stonith id]] [--full] <br>
Show all currently configured stonith devices or if a
stonith id is specified show the options for the configured
stonith device. If --full is specified all configured <br>
stonith options will be displayed.</p>

<p style="margin-top: 1em">list [filter] [--nodesc] <br>
Show list of all available stonith agents (if filter is
provided then only stonith agents matching the filter will
be shown). If --nodesc is used then descriptions of <br>
stonith agents are not printed.</p>

<p style="margin-top: 1em">describe &lt;stonith agent&gt;
<br>
Show options for specified stonith agent.</p>

<p style="margin-top: 1em">create &lt;stonith id&gt;
&lt;stonith device type&gt; [stonith device options] [op
&lt;operation action&gt; &lt;operation options&gt;
[&lt;operation action&gt; &lt;operation options&gt;]...]
[meta &lt;meta <br>
options&gt;...] <br>
Create stonith device with specified type and options.</p>

<p style="margin-top: 1em">update &lt;stonith id&gt;
[stonith device options] <br>
Add/Change options to specified stonith id.</p>

<p style="margin-top: 1em">delete &lt;stonith id&gt; <br>
Remove stonith id from configuration.</p>

<p style="margin-top: 1em">cleanup [&lt;stonith id&gt;]
[--node &lt;node&gt;] <br>
Cleans up the stonith device in the lrmd (useful to reset
the status and failcount). This tells the cluster to forget
the operation history of a stonith device and re- <br>
detect its current state. This can be useful to purge
knowledge of past failures that have since been resolved. If
a stonith id is not specified then all <br>
resources/stonith devices will be cleaned up. If a node is
not specified then resources on all nodes will be cleaned
up.</p>

<p style="margin-top: 1em">level Lists all of the fencing
levels currently configured.</p>

<p style="margin-top: 1em">level add &lt;level&gt;
&lt;node&gt; &lt;devices&gt; <br>
Add the fencing level for the specified node with a comma
separated list of devices (stonith ids) to attempt for that
node at that level. Fence levels are attempted in <br>
numerical order (starting with 1) if a level succeeds
(meaning all devices are successfully fenced in that level)
then no other levels are tried, and the node is
consid&acirc; <br>
ered fenced.</p>

<p style="margin-top: 1em">level remove &lt;level&gt; [node
id] [stonith id] ... [stonith id] <br>
Removes the fence level for the level, node and/or devices
specified. If no nodes or devices are specified then the
fence level is removed.</p>

<p style="margin-top: 1em">level clear [node|stonith id(s)]
<br>
Clears the fence levels on the node (or stonith id)
specified or clears all fence levels if a node/stonith id is
not specified. If more than one stonith id is specified <br>
they must be separated by a comma and no spaces. Example:
pcs stonith level clear dev_a,dev_b</p>

<p style="margin-top: 1em">level verify <br>
Verifies all fence devices and nodes specified in fence
levels exist.</p>

<p style="margin-top: 1em">fence &lt;node&gt; [--off] <br>
Fence the node specified (if --off is specified, use the
&rsquo;off&rsquo; API call to stonith which will turn the
node off instead of rebooting it).</p>

<p style="margin-top: 1em">confirm &lt;node&gt; [--force]
<br>
Confirm that the host specified is currently down. This
command should ONLY be used when the node specified has
already been confirmed to be powered off and to have no <br>
access to shared resources.</p>

<p style="margin-top: 1em">WARNING: If this node is not
actually powered off or it does have access to shared
resources, data corruption/cluster failure can occur. To
prevent accidental running of <br>
this command, --force or interactive user response is
required in order to proceed.</p>

<p style="margin-top: 1em">sbd enable
[--watchdog=&lt;path&gt;[@&lt;node&gt;]] ...
[&lt;SBD_OPTION&gt;=&lt;value&gt;] ... <br>
Enable SBD in cluster. Default path for watchdog device is
/dev/watchdog. Allowed SBD options: SBD_WATCHDOG_TIMEOUT
(default: 5), SBD_DELAY_START (default: no) and <br>
SBD_STARTMODE (default: clean).</p>

<p style="margin-top: 1em">WARNING: Cluster has to be
restarted in order to apply these changes.</p>

<p style="margin-top: 1em">Example of enabling SBD in
cluster with watchdogs on node1 will be /dev/watchdog2, on
node2 /dev/watchdog1, /dev/watchdog0 on all other nodes and
watchdog timeout will bet <br>
set to 10 seconds:</p>

<p style="margin-top: 1em">pcs stonith sbd enable
--watchdog=/dev/watchdog2@node1
--watchdog=/dev/watchdog1@node2 --watchdog=/dev/watchdog0
SBD_WATCHDOG_TIMEOUT=10</p>

<p style="margin-top: 1em">sbd disable <br>
Disable SBD in cluster.</p>

<p style="margin-top: 1em">WARNING: Cluster has to be
restarted in order to apply these changes.</p>

<p style="margin-top: 1em">sbd status <br>
Show status of SBD services in cluster.</p>

<p style="margin-top: 1em">sbd config <br>
Show SBD configuration in cluster.</p>

<p style="margin-top: 1em">acl <br>
[show] List all current access control lists.</p>

<p style="margin-top: 1em">enable Enable access control
lists.</p>

<p style="margin-top: 1em">disable <br>
Disable access control lists.</p>

<p style="margin-top: 1em">role create &lt;role id&gt;
[description=&lt;description&gt;] [((read | write | deny)
(xpath &lt;query&gt; | id &lt;id&gt;))...] <br>
Create a role with the id and (optional) description
specified. Each role can also have an unlimited number of
permissions (read/write/deny) applied to either an xpath
<br>
query or the id of a specific element in the cib.</p>

<p style="margin-top: 1em">role delete &lt;role id&gt; <br>
Delete the role specified and remove it from any
users/groups it was assigned to.</p>

<p style="margin-top: 1em">role assign &lt;role id&gt; [to]
&lt;username/group&gt; <br>
Assign a role to a user or group already created with
&rsquo;pcs acl user/group create&rsquo;.</p>

<p style="margin-top: 1em">role unassign &lt;role id&gt;
[from] &lt;username/group&gt; <br>
Remove a role from the specified user.</p>

<p style="margin-top: 1em">user create &lt;username&gt;
&lt;role id&gt; [&lt;role id&gt;]... <br>
Create an ACL for the user specified and assign roles to the
user.</p>

<p style="margin-top: 1em">user delete &lt;username&gt;
<br>
Remove the user specified (and roles assigned will be
unassigned for the specified user).</p>

<p style="margin-top: 1em">group create &lt;group&gt;
&lt;role id&gt; [&lt;role id&gt;]... <br>
Create an ACL for the group specified and assign roles to
the group.</p>

<p style="margin-top: 1em">group delete &lt;group&gt; <br>
Remove the group specified (and roles assigned will be
unassigned for the specified group).</p>

<p style="margin-top: 1em">permission add &lt;role id&gt;
((read | write | deny) (xpath &lt;query&gt; | id
&lt;id&gt;))... <br>
Add the listed permissions to the role specified.</p>

<p style="margin-top: 1em">permission delete &lt;permission
id&gt; <br>
Remove the permission id specified (permission id&rsquo;s
are listed in parenthesis after permissions in &rsquo;pcs
acl&rsquo; output).</p>

<p style="margin-top: 1em">property <br>
[list|show [&lt;property&gt; | --all | --defaults]] | [--all
| --defaults] <br>
List property settings (default: lists configured
properties). If --defaults is specified will show all
property defaults, if --all is specified, current configured
prop&acirc; <br>
erties will be shown with unset properties and their
defaults. Run &rsquo;man pengine&rsquo; and &rsquo;man
crmd&rsquo; to get a description of the properties.</p>

<p style="margin-top: 1em">set [--force | --node
&lt;nodename&gt;] &lt;property&gt;=[&lt;value&gt;]
[&lt;property&gt;=[&lt;value&gt;] ...] <br>
Set specific pacemaker properties (if the value is blank
then the property is removed from the configuration). If a
property is not recognized by pcs the property will <br>
not be created unless the --force is used. If --node is used
a node attribute is set on the specified node. Run
&rsquo;man pengine&rsquo; and &rsquo;man crmd&rsquo; to get
a description of the <br>
properties.</p>

<p style="margin-top: 1em">unset [--node &lt;nodename&gt;]
&lt;property&gt; <br>
Remove property from configuration (or remove attribute from
specified node if --node is used). Run &rsquo;man
pengine&rsquo; and &rsquo;man crmd&rsquo; to get a
description of the properties.</p>

<p style="margin-top: 1em">constraint <br>
[list|show] --full <br>
List all current location, order and colocation constraints,
if --full is specified also list the constraint ids.</p>

<p style="margin-top: 1em">location &lt;resource id&gt;
prefers &lt;node[=score]&gt;... <br>
Create a location constraint on a resource to prefer the
specified node and score (default score: INFINITY).</p>

<p style="margin-top: 1em">location &lt;resource id&gt;
avoids &lt;node[=score]&gt;... <br>
Create a location constraint on a resource to avoid the
specified node and score (default score: INFINITY).</p>

<p style="margin-top: 1em">location &lt;resource id&gt;
rule [id=&lt;rule id&gt;]
[resource-discovery=&lt;option&gt;] [role=master|slave]
[constraint-id=&lt;id&gt;]
[score=&lt;score&gt;|score-attribute=&lt;attribute&gt;]
&lt;expression&gt; <br>
Creates a location rule on the specified resource where the
expression looks like one of the following: <br>
defined|not_defined &lt;attribute&gt; <br>
&lt;attribute&gt; lt|gt|lte|gte|eq|ne
[string|integer|version] &lt;value&gt; <br>
date gt|lt &lt;date&gt; <br>
date in_range &lt;date&gt; to &lt;date&gt; <br>
date in_range &lt;date&gt; to duration &lt;duration
options&gt;... <br>
date-spec &lt;date spec options&gt;... <br>
&lt;expression&gt; and|or &lt;expression&gt; <br>
( &lt;expression&gt; ) <br>
where duration options and date spec options are: hours,
monthdays, weekdays, yeardays, months, weeks, years,
weekyears, moon. If score is omitted it defaults to
INFINITY. <br>
If id is omitted one is generated from the resource id. If
resource-discovery is omitted it defaults to
&rsquo;always&rsquo;.</p>

<p style="margin-top: 1em">location [show [resources|nodes
[node id|resource id]...] [--full]] <br>
List all the current location constraints, if
&rsquo;resources&rsquo; is specified location constraints
are displayed per resource (default), if &rsquo;nodes&rsquo;
is specified location con&acirc; <br>
straints are displayed per node. If specific nodes or
resources are specified then we only show information about
them. If --full is specified show the internal con&acirc;
<br>
straint id&rsquo;s as well.</p>

<p style="margin-top: 1em">location add &lt;id&gt;
&lt;resource id&gt; &lt;node&gt; &lt;score&gt;
[resource-discovery=&lt;option&gt;] <br>
Add a location constraint with the appropriate id, resource
id, node name and score. (For more advanced pacemaker
usage.)</p>

<p style="margin-top: 1em">location remove &lt;id&gt;
[&lt;resource id&gt; &lt;node&gt; &lt;score&gt;] <br>
Remove a location constraint with the appropriate id,
resource id, node name and score. (For more advanced
pacemaker usage.)</p>

<p style="margin-top: 1em">order [show] [--full] <br>
List all current ordering constraints (if --full is
specified show the internal constraint id&rsquo;s as
well).</p>

<p style="margin-top: 1em">order [action] &lt;resource
id&gt; then [action] &lt;resource id&gt; [options] <br>
Add an ordering constraint specifying actions (start, stop,
promote, demote) and if no action is specified the default
action will be start. Available options are <br>
kind=Optional/Mandatory/Serialize, symmetrical=true/false,
require-all=true/false and id=&lt;constraint-id&gt;.</p>

<p style="margin-top: 1em">order set &lt;resource1&gt;
[resourceN]... [options] [set &lt;resourceX&gt; ...
[options]] [setoptions [constraint_options]] <br>
Create an ordered set of resources. Available options are
sequential=true/false, require-all=true/false,
action=start/promote/demote/stop and
role=Stopped/Started/Mas&acirc; <br>
ter/Slave. Available constraint_options are
id=&lt;constraint-id&gt;, kind=Optional/Mandatory/Serialize
and symmetrical=true/false.</p>

<p style="margin-top: 1em">order remove &lt;resource1&gt;
[resourceN]... <br>
Remove resource from any ordering constraint</p>

<p style="margin-top: 1em">colocation [show] [--full] <br>
List all current colocation constraints (if --full is
specified show the internal constraint id&rsquo;s as
well).</p>

<p style="margin-top: 1em">colocation add [master|slave]
&lt;source resource id&gt; with [master|slave] &lt;target
resource id&gt; [score] [options] [id=constraint-id] <br>
Request &lt;source resource&gt; to run on the same node
where pacemaker has determined &lt;target resource&gt;
should run. Positive values of score mean the resources
should be run <br>
on the same node, negative values mean the resources should
not be run on the same node. Specifying
&rsquo;INFINITY&rsquo; (or &rsquo;-INFINITY&rsquo;) for the
score forces &lt;source resource&gt; to <br>
run (or not run) with &lt;target resource&gt; (score
defaults to &quot;INFINITY&quot;). A role can be master or
slave (if no role is specified, it defaults to
&rsquo;started&rsquo;).</p>

<p style="margin-top: 1em">colocation set &lt;resource1&gt;
[resourceN]... [options] [set &lt;resourceX&gt; ...
[options]] [setoptions [constraint_options]] <br>
Create a colocation constraint with a resource set.
Available options are sequential=true/false,
require-all=true/false, action=start/promote/demote/stop and
<br>
role=Stopped/Started/Master/Slave. Available
constraint_options are id, score, score-attribute and
score-attribute-mangle.</p>

<p style="margin-top: 1em">colocation remove &lt;source
resource id&gt; &lt;target resource id&gt; <br>
Remove colocation constraints with specified resources.</p>

<p style="margin-top: 1em">ticket [show] [--full] <br>
List all current ticket constraints (if --full is specified
show the internal constraint id&rsquo;s as well).</p>

<p style="margin-top: 1em">ticket add &lt;ticket&gt;
[&lt;role&gt;] &lt;resource id&gt; [&lt;options&gt;]
[id=&lt;constraint-id&gt;] <br>
Create a ticket constraint for &lt;resource id&gt;.
Available option is loss-policy=fence/stop/freeze/demote. A
role can be master, slave, started or stopped.</p>

<p style="margin-top: 1em">ticket set &lt;resource1&gt;
[&lt;resourceN&gt;]... [&lt;options&gt;] [set
&lt;resourceX&gt; ... [&lt;options&gt;]] setoptions
&lt;constraint_options&gt; <br>
Create a ticket constraint with a resource set. Available
options are sequential=true/false, require-all=true/false,
action=start/promote/demote/stop and <br>
role=Stopped/Started/Master/Slave. Required constraint
option is ticket=&lt;ticket&gt;. Optional constraint options
are id=&lt;constraint-id&gt; and loss-pol&acirc; <br>
icy=fence/stop/freeze/demote.</p>

<p style="margin-top: 1em">ticket remove &lt;ticket&gt;
&lt;resource id&gt; <br>
Remove all ticket constraints with &lt;ticket&gt; from
&lt;resource id&gt;.</p>

<p style="margin-top: 1em">remove [constraint id]... <br>
Remove constraint(s) or constraint rules with the specified
id(s).</p>

<p style="margin-top: 1em">ref &lt;resource&gt;... <br>
List constraints referencing specified resource.</p>

<p style="margin-top: 1em">rule add &lt;constraint id&gt;
[id=&lt;rule id&gt;] [role=master|slave]
[score=&lt;score&gt;|score-attribute=&lt;attribute&gt;]
&lt;expression&gt; <br>
Add a rule to a constraint where the expression looks like
one of the following: <br>
defined|not_defined &lt;attribute&gt; <br>
&lt;attribute&gt; lt|gt|lte|gte|eq|ne
[string|integer|version] &lt;value&gt; <br>
date gt|lt &lt;date&gt; <br>
date in_range &lt;date&gt; to &lt;date&gt; <br>
date in_range &lt;date&gt; to duration &lt;duration
options&gt;... <br>
date-spec &lt;date spec options&gt;... <br>
&lt;expression&gt; and|or &lt;expression&gt; <br>
( &lt;expression&gt; ) <br>
where duration options and date spec options are: hours,
monthdays, weekdays, yeardays, months, weeks, years,
weekyears, moon If score is ommited it defaults to INFINITY.
<br>
If id is ommited one is generated from the constraint
id.</p>

<p style="margin-top: 1em">rule remove &lt;rule id&gt; <br>
Remove a rule if a rule id is specified, if rule is last
rule in its constraint, the constraint will be removed.</p>

<p style="margin-top: 1em">qdevice <br>
status &lt;device model&gt; [--full] [&lt;cluster name&gt;]
<br>
Show runtime status of specified model of quorum device
provider. Using --full will give more detailed output. If
&lt;cluster name&gt; is specified, only information about
the <br>
specified cluster will be displayed.</p>

<p style="margin-top: 1em">setup model &lt;device model&gt;
[--enable] [--start] <br>
Configure specified model of quorum device provider. Quorum
device then can be added to clusters by running &quot;pcs
quorum device add&quot; command in a cluster. --start will
<br>
also start the provider. --enable will configure the
provider to start on boot.</p>

<p style="margin-top: 1em">destroy &lt;device model&gt;
<br>
Disable and stop specified model of quorum device provider
and delete its configuration files.</p>

<p style="margin-top: 1em">start &lt;device model&gt; <br>
Start specified model of quorum device provider.</p>

<p style="margin-top: 1em">stop &lt;device model&gt; <br>
Stop specified model of quorum device provider.</p>

<p style="margin-top: 1em">kill &lt;device model&gt; <br>
Force specified model of quorum device provider to stop
(performs kill -9). Note that init system (e.g. systemd) can
detect that the qdevice is not running and start it <br>
again. If you want to stop the qdevice, run &quot;pcs
qdevice stop&quot; command.</p>

<p style="margin-top: 1em">enable &lt;device model&gt; <br>
Configure specified model of quorum device provider to start
on boot.</p>

<p style="margin-top: 1em">disable &lt;device model&gt;
<br>
Configure specified model of quorum device provider to not
start on boot.</p>

<p style="margin-top: 1em">quorum <br>
[config] <br>
Show quorum configuration.</p>

<p style="margin-top: 1em">status Show quorum runtime
status.</p>

<p style="margin-top: 1em">device add [&lt;generic
options&gt;] model &lt;device model&gt; [&lt;model
options&gt;] <br>
Add a quorum device to the cluster. Quorum device needs to
be created first by &quot;pcs qdevice setup&quot; command.
It is not possible to use more than one quorum device in a
<br>
cluster simultaneously. Generic options, model and model
options are all documented in corosync&rsquo;s
corosync-qdevice(8) man page.</p>

<p style="margin-top: 1em">device remove <br>
Remove a quorum device from the cluster.</p>

<p style="margin-top: 1em">device status [--full] <br>
Show quorum device runtime status. Using --full will give
more detailed output.</p>

<p style="margin-top: 1em">device update [&lt;generic
options&gt;] [model &lt;model options&gt;] <br>
Add/Change quorum device options. Generic options and model
options are all documented in corosync&rsquo;s
corosync-qdevice(8) man page. Requires the cluster to be
stopped.</p>

<p style="margin-top: 1em">WARNING: If you want to change
&quot;host&quot; option of qdevice model net, use &quot;pcs
quorum device remove&quot; and &quot;pcs quorum device
add&quot; commands to set up configuration properly <br>
unless old and new host is the same machine.</p>

<p style="margin-top: 1em">expected-votes &lt;votes&gt;
<br>
Set expected votes in the live cluster to specified value.
This only affects the live cluster, not changes any
configuration files.</p>

<p style="margin-top: 1em">unblock [--force] <br>
Cancel waiting for all nodes when establishing quorum.
Useful in situations where you know the cluster is
inquorate, but you are confident that the cluster should
proceed <br>
with resource management regardless. This command should
ONLY be used when nodes which the cluster is waiting for
have been confirmed to be powered off and to have no <br>
access to shared resources.</p>

<p style="margin-top: 1em">WARNING: If the nodes are not
actually powered off or they do have access to shared
resources, data corruption/cluster failure can occur. To
prevent accidental running of <br>
this command, --force or interactive user response is
required in order to proceed.</p>

<p style="margin-top: 1em">update [auto_tie_breaker=[0|1]]
[last_man_standing=[0|1]]
[last_man_standing_window=[&lt;time in ms&gt;]]
[wait_for_all=[0|1]] <br>
Add/Change quorum options. At least one option must be
specified. Options are documented in corosync&rsquo;s
votequorum(5) man page. Requires the cluster to be
stopped.</p>

<p style="margin-top: 1em">booth <br>
setup sites &lt;address&gt; &lt;address&gt;
[&lt;address&gt;...] [arbitrators &lt;address&gt; ...]
[--force] <br>
Write new booth configuration with specified sites and
arbitrators. Total number of peers (sites and arbitrators)
must be odd. When the configuration file already <br>
exists, command fails unless --force is specified.</p>

<p style="margin-top: 1em">destroy <br>
Remove booth configuration files.</p>

<p style="margin-top: 1em">ticket add &lt;ticket&gt;
[&lt;name&gt;=&lt;value&gt; ...] <br>
Add new ticket to the current configuration. Ticket options
are specified in booth manpage.</p>

<p style="margin-top: 1em">ticket remove &lt;ticket&gt;
<br>
Remove the specified ticket from the current
configuration.</p>

<p style="margin-top: 1em">config [&lt;node&gt;] <br>
Show booth configuration from the specified node or from the
current node if node not specified.</p>

<p style="margin-top: 1em">create ip &lt;address&gt; <br>
Make the cluster run booth service on the specified ip
address as a cluster resource. Typically this is used to run
booth site.</p>

<p style="margin-top: 1em">remove Remove booth resources
created by the &quot;pcs booth create&quot; command.</p>

<p style="margin-top: 1em">restart <br>
Restart booth resources created by the &quot;pcs booth
create&quot; command.</p>

<p style="margin-top: 1em">ticket grant &lt;ticket&gt;
[&lt;site address&gt;] <br>
Grant the ticket for the site specified by address. Site
address which has been specified with &rsquo;pcs booth
create&rsquo; command is used if &rsquo;site address&rsquo;
is omitted. Specify&acirc; <br>
ing site address is mandatory when running this command on
an arbitrator.</p>

<p style="margin-top: 1em">ticket revoke &lt;ticket&gt;
[&lt;site address&gt;] <br>
Revoke the ticket for the site specified by address. Site
address which has been specified with &rsquo;pcs booth
create&rsquo; command is used if &rsquo;site address&rsquo;
is omitted. Specify&acirc; <br>
ing site address is mandatory when running this command on
an arbitrator.</p>

<p style="margin-top: 1em">status Print current status of
booth on the local node.</p>

<p style="margin-top: 1em">pull &lt;node&gt; <br>
Pull booth configuration from the specified node.</p>

<p style="margin-top: 1em">sync [--skip-offline] <br>
Send booth configuration from the local node to all nodes in
the cluster.</p>

<p style="margin-top: 1em">enable Enable booth arbitrator
service.</p>

<p style="margin-top: 1em">disable <br>
Disable booth arbitrator service.</p>

<p style="margin-top: 1em">start Start booth arbitrator
service.</p>

<p style="margin-top: 1em">stop Stop booth arbitrator
service.</p>

<p style="margin-top: 1em">status <br>
[status] [--full | --hide-inactive] <br>
View all information about the cluster and resources (--full
provides more details, --hide-inactive hides inactive
resources).</p>

<p style="margin-top: 1em">resources [&lt;resource id&gt; |
--full | --groups | --hide-inactive] <br>
Show all currently configured resources or if a resource is
specified show the options for the configured resource. If
--full is specified, all configured resource <br>
options will be displayed. If --groups is specified, only
show groups (and their resources). If --hide-inactive is
specified, only show active resources.</p>

<p style="margin-top: 1em">groups View currently configured
groups and their resources.</p>

<p style="margin-top: 1em">cluster <br>
View current cluster status.</p>

<p style="margin-top: 1em">corosync <br>
View current membership information as seen by corosync.</p>

<p style="margin-top: 1em">quorum View current quorum
status.</p>

<p style="margin-top: 1em">qdevice &lt;device model&gt;
[--full] [&lt;cluster name&gt;] <br>
Show runtime status of specified model of quorum device
provider. Using --full will give more detailed output. If
&lt;cluster name&gt; is specified, only information about
the <br>
specified cluster will be displayed.</p>

<p style="margin-top: 1em">nodes [corosync|both|config]
<br>
View current status of nodes from pacemaker. If
&rsquo;corosync&rsquo; is specified, print nodes currently
configured in corosync, if &rsquo;both&rsquo; is specified,
print nodes from both <br>
corosync &amp; pacemaker. If &rsquo;config&rsquo; is
specified, print nodes from corosync &amp; pacemaker
configuration.</p>

<p style="margin-top: 1em">pcsd [&lt;node&gt;] ... <br>
Show the current status of pcsd on the specified nodes. When
no nodes are specified, status of all nodes is
displayed.</p>

<p style="margin-top: 1em">xml View xml version of status
(output from crm_mon -r -1 -X).</p>

<p style="margin-top: 1em">config <br>
[show] View full cluster configuration.</p>

<p style="margin-top: 1em">backup [filename] <br>
Creates the tarball containing the cluster configuration
files. If filename is not specified the standard output will
be used.</p>

<p style="margin-top: 1em">restore [--local] [filename]
<br>
Restores the cluster configuration files on all nodes from
the backup. If filename is not specified the standard input
will be used. If --local is specified only the <br>
files on the current node will be restored.</p>

<p style="margin-top: 1em">checkpoint <br>
List all available configuration checkpoints.</p>

<p style="margin-top: 1em">checkpoint view
&lt;checkpoint_number&gt; <br>
Show specified configuration checkpoint.</p>

<p style="margin-top: 1em">checkpoint restore
&lt;checkpoint_number&gt; <br>
Restore cluster configuration to specified checkpoint.</p>

<p style="margin-top: 1em">import-cman
output=&lt;filename&gt; [input=&lt;filename&gt;]
[--interactive] [output-format=corosync.conf|cluster.conf]
[dist=&lt;dist&gt;] <br>
Converts RHEL 6 (CMAN) cluster configuration to Pacemaker
cluster configuration. Converted configuration will be saved
to &rsquo;output&rsquo; file. To send the configuration to
the <br>
cluster nodes the &rsquo;pcs config restore&rsquo; command
can be used. If --interactive is specified you will be
prompted to solve incompatibilities manually. If no input is
speci&acirc; <br>
fied /etc/cluster/cluster.conf will be used. You can force
to create output containing either cluster.conf or
corosync.conf using the output-format option. Optionally
<br>
you can specify output version by setting &rsquo;dist&rsquo;
option e. g. rhel,6.8 or redhat,7.3 or debian,7 or
ubuntu,trusty. You can get the list of supported dist values
by run&acirc; <br>
ning the &quot;clufter --list-dists&quot; command. If
&rsquo;dist&rsquo; is not specified, it defaults to this
node&rsquo;s version if that matches output-format,
otherwise redhat,6.7 is used for <br>
cluster.conf and redhat,7.1 is used for corosync.conf.</p>

<p style="margin-top: 1em">import-cman
output=&lt;filename&gt; [input=&lt;filename&gt;]
[--interactive]
output-format=pcs-commands|pcs-commands-verbose
[dist=&lt;dist&gt;] <br>
Converts RHEL 6 (CMAN) cluster configuration to a list of
pcs commands which recreates the same cluster as Pacemaker
cluster when executed. Commands will be saved to <br>
&rsquo;output&rsquo; file. For other options see above.</p>

<p style="margin-top: 1em">export
pcs-commands|pcs-commands-verbose [output=&lt;filename&gt;]
[dist=&lt;dist&gt;] <br>
Creates a list of pcs commands which upon execution
recreates the current cluster running on this node. Commands
will be saved to &rsquo;output&rsquo; file or written to
stdout if <br>
&rsquo;output&rsquo; is not specified. Use pcs-commands to
get a simple list of commands, whereas pcs-commands-verbose
creates a list including comments and debug messages.
Option&acirc; <br>
ally specify output version by setting &rsquo;dist&rsquo;
option e. g. rhel,6.8 or redhat,7.3 or debian,7 or
ubuntu,trusty. You can get the list of supported dist values
by running <br>
the &quot;clufter --list-dists&quot; command. If
&rsquo;dist&rsquo; is not specified, it defaults to this
node&rsquo;s version.</p>

<p style="margin-top: 1em">pcsd <br>
certkey &lt;certificate file&gt; &lt;key file&gt; <br>
Load custom certificate and key files for use in pcsd.</p>

<p style="margin-top: 1em">sync-certificates <br>
Sync pcsd certificates to all nodes found from current
corosync.conf file (cluster.conf on systems running Corosync
1.x). WARNING: This will restart pcsd daemon on the <br>
nodes.</p>

<p style="margin-top: 1em">clear-auth [--local] [--remote]
<br>
Removes all system tokens which allow pcs/pcsd on the
current system to authenticate with remote pcs/pcsd
instances and vice-versa. After this command is run this
node <br>
will need to be re-authenticated with other nodes (using
&rsquo;pcs cluster auth&rsquo;). Using --local only removes
tokens used by local pcs (and pcsd if root) to connect to
other <br>
pcsd instances, using --remote clears authentication tokens
used by remote systems to connect to the local pcsd
instance.</p>

<p style="margin-top: 1em">node <br>
attribute [[&lt;node&gt;] [--name &lt;name&gt;] |
&lt;node&gt; &lt;name&gt;=&lt;value&gt; ...] <br>
Manage node attributes. If no parameters are specified, show
attributes of all nodes. If one parameter is specified, show
attributes of specified node. If --name is <br>
specified, show specified attribute&rsquo;s value from all
nodes. If more parameters are specified, set attributes of
specified node. Attributes can be removed by setting an <br>
attribute without a value.</p>

<p style="margin-top: 1em">maintenance [--all] |
[&lt;node&gt;]... <br>
Put specified node(s) into maintenance mode, if no node or
options are specified the current node will be put into
maintenance mode, if --all is specified all nodes will <br>
be put into maintenace mode.</p>

<p style="margin-top: 1em">unmaintenance [--all] |
[&lt;node&gt;]... <br>
Remove node(s) from maintenance mode, if no node or options
are specified the current node will be removed from
maintenance mode, if --all is specified all nodes will be
<br>
removed from maintenance mode.</p>

<p style="margin-top: 1em">standby [--all | &lt;node&gt;]
[--wait[=n]] <br>
Put specified node into standby mode (the node specified
will no longer be able to host resources), if no node or
options are specified the current node will be put into <br>
standby mode, if --all is specified all nodes will be put
into standby mode. If --wait is specified, pcs will wait up
to &rsquo;n&rsquo; seconds for the node(s) to be put into
<br>
standby mode and then return 0 on success or 1 if the
operation not succeeded yet. If &rsquo;n&rsquo; is not
specified it defaults to 60 minutes.</p>

<p style="margin-top: 1em">unstandby [--all | &lt;node&gt;]
[--wait[=n]] <br>
Remove node from standby mode (the node specified will now
be able to host resources), if no node or options are
specified the current node will be removed from standby <br>
mode, if --all is specified all nodes will be removed from
standby mode. If --wait is specified, pcs will wait up to
&rsquo;n&rsquo; seconds for the node(s) to be removed from
<br>
standby mode and then return 0 on success or 1 if the
operation not succeeded yet. If &rsquo;n&rsquo; is not
specified it defaults to 60 minutes.</p>

<p style="margin-top: 1em">utilization [[&lt;node&gt;]
[--name &lt;name&gt;] | &lt;node&gt;
&lt;name&gt;=&lt;value&gt; ...] <br>
Add specified utilization options to specified node. If node
is not specified, shows utilization of all nodes. If --name
is specified, shows specified utilization value <br>
from all nodes. If utilization options are not specified,
shows utilization of specified node. Utilization option
should be in format name=value, value has to be integer.
<br>
Options may be removed by setting an option without a value.
Example: pcs node utilization node1 cpu=4 ram=</p>

<p style="margin-top: 1em">alert <br>
[config|show] <br>
Show all configured alerts.</p>

<p style="margin-top: 1em">create path=&lt;path&gt;
[id=&lt;alert-id&gt;] [description=&lt;description&gt;]
[options [&lt;option&gt;=&lt;value&gt;]...] [meta
[&lt;meta-option&gt;=&lt;value&gt;]...] <br>
Define an alert handler with specified path. Id will be
automatically generated if it is not specified.</p>

<p style="margin-top: 1em">update &lt;alert-id&gt;
[path=&lt;path&gt;] [description=&lt;description&gt;]
[options [&lt;option&gt;=&lt;value&gt;]...] [meta
[&lt;meta-option&gt;=&lt;value&gt;]...] <br>
Update existing alert handler with specified id.</p>

<p style="margin-top: 1em">remove &lt;alert-id&gt; <br>
Remove alert handler with specified id.</p>

<p style="margin-top: 1em">recipient add &lt;alert-id&gt;
value=&lt;recipient-value&gt; [id=&lt;recipient-id&gt;]
[description=&lt;description&gt;] [options
[&lt;option&gt;=&lt;value&gt;]...] [meta
[&lt;meta-option&gt;=&lt;value&gt;]...] <br>
Add new recipient to specified alert handler.</p>

<p style="margin-top: 1em">recipient update
&lt;recipient-id&gt; [value=&lt;recipient-value&gt;]
[description=&lt;description&gt;] [options
[&lt;option&gt;=&lt;value&gt;]...] [meta
[&lt;meta-option&gt;=&lt;value&gt;]...] <br>
Update existing recipient identified by it&rsquo;s id.</p>

<p style="margin-top: 1em">recipient remove
&lt;recipient-id&gt; <br>
Remove specified recipient.</p>

<p style="margin-top: 1em">EXAMPLES <br>
Show all resources <br>
# pcs resource show</p>

<p style="margin-top: 1em">Show options specific to the
&rsquo;VirtualIP&rsquo; resource <br>
# pcs resource show VirtualIP</p>

<p style="margin-top: 1em">Create a new resource called
&rsquo;VirtualIP&rsquo; with options <br>
# pcs resource create VirtualIP ocf:heartbeat:IPaddr2
ip=192.168.0.99 cidr_netmask=32 nic=eth2 op monitor
interval=30s</p>

<p style="margin-top: 1em">Create a new resource called
&rsquo;VirtualIP&rsquo; with options <br>
# pcs resource create VirtualIP IPaddr2 ip=192.168.0.99
cidr_netmask=32 nic=eth2 op monitor interval=30s</p>

<p style="margin-top: 1em">Change the ip address of
VirtualIP and remove the nic option <br>
# pcs resource update VirtualIP ip=192.168.0.98 nic=</p>

<p style="margin-top: 1em">Delete the VirtualIP resource
<br>
# pcs resource delete VirtualIP</p>

<p style="margin-top: 1em">Create the MyStonith stonith
fence_virt device which can fence host &rsquo;f1&rsquo; <br>
# pcs stonith create MyStonith fence_virt
pcmk_host_list=f1</p>

<p style="margin-top: 1em">Set the stonith-enabled property
to false on the cluster (which disables stonith) <br>
# pcs property set stonith-enabled=false</p>

<p style="margin-top: 1em">pcs 0.9.152 June 2016 PCS(8)</p>
<hr>
</body>
</html>
