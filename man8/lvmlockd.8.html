<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 19:13:09 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>LVMLOCKD(8) LVMLOCKD(8)</p>

<p style="margin-top: 1em">NAME <br>
lvmlockd &acirc; LVM locking daemon</p>

<p style="margin-top: 1em">DESCRIPTION <br>
LVM commands use lvmlockd to coordinate access to shared
storage. <br>
When LVM is used on devices shared by multiple hosts, locks
will:</p>

<p style="margin-top: 1em">&Acirc;&middot; coordinate
reading and writing of LVM metadata <br>
&Acirc;&middot; validate caching of LVM metadata <br>
&Acirc;&middot; prevent concurrent activation of logical
volumes</p>

<p style="margin-top: 1em">lvmlockd uses an external lock
manager to perform basic locking. <br>
Lock manager (lock type) options are:</p>

<p style="margin-top: 1em">&Acirc;&middot; sanlock: places
locks on disk within LVM storage. <br>
&Acirc;&middot; dlm: uses network communication and a
cluster manager.</p>

<p style="margin-top: 1em">OPTIONS <br>
lvmlockd [options]</p>

<p style="margin-top: 1em">For default settings, see
lvmlockd -h.</p>

<p style="margin-top: 1em">--help | -h <br>
Show this help information.</p>

<p style="margin-top: 1em">--version | -V <br>
Show version of lvmlockd.</p>

<p style="margin-top: 1em">--test | -T <br>
Test mode, do not call lock manager.</p>

<p style="margin-top: 1em">--foreground | -f <br>
Don&rsquo;t fork.</p>

<p style="margin-top: 1em">--daemon-debug | -D <br>
Don&rsquo;t fork and print debugging to stdout.</p>

<p style="margin-top: 1em">--pid-file | -p path <br>
Set path to the pid file.</p>

<p style="margin-top: 1em">--socket-path | -s path <br>
Set path to the socket to listen on.</p>

<p style="margin-top: 1em">--syslog-priority | -S
err|warning|debug <br>
Write log messages from this level up to syslog.</p>

<p style="margin-top: 1em">--gl-type | -g sanlock|dlm <br>
Set global lock type to be sanlock or dlm.</p>

<p style="margin-top: 1em">--host-id | -i num <br>
Set the local sanlock host id.</p>

<p style="margin-top: 1em">--host-id-file | -F path <br>
A file containing the local sanlock host_id.</p>

<p style="margin-top: 1em">--sanlock-timeout | -o seconds
<br>
Override the default sanlock I/O timeout.</p>

<p style="margin-top: 1em">--adopt | -A 0|1 <br>
Adopt locks from a previous instance of lvmlockd.</p>

<p style="margin-top: 1em">USAGE <br>
Initial set up <br>
Using LVM with lvmlockd for the first time includes some
one-time set up steps:</p>

<p style="margin-top: 1em">1. choose a lock manager <br>
dlm <br>
If dlm (or corosync) are already being used by other cluster
software, then select dlm. dlm uses corosync which requires
additional configuration beyond the scope of this
docu&acirc; <br>
ment. See corosync and dlm documentation for instructions on
configuration, setup and usage.</p>

<p style="margin-top: 1em">sanlock <br>
Choose sanlock if dlm/corosync are not otherwise required.
sanlock does not depend on any clustering software or
configuration.</p>

<p style="margin-top: 1em">2. configure hosts to use
lvmlockd <br>
On all hosts running lvmlockd, configure lvm.conf: <br>
locking_type = 1 <br>
use_lvmlockd = 1</p>

<p style="margin-top: 1em">sanlock <br>
Assign each host a unique host_id in the range 1-2000 by
setting <br>
/etc/lvm/lvmlocal.conf local/host_id</p>

<p style="margin-top: 1em">3. start lvmlockd <br>
Use a service/init file if available, or just run
&quot;lvmlockd&quot;.</p>

<p style="margin-top: 1em">4. start lock manager <br>
sanlock <br>
systemctl start wdmd sanlock</p>

<p style="margin-top: 1em">dlm <br>
Follow external clustering documentation when applicable,
otherwise: <br>
systemctl start corosync dlm</p>

<p style="margin-top: 1em">5. create VG on shared devices
<br>
vgcreate --shared &lt;vgname&gt; &lt;devices&gt;</p>

<p style="margin-top: 1em">The shared option sets the VG
lock type to sanlock or dlm depending on which lock manager
is running. LVM commands will perform locking for the VG
using lvmlockd. lvmlockd will <br>
use the chosen lock manager.</p>

<p style="margin-top: 1em">6. start VG on all hosts <br>
vgchange --lock-start</p>

<p style="margin-top: 1em">lvmlockd requires shared VGs to
be started before they are used. This is a lock manager
operation to start (join) the VG lockspace, and it may take
some time. Until the start <br>
completes, locks for the VG are not available. LVM commands
are allowed to read the VG while start is in progress. (An
init/unit file can also be used to start VGs.)</p>

<p style="margin-top: 1em">7. create and activate LVs <br>
Standard lvcreate and lvchange commands are used to create
and activate LVs in a shared VG.</p>

<p style="margin-top: 1em">An LV activated exclusively on
one host cannot be activated on another. When multiple hosts
need to use the same LV concurrently, the LV can be
activated with a shared lock (see <br>
lvchange options -aey vs -asy.) (Shared locks are disallowed
for certain LV types that cannot be used from multiple
hosts.)</p>

<p style="margin-top: 1em">Normal start up and shut down
<br>
After initial set up, start up and shut down include the
following general steps. They can be performed manually or
using the system service manager.</p>

<p style="margin-top: 1em">&Acirc;&middot; start lvmetad
<br>
&Acirc;&middot; start lvmlockd <br>
&Acirc;&middot; start lock manager <br>
&Acirc;&middot; vgchange --lock-start <br>
&Acirc;&middot; activate LVs in shared VGs</p>

<p style="margin-top: 1em">The shut down sequence is the
reverse:</p>

<p style="margin-top: 1em">&Acirc;&middot; deactivate LVs
in shared VGs <br>
&Acirc;&middot; vgchange --lock-stop <br>
&Acirc;&middot; stop lock manager <br>
&Acirc;&middot; stop lvmlockd <br>
&Acirc;&middot; stop lvmetad</p>

<p style="margin-top: 1em">TOPICS <br>
VG access control <br>
The following terms are used to describe different forms of
VG access control.</p>

<p style="margin-top: 1em">lockd VG</p>

<p style="margin-top: 1em">A &quot;lockd VG&quot; is a
shared VG that has a &quot;lock type&quot; of dlm or
sanlock. Using it requires lvmlockd. These VGs exist on
shared storage that is visible to multiple hosts. LVM
com&acirc; <br>
mands use lvmlockd to perform locking for these VGs when
they are used.</p>

<p style="margin-top: 1em">If the lock manager for the lock
type is not available (e.g. not started or failed), lvmlockd
is unable to acquire locks for LVM commands. LVM commands
that only read the VG <br>
will generally be allowed to continue without locks in this
case (with a warning). Commands to modify or activate the VG
will fail without the necessary locks.</p>

<p style="margin-top: 1em">local VG</p>

<p style="margin-top: 1em">A &quot;local VG&quot; is meant
to be used by a single host. It has no lock type or lock
type &quot;none&quot;. LVM commands and lvmlockd do not
perform locking for these VGs. A local VG typi&acirc; <br>
cally exists on local (non-shared) devices and cannot be
used concurrently from different hosts.</p>

<p style="margin-top: 1em">If a local VG does exist on
shared devices, it should be owned by a single host by
having its system ID set, see lvmsystemid(7). Only the host
with a matching system ID can use <br>
the local VG. A VG with no lock type and no system ID should
be excluded from all but one host using lvm.conf filters.
Without any of these protections, a local VG on shared <br>
devices can be easily damaged or destroyed.</p>

<p style="margin-top: 1em">clvm VG</p>

<p style="margin-top: 1em">A &quot;clvm VG&quot; is a VG on
shared storage (like a lockd VG) that requires clvmd for
clustering. See below for converting a clvm VG to a lockd
VG.</p>

<p style="margin-top: 1em">lockd VGs from hosts not using
lvmlockd <br>
Only hosts that use lockd VGs should be configured to run
lvmlockd. However, shared devices used by lockd VGs may be
visible from hosts not using lvmlockd. From a host not <br>
using lvmlockd, visible lockd VGs are ignored in the same
way as foreign VGs (see lvmsystemid(7).)</p>

<p style="margin-top: 1em">The --shared option for
reporting and display commands causes lockd VGs to be
displayed on a host not using lvmlockd, like the --foreign
option does for foreign VGs.</p>

<p style="margin-top: 1em">vgcreate comparison <br>
The type of VG access control is specified in the vgcreate
command. See vgcreate(8) for all vgcreate options.</p>

<p style="margin-top: 1em">vgcreate &lt;vgname&gt;
&lt;devices&gt;</p>

<p style="margin-top: 1em">&Acirc;&middot; Creates a local
VG with the local system ID when neither lvmlockd nor clvm
are configured.</p>

<p style="margin-top: 1em">&Acirc;&middot; Creates a local
VG with the local system ID when lvmlockd is configured.</p>

<p style="margin-top: 1em">&Acirc;&middot; Creates a clvm
VG when clvm is configured.</p>

<p style="margin-top: 1em">vgcreate --shared &lt;vgname&gt;
&lt;devices&gt;</p>

<p style="margin-top: 1em">&Acirc;&middot; Requires
lvmlockd to be configured and running.</p>

<p style="margin-top: 1em">&Acirc;&middot; Creates a lockd
VG with lock type sanlock|dlm depending on which lock
manager is running.</p>

<p style="margin-top: 1em">&Acirc;&middot; LVM commands
request locks from lvmlockd to use the VG.</p>

<p style="margin-top: 1em">&Acirc;&middot; lvmlockd obtains
locks from the selected lock manager.</p>

<p style="margin-top: 1em">vgcreate -c|--clustered y
&lt;vgname&gt; &lt;devices&gt;</p>

<p style="margin-top: 1em">&Acirc;&middot; Requires clvm to
be configured and running.</p>

<p style="margin-top: 1em">&Acirc;&middot; Creates a clvm
VG with the &quot;clustered&quot; flag.</p>

<p style="margin-top: 1em">&Acirc;&middot; LVM commands
request locks from clvmd to use the VG.</p>

<p style="margin-top: 1em">creating the first sanlock VG
<br>
Creating the first sanlock VG is not protected by locking
and requires special attention. This is because sanlock
locks exist within the VG, so they are not available until
the <br>
VG exists. The first sanlock VG will contain the
&quot;global lock&quot;.</p>

<p style="margin-top: 1em">&Acirc;&middot; The first
vgcreate command needs to be given the path to a device that
has not yet been initialized with pvcreate. The pvcreate
initialization will be done by vgcreate. This <br>
is because the pvcreate command requires the global lock,
which will not be available until after the first sanlock VG
is created.</p>

<p style="margin-top: 1em">&Acirc;&middot; While running
vgcreate for the first sanlock VG, ensure that the device
being used is not used by another LVM command. Allocation of
shared devices is usually protected by the <br>
global lock, but this cannot be done for the first sanlock
VG which will hold the global lock.</p>

<p style="margin-top: 1em">&Acirc;&middot; While running
vgcreate for the first sanlock VG, ensure that the VG name
being used is not used by another LVM command. Uniqueness of
VG names is usually ensured by the global <br>
lock.</p>

<p style="margin-top: 1em">&Acirc;&middot; Because the
first sanlock VG will contain the global lock, this VG needs
to be accessible to all hosts that will use sanlock shared
VGs. All hosts will need to use the global <br>
lock from the first sanlock VG.</p>

<p style="margin-top: 1em">See below for more information
about managing the sanlock global lock.</p>

<p style="margin-top: 1em">using lockd VGs <br>
There are some special considerations when using lockd
VGs.</p>

<p style="margin-top: 1em">When use_lvmlockd is first
enabled in lvm.conf, and before the first lockd VG is
created, no global lock will exist. In this initial state,
LVM commands try and fail to acquire <br>
the global lock, producing a warning, and some commands are
disallowed. Once the first lockd VG is created, the global
lock will be available, and LVM will be fully
operational.</p>

<p style="margin-top: 1em">When a new lockd VG is created,
its lockspace is automatically started on the host that
creates it. Other hosts need to run &rsquo;vgchange
--lock-start&rsquo; to start the new VG before <br>
they can use it.</p>

<p style="margin-top: 1em">From the &rsquo;vgs&rsquo;
command, lockd VGs are indicated by &quot;s&quot; (for
shared) in the sixth attr field. The specific lock type and
lock args for a lockd VG can be displayed with &rsquo;vgs
<br>
-o+locktype,lockargs&rsquo;.</p>

<p style="margin-top: 1em">lockd VGs need to be
&quot;started&quot; and &quot;stopped&quot;, unlike other
types of VGs. See the following section for a full
description of starting and stopping.</p>

<p style="margin-top: 1em">vgremove of a lockd VG will fail
if other hosts have the VG started. Run vgchange --lock-stop
&lt;vgname&gt; on all other hosts before vgremove. (It may
take several seconds before <br>
vgremove recognizes that all hosts have stopped a sanlock
VG.)</p>

<p style="margin-top: 1em">starting and stopping VGs <br>
Starting a lockd VG (vgchange --lock-start) causes the lock
manager to start (join) the lockspace for the VG on the host
where it is run. This makes locks for the VG available <br>
to LVM commands on the host. Before a VG is started, only
LVM commands that read/display the VG are allowed to
continue without locks (and with a warning).</p>

<p style="margin-top: 1em">Stopping a lockd VG (vgchange
--lock-stop) causes the lock manager to stop (leave) the
lockspace for the VG on the host where it is run. This makes
locks for the VG inaccessible <br>
to the host. A VG cannot be stopped while it has active
LVs.</p>

<p style="margin-top: 1em">When using the lock type
sanlock, starting a VG can take a long time (potentially
minutes if the host was previously shut down without cleanly
stopping the VG.)</p>

<p style="margin-top: 1em">A lockd VG can be started after
all the following are true: <br>
&Acirc;&middot; lvmlockd is running <br>
&Acirc;&middot; the lock manager is running <br>
&Acirc;&middot; the VG is visible to the system</p>

<p style="margin-top: 1em">A lockd VG can be stopped if all
LVs are deactivated.</p>

<p style="margin-top: 1em">All lockd VGs can be
started/stopped using: <br>
vgchange --lock-start <br>
vgchange --lock-stop</p>

<p style="margin-top: 1em">Individual VGs can be
started/stopped using: <br>
vgchange --lock-start &lt;vgname&gt; ... <br>
vgchange --lock-stop &lt;vgname&gt; ...</p>

<p style="margin-top: 1em">To make vgchange not wait for
start to complete: <br>
vgchange --lock-start --lock-opt nowait ...</p>

<p style="margin-top: 1em">lvmlockd can be asked directly
to stop all lockspaces: <br>
lvmlockctl --stop-lockspaces</p>

<p style="margin-top: 1em">To start only selected lockd
VGs, use the lvm.conf activation/lock_start_list. When
defined, only VG names in this list are started by vgchange.
If the list is not defined (the <br>
default), all visible lockd VGs are started. To start only
&quot;vg1&quot;, use the following lvm.conf
configuration:</p>

<p style="margin-top: 1em">activation { <br>
lock_start_list = [ &quot;vg1&quot; ] <br>
... <br>
}</p>

<p style="margin-top: 1em">automatic starting and automatic
activation <br>
Scripts or programs on a host that automatically start VGs
will use the &quot;auto&quot; option to indicate that the
command is being run automatically by the system:</p>

<p style="margin-top: 1em">vgchange --lock-start --lock-opt
auto [&lt;vgname&gt; ...]</p>

<p style="margin-top: 1em">Without any additional
configuration, including the &quot;auto&quot; option has no
effect; all VGs are started unless restricted by
lock_start_list.</p>

<p style="margin-top: 1em">However, when the lvm.conf
activation/auto_lock_start_list is defined, the auto start
command performs an additional filtering phase to all VGs
being started, testing each VG <br>
name against the auto_lock_start_list. The
auto_lock_start_list defines lockd VGs that will be started
by the auto start command. Visible lockd VGs not included in
the list are <br>
ignored by the auto start command. If the list is undefined,
all VG names pass this filter. (The lock_start_list is also
still used to filter all VGs.)</p>

<p style="margin-top: 1em">The auto_lock_start_list allows
a user to select certain lockd VGs that should be
automatically started by the system (or indirectly, those
that should not).</p>

<p style="margin-top: 1em">To use auto activation of lockd
LVs (see auto_activation_volume_list), auto starting of the
corresponding lockd VGs is necessary.</p>

<p style="margin-top: 1em">internal command locking <br>
To optimize the use of LVM with lvmlockd, be aware of the
three kinds of locks and when they are used:</p>

<p style="margin-top: 1em">GL lock</p>

<p style="margin-top: 1em">The global lock (GL lock) is
associated with global information, which is information not
isolated to a single VG. This includes:</p>

<p style="margin-top: 1em">&Acirc;&middot; The global VG
namespace. <br>
&Acirc;&middot; The set of orphan PVs and unused devices.
<br>
&Acirc;&middot; The properties of orphan PVs, e.g. PV
size.</p>

<p style="margin-top: 1em">The global lock is used in
shared mode by commands that read this information, or in
exclusive mode by commands that change it.</p>

<p style="margin-top: 1em">The command &rsquo;vgs&rsquo;
acquires the global lock in shared mode because it reports
the list of all VG names.</p>

<p style="margin-top: 1em">The vgcreate command acquires
the global lock in exclusive mode because it creates a new
VG name, and it takes a PV from the list of unused PVs.</p>

<p style="margin-top: 1em">When an LVM command is given a
tag argument, or uses select, it must read all VGs to match
the tag or selection, which causes the global lock to be
acquired.</p>

<p style="margin-top: 1em">VG lock</p>

<p style="margin-top: 1em">A VG lock is associated with
each VG. The VG lock is acquired in shared mode to read the
VG and in exclusive mode to change the VG (modify the VG
metadata or activate LVs). <br>
This lock serializes access to a VG with all other LVM
commands accessing the VG from all hosts.</p>

<p style="margin-top: 1em">The command &rsquo;vgs&rsquo;
will not only acquire the GL lock to read the list of all VG
names, but will acquire the VG lock for each VG prior to
reading it.</p>

<p style="margin-top: 1em">The command &rsquo;vgs
&lt;vgname&gt;&rsquo; does not acquire the GL lock (it does
not need the list of all VG names), but will acquire the VG
lock on each VG name argument.</p>

<p style="margin-top: 1em">LV lock</p>

<p style="margin-top: 1em">An LV lock is acquired before
the LV is activated, and is released after the LV is
deactivated. If the LV lock cannot be acquired, the LV is
not activated. LV locks are persis&acirc; <br>
tent and remain in place after the activation command is
done. GL and VG locks are transient, and are held only while
an LVM command is running.</p>

<p style="margin-top: 1em">lock retries</p>

<p style="margin-top: 1em">If a request for a GL or VG lock
fails due to a lock conflict with another host, lvmlockd
automatically retries for a short time before returning a
failure to the LVM command. <br>
If those retries are insufficient, the LVM command will
retry the entire lock request a number of times specified by
global/lvmlockd_lock_retries before failing. If a request
<br>
for an LV lock fails due to a lock conflict, the command
fails immediately.</p>

<p style="margin-top: 1em">managing the global lock in
sanlock VGs <br>
The global lock exists in one of the sanlock VGs. The first
sanlock VG created will contain the global lock. Subsequent
sanlock VGs will each contain disabled global locks that
<br>
can be enabled later if necessary.</p>

<p style="margin-top: 1em">The VG containing the global
lock must be visible to all hosts using sanlock VGs. This
can be a reason to create a small sanlock VG, visible to all
hosts, and dedicated to just <br>
holding the global lock. While not required, this strategy
can help to avoid difficulty in the future if VGs are moved
or removed.</p>

<p style="margin-top: 1em">The vgcreate command typically
acquires the global lock, but in the case of the first
sanlock VG, there will be no global lock to acquire until
the first vgcreate is complete. <br>
So, creating the first sanlock VG is a special case that
skips the global lock.</p>

<p style="margin-top: 1em">vgcreate for a sanlock VG
determines it is the first one to exist if no other sanlock
VGs are visible. It is possible that other sanlock VGs do
exist but are not visible on the <br>
host running vgcreate. In this case, vgcreate would create a
new sanlock VG with the global lock enabled. When the other
VG containing a global lock appears, lvmlockd will see <br>
more than one VG with a global lock enabled, and LVM
commands will report that there are duplicate global
locks.</p>

<p style="margin-top: 1em">If the situation arises where
more than one sanlock VG contains a global lock, the global
lock should be manually disabled in all but one of them with
the command:</p>

<p style="margin-top: 1em">lvmlockctl --gl-disable
&lt;vgname&gt;</p>

<p style="margin-top: 1em">(The one VG with the global lock
enabled must be visible to all hosts.)</p>

<p style="margin-top: 1em">An opposite problem can occur if
the VG holding the global lock is removed. In this case, no
global lock will exist following the vgremove, and
subsequent LVM commands will fail <br>
to acquire it. In this case, the global lock needs to be
manually enabled in one of the remaining sanlock VGs with
the command:</p>

<p style="margin-top: 1em">lvmlockctl --gl-enable
&lt;vgname&gt;</p>

<p style="margin-top: 1em">A small sanlock VG dedicated to
holding the global lock can avoid the case where the GL lock
must be manually enabled after a vgremove.</p>

<p style="margin-top: 1em">internal lvmlock LV <br>
A sanlock VG contains a hidden LV called &quot;lvmlock&quot;
that holds the sanlock locks. vgreduce cannot yet remove the
PV holding the lvmlock LV. To remove this PV, change the VG
lock <br>
type to &quot;none&quot;, run vgreduce, then change the VG
lock type back to &quot;sanlock&quot;. Similarly, pvmove
cannot be used on a PV used by the lvmlock LV.</p>

<p style="margin-top: 1em">To place the lvmlock LV on a
specific device, create the VG with only that device, then
use vgextend to add other devices.</p>

<p style="margin-top: 1em">shared LVs <br>
When an LV is used concurrently from multiple hosts (e.g. by
a multi-host/cluster application or file system), the LV can
be activated on multiple hosts concurrently using a <br>
shared lock.</p>

<p style="margin-top: 1em">To activate the LV with a shared
lock: lvchange -asy vg/lv.</p>

<p style="margin-top: 1em">With lvmlockd, an unspecified
activation mode is always exclusive, i.e. -ay defaults to
-aey.</p>

<p style="margin-top: 1em">If the LV type does not allow
the LV to be used concurrently from multiple hosts, then a
shared activation lock is not allowed and the lvchange
command will report an error. LV <br>
types that cannot be used concurrently from multiple hosts
include thin, cache, raid, mirror, and snapshot.</p>

<p style="margin-top: 1em">lvextend on LV with shared locks
is not yet allowed. The LV must be deactivated, or activated
exclusively to run lvextend.</p>

<p style="margin-top: 1em">recover from lost PV holding
sanlock locks <br>
The general approach is to change the VG lock type to
&quot;none&quot;, and then change the lock type back to
&quot;sanlock&quot;. This recreates the internal lvmlock LV
and the necessary locks on <br>
it. Additional steps may be required to deal with the
missing PV.</p>

<p style="margin-top: 1em">locking system failures <br>
lvmlockd failure</p>

<p style="margin-top: 1em">If lvmlockd fails or is killed
while holding locks, the locks are orphaned in the lock
manager. lvmlockd can be restarted with an option to adopt
locks in the lock manager that <br>
had been held by the previous instance.</p>

<p style="margin-top: 1em">dlm/corosync failure</p>

<p style="margin-top: 1em">If dlm or corosync fail, the
clustering system will fence the host using a method
configured within the dlm/corosync clustering
environment.</p>

<p style="margin-top: 1em">LVM commands on other hosts will
be blocked from acquiring any locks until the dlm/corosync
recovery process is complete.</p>

<p style="margin-top: 1em">sanlock lease storage
failure</p>

<p style="margin-top: 1em">If the PV under a sanlock
VG&rsquo;s lvmlock LV is disconnected, unresponsive or too
slow, sanlock cannot renew the lease for the VG&rsquo;s
locks. After some time, the lease will expire, <br>
and locks that the host owns in the VG can be acquired by
other hosts. The VG must be forcibly deactivated on the host
with the expiring lease before other hosts can acquire its
<br>
locks.</p>

<p style="margin-top: 1em">When the sanlock daemon detects
that the lease storage is lost, it runs the command
lvmlockctl --kill &lt;vgname&gt;. This command emits a
syslog message stating that lease storage is <br>
lost for the VG and LVs must be immediately deactivated.</p>

<p style="margin-top: 1em">If no LVs are active in the VG,
then the lockspace with an expiring lease will be removed,
and errors will be reported when trying to use the VG. Use
the lvmlockctl --drop com&acirc; <br>
mand to clear the stale lockspace from lvmlockd.</p>

<p style="margin-top: 1em">If the VG has active LVs when
the lock storage is lost, the LVs must be quickly
deactivated before the lockspace lease expires. After all
LVs are deactivated, run lvmlockctl <br>
--drop &lt;vgname&gt; to clear the expiring lockspace from
lvmlockd. If all LVs in the VG are not deactivated within
about 40 seconds, sanlock will reset the host using the
local <br>
watchdog. The machine reset is effectively a severe form of
&quot;deactivating&quot; LVs before they can be activated on
other hosts. The reset is considered a better alternative
than <br>
having LVs used by multiple hosts at once, which could
easily damage or destroy their content.</p>

<p style="margin-top: 1em">In the future, the lvmlockctl
kill command may automatically attempt to forcibly
deactivate LVs before the sanlock lease expires. Until then,
the user must notice the syslog <br>
message and manually deactivate the VG before sanlock resets
the machine.</p>

<p style="margin-top: 1em">sanlock daemon failure</p>

<p style="margin-top: 1em">If the sanlock daemon fails or
exits while a lockspace is started, the local watchdog will
reset the host. This is necessary to protect any application
resources that depend on <br>
sanlock leases which will be lost without sanlock
running.</p>

<p style="margin-top: 1em">changing dlm cluster name <br>
When a dlm VG is created, the cluster name is saved in the
VG metadata. To use the VG, a host must be in the named dlm
cluster. If the dlm cluster name changes, or the VG is <br>
moved to a new cluster, the dlm cluster name saved in the VG
must also be changed.</p>

<p style="margin-top: 1em">To see the dlm cluster name
saved in the VG, use the command: <br>
vgs -o+locktype,lockargs &lt;vgname&gt;</p>

<p style="margin-top: 1em">To change the dlm cluster name
in the VG when the VG is still used by the original
cluster:</p>

<p style="margin-top: 1em">&Acirc;&middot; Stop the VG on
all hosts: <br>
vgchange --lock-stop &lt;vgname&gt;</p>

<p style="margin-top: 1em">&Acirc;&middot; Change the VG
lock type to none: <br>
vgchange --lock-type none &lt;vgname&gt;</p>

<p style="margin-top: 1em">&Acirc;&middot; Change the dlm
cluster name on the host or move the VG to the new cluster.
The new dlm cluster must now be active on the host. Verify
the new name by: <br>
cat /sys/kernel/config/dlm/cluster/cluster_name</p>

<p style="margin-top: 1em">&Acirc;&middot; Change the VG
lock type back to dlm which sets the new cluster name: <br>
vgchange --lock-type dlm &lt;vgname&gt;</p>

<p style="margin-top: 1em">&Acirc;&middot; Start the VG on
hosts to use it: <br>
vgchange --lock-start &lt;vgname&gt;</p>

<p style="margin-top: 1em">To change the dlm cluster name
in the VG when the dlm cluster name has already changed, or
the VG has already moved to a different cluster:</p>

<p style="margin-top: 1em">&Acirc;&middot; Ensure the VG is
not being used by any hosts.</p>

<p style="margin-top: 1em">&Acirc;&middot; The new dlm
cluster must be active on the host making the change. The
current dlm cluster name can be seen by: <br>
cat /sys/kernel/config/dlm/cluster/cluster_name</p>

<p style="margin-top: 1em">&Acirc;&middot; Change the VG
lock type to none: <br>
vgchange --lock-type none --force &lt;vgname&gt;</p>

<p style="margin-top: 1em">&Acirc;&middot; Change the VG
lock type back to dlm which sets the new cluster name: <br>
vgchange --lock-type dlm &lt;vgname&gt;</p>

<p style="margin-top: 1em">&Acirc;&middot; Start the VG on
hosts to use it: <br>
vgchange --lock-start &lt;vgname&gt;</p>

<p style="margin-top: 1em">changing a local VG to a lockd
VG <br>
All LVs must be inactive to change the lock type.</p>

<p style="margin-top: 1em">lvmlockd must be configured and
running as described in USAGE.</p>

<p style="margin-top: 1em">Change a local VG to a lockd VG
with the command: <br>
vgchange --lock-type sanlock|dlm &lt;vgname&gt;</p>

<p style="margin-top: 1em">Start the VG on hosts to use it:
<br>
vgchange --lock-start &lt;vgname&gt;</p>

<p style="margin-top: 1em">changing a lockd VG to a local
VG <br>
Stop the lockd VG on all hosts, then run: <br>
vgchange --lock-type none &lt;vgname&gt;</p>

<p style="margin-top: 1em">To change a VG from one lockd
type to another (i.e. between sanlock and dlm), first change
it to a local VG, then to the new type.</p>

<p style="margin-top: 1em">changing a clvm VG to a lockd VG
<br>
All LVs must be inactive to change the lock type.</p>

<p style="margin-top: 1em">First change the clvm VG to a
local VG. Within a running clvm cluster, change a clvm VG to
a local VG with the command:</p>

<p style="margin-top: 1em">vgchange -cn &lt;vgname&gt;</p>

<p style="margin-top: 1em">If the clvm cluster is no longer
running on any nodes, then extra options can be used to
forcibly make the VG local. Caution: this is only safe if
all nodes have stopped using <br>
the VG:</p>

<p style="margin-top: 1em">vgchange --config
&rsquo;global/locking_type=0 global/use_lvmlockd=0&rsquo;
<br>
-cn &lt;vgname&gt;</p>

<p style="margin-top: 1em">After the VG is local, follow
the steps described in &quot;changing a local VG to a lockd
VG&quot;.</p>

<p style="margin-top: 1em">limitations of lockd VGs <br>
Things that do not yet work in lockd VGs: <br>
&Acirc;&middot; creating a new thin pool and a new thin LV
in a single command <br>
&Acirc;&middot; using lvcreate to create cache pools or
cache LVs (use lvconvert) <br>
&Acirc;&middot; using external origins for thin LVs <br>
&Acirc;&middot; splitting mirrors and snapshots from LVs
<br>
&Acirc;&middot; vgsplit <br>
&Acirc;&middot; vgmerge <br>
&Acirc;&middot; resizing an LV that is active in the shared
mode on multiple hosts</p>

<p style="margin-top: 1em">lvmlockd changes from clvmd <br>
(See above for converting an existing clvm VG to a lockd
VG.)</p>

<p style="margin-top: 1em">While lvmlockd and clvmd are
entirely different systems, LVM command usage remains
similar. Differences are more notable when using
lvmlockd&rsquo;s sanlock option.</p>

<p style="margin-top: 1em">Visible usage differences
between lockd VGs with lvmlockd and clvm VGs with clvmd:</p>

<p style="margin-top: 1em">&Acirc;&middot; lvm.conf must be
configured to use either lvmlockd (use_lvmlockd=1) or clvmd
(locking_type=3), but not both.</p>

<p style="margin-top: 1em">&Acirc;&middot; vgcreate
--shared creates a lockd VG, and vgcreate --clustered y
creates a clvm VG.</p>

<p style="margin-top: 1em">&Acirc;&middot; lvmlockd adds
the option of using sanlock for locking, avoiding the need
for network clustering.</p>

<p style="margin-top: 1em">&Acirc;&middot; lvmlockd
defaults to the exclusive activation mode whenever the
activation mode is unspecified, i.e. -ay means -aey, not
-asy.</p>

<p style="margin-top: 1em">&Acirc;&middot; lvmlockd
commands always apply to the local host, and never have an
effect on a remote host. (The activation option
&rsquo;l&rsquo; is not used.)</p>

<p style="margin-top: 1em">&Acirc;&middot; lvmlockd works
with thin and cache pools and LVs.</p>

<p style="margin-top: 1em">&Acirc;&middot; lvmlockd works
with lvmetad.</p>

<p style="margin-top: 1em">&Acirc;&middot; lvmlockd saves
the cluster name for a lockd VG using dlm. Only hosts in the
matching cluster can use the VG.</p>

<p style="margin-top: 1em">&Acirc;&middot; lvmlockd
requires starting/stopping lockd VGs with vgchange
--lock-start and --lock-stop.</p>

<p style="margin-top: 1em">&Acirc;&middot; vgremove of a
sanlock VG may fail indicating that all hosts have not
stopped the VG lockspace. Stop the VG on all hosts using
vgchange --lock-stop.</p>

<p style="margin-top: 1em">&Acirc;&middot; vgreduce or
pvmove of a PV in a sanlock VG will fail if it holds the
internal &quot;lvmlock&quot; LV that holds the sanlock
locks.</p>

<p style="margin-top: 1em">&Acirc;&middot; lvmlockd uses
lock retries instead of lock queueing, so high lock
contention may require increasing
global/lvmlockd_lock_retries to avoid transient lock
failures.</p>

<p style="margin-top: 1em">&Acirc;&middot; lvmlockd
includes VG reporting options lock_type and lock_args, and
LV reporting option lock_args to view the corresponding
metadata fields.</p>

<p style="margin-top: 1em">&Acirc;&middot; In the
&rsquo;vgs&rsquo; command&rsquo;s sixth VG attr field,
&quot;s&quot; for &quot;shared&quot; is displayed for lockd
VGs.</p>

<p style="margin-top: 1em">&Acirc;&middot; If lvmlockd
fails or is killed while in use, locks it held remain but
are orphaned in the lock manager. lvmlockd can be restarted
with an option to adopt the orphan locks from <br>
the previous instance of lvmlockd.</p>

<p style="margin-top: 1em">Red Hat, Inc LVM TOOLS
2.02.166(2)-RHEL7 (2016-11-16) LVMLOCKD(8)</p>
<hr>
</body>
</html>
