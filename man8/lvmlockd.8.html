<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8"/>
  <style>
    table.head, table.foot { width: 100%; }
    td.head-rtitle, td.foot-os { text-align: right; }
    td.head-vol { text-align: center; }
    div.Pp { margin: 1ex 0ex; }
  </style>
  <title>LVMLOCKD(8)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">LVMLOCKD(8)</td>
    <td class="head-vol"></td>
    <td class="head-rtitle">LVMLOCKD(8)</td>
  </tr>
</table>
<div class="manual-text">
<h1 class="Sh" title="Sh" id="NAME"><a class="selflink" href="#NAME">NAME</a></h1>
lvmlockd &#x2014; LVM locking daemon
<div style="height: 1.00em;">&#x00A0;</div>
<h1 class="Sh" title="Sh" id="DESCRIPTION"><a class="selflink" href="#DESCRIPTION">DESCRIPTION</a></h1>
LVM commands use lvmlockd to coordinate access to shared storage.
<div>&#x00A0;</div>
When LVM is used on devices shared by multiple hosts, locks will:
<div style="height: 1.00em;">&#x00A0;</div>
&#x2022; coordinate reading and writing of LVM metadata
<div>&#x00A0;</div>
&#x2022; validate caching of LVM metadata
<div>&#x00A0;</div>
&#x2022; prevent concurrent activation of logical volumes
<div>&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
lvmlockd uses an external lock manager to perform basic locking.
<div>&#x00A0;</div>
Lock manager (lock type) options are:
<div style="height: 1.00em;">&#x00A0;</div>
&#x2022; sanlock: places locks on disk within LVM storage.
<div>&#x00A0;</div>
&#x2022; dlm: uses network communication and a cluster manager.
<div>&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h1 class="Sh" title="Sh" id="OPTIONS"><a class="selflink" href="#OPTIONS">OPTIONS</a></h1>
lvmlockd [options]
<div style="height: 1.00em;">&#x00A0;</div>
For default settings, see lvmlockd -h.
<div style="height: 1.00em;">&#x00A0;</div>
<b>--help | -h</b>
<br/>
 Show this help information.
<div style="height: 1.00em;">&#x00A0;</div>
<b>--version | -V</b>
<br/>
 Show version of lvmlockd.
<div style="height: 1.00em;">&#x00A0;</div>
<b>--test | -T</b>
<br/>
 Test mode, do not call lock manager.
<div style="height: 1.00em;">&#x00A0;</div>
<b>--foreground | -f</b>
<br/>
 Don't fork.
<div style="height: 1.00em;">&#x00A0;</div>
<b>--daemon-debug | -D</b>
<br/>
 Don't fork and print debugging to stdout.
<div style="height: 1.00em;">&#x00A0;</div>
<b>--pid-file | -p</b> <i>path</i>
<br/>
 Set path to the pid file.
<div style="height: 1.00em;">&#x00A0;</div>
<b>--socket-path | -s</b> <i>path</i>
<br/>
 Set path to the socket to listen on.
<div style="height: 1.00em;">&#x00A0;</div>
<b>--syslog-priority | -S err|warning|debug</b>
<br/>
 Write log messages from this level up to syslog.
<div style="height: 1.00em;">&#x00A0;</div>
<b>--gl-type | -g sanlock|dlm</b>
<br/>
 Set global lock type to be sanlock or dlm.
<div style="height: 1.00em;">&#x00A0;</div>
<b>--host-id | -i</b> <i>num</i>
<br/>
 Set the local sanlock host id.
<div style="height: 1.00em;">&#x00A0;</div>
<b>--host-id-file | -F</b> <i>path</i>
<br/>
 A file containing the local sanlock host_id.
<div style="height: 1.00em;">&#x00A0;</div>
<b>--sanlock-timeout | -o</b> <i>seconds</i>
<br/>
 Override the default sanlock I/O timeout.
<div style="height: 1.00em;">&#x00A0;</div>
<b>--adopt | -A 0|1</b>
<br/>
 Adopt locks from a previous instance of lvmlockd.
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h1 class="Sh" title="Sh" id="USAGE"><a class="selflink" href="#USAGE">USAGE</a></h1>
<h2 class="Ss" title="Ss" id="Initial_set_up"><a class="selflink" href="#Initial_set_up">Initial
  set up</a></h2>
Using LVM with lvmlockd for the first time includes some one-time set up steps:
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="1._choose_a_lock_manager"><a class="selflink" href="#1._choose_a_lock_manager">1.
  choose a lock manager</a></h2>
<i>dlm</i>
<div>&#x00A0;</div>
If dlm (or corosync) are already being used by other cluster software, then
  select dlm. dlm uses corosync which requires additional configuration beyond
  the scope of this document. See corosync and dlm documentation for
  instructions on configuration, setup and usage.
<div style="height: 1.00em;">&#x00A0;</div>
<i>sanlock</i>
<div>&#x00A0;</div>
Choose sanlock if dlm/corosync are not otherwise required. sanlock does not
  depend on any clustering software or configuration.
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="2._configure_hosts_to_use_lvmlockd"><a class="selflink" href="#2._configure_hosts_to_use_lvmlockd">2.
  configure hosts to use lvmlockd</a></h2>
On all hosts running lvmlockd, configure lvm.conf:
<pre>
locking_type = 1
use_lvmlockd = 1
</pre>
<div style="height: 1.00em;">&#x00A0;</div>
<i>sanlock</i>
<div>&#x00A0;</div>
Assign each host a unique host_id in the range 1-2000 by setting
<div>&#x00A0;</div>
/etc/lvm/lvmlocal.conf local/host_id
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="3._start_lvmlockd"><a class="selflink" href="#3._start_lvmlockd">3.
  start lvmlockd</a></h2>
Use a service/init file if available, or just run &quot;lvmlockd&quot;.
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="4._start_lock_manager"><a class="selflink" href="#4._start_lock_manager">4.
  start lock manager</a></h2>
<i>sanlock</i>
<div>&#x00A0;</div>
systemctl start wdmd sanlock
<div style="height: 1.00em;">&#x00A0;</div>
<i>dlm</i>
<div>&#x00A0;</div>
Follow external clustering documentation when applicable, otherwise:
<div>&#x00A0;</div>
systemctl start corosync dlm
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="5._create_VG_on_shared_devices"><a class="selflink" href="#5._create_VG_on_shared_devices">5.
  create VG on shared devices</a></h2>
vgcreate --shared &lt;vgname&gt; &lt;devices&gt;
<div style="height: 1.00em;">&#x00A0;</div>
The shared option sets the VG lock type to sanlock or dlm depending on which
  lock manager is running. LVM commands will perform locking for the VG using
  lvmlockd. lvmlockd will use the chosen lock manager.
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="6._start_VG_on_all_hosts"><a class="selflink" href="#6._start_VG_on_all_hosts">6.
  start VG on all hosts</a></h2>
vgchange --lock-start
<div style="height: 1.00em;">&#x00A0;</div>
lvmlockd requires shared VGs to be started before they are used. This is a lock
  manager operation to start (join) the VG lockspace, and it may take some time.
  Until the start completes, locks for the VG are not available. LVM commands
  are allowed to read the VG while start is in progress. (An init/unit file can
  also be used to start VGs.)
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="7._create_and_activate_LVs"><a class="selflink" href="#7._create_and_activate_LVs">7.
  create and activate LVs</a></h2>
Standard lvcreate and lvchange commands are used to create and activate LVs in a
  shared VG.
<div style="height: 1.00em;">&#x00A0;</div>
An LV activated exclusively on one host cannot be activated on another. When
  multiple hosts need to use the same LV concurrently, the LV can be activated
  with a shared lock (see lvchange options -aey vs -asy.) (Shared locks are
  disallowed for certain LV types that cannot be used from multiple hosts.)
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="Normal_start_up_and_shut_down"><a class="selflink" href="#Normal_start_up_and_shut_down">Normal
  start up and shut down</a></h2>
After initial set up, start up and shut down include the following general
  steps. They can be performed manually or using the system service manager.
<div style="height: 1.00em;">&#x00A0;</div>
&#x2022; start lvmetad
<div>&#x00A0;</div>
&#x2022; start lvmlockd
<div>&#x00A0;</div>
&#x2022; start lock manager
<div>&#x00A0;</div>
&#x2022; vgchange --lock-start
<div>&#x00A0;</div>
&#x2022; activate LVs in shared VGs
<div>&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
The shut down sequence is the reverse:
<div style="height: 1.00em;">&#x00A0;</div>
&#x2022; deactivate LVs in shared VGs
<div>&#x00A0;</div>
&#x2022; vgchange --lock-stop
<div>&#x00A0;</div>
&#x2022; stop lock manager
<div>&#x00A0;</div>
&#x2022; stop lvmlockd
<div>&#x00A0;</div>
&#x2022; stop lvmetad
<div>&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h1 class="Sh" title="Sh" id="TOPICS"><a class="selflink" href="#TOPICS">TOPICS</a></h1>
<h2 class="Ss" title="Ss" id="VG_access_control"><a class="selflink" href="#VG_access_control">VG
  access control</a></h2>
The following terms are used to describe different forms of VG access control.
<div style="height: 1.00em;">&#x00A0;</div>
<i>lockd VG</i>
<div style="height: 1.00em;">&#x00A0;</div>
A &quot;lockd VG&quot; is a shared VG that has a &quot;lock type&quot; of dlm or
  sanlock. Using it requires lvmlockd. These VGs exist on shared storage that is
  visible to multiple hosts. LVM commands use lvmlockd to perform locking for
  these VGs when they are used.
<div style="height: 1.00em;">&#x00A0;</div>
If the lock manager for the lock type is not available (e.g. not started or
  failed), lvmlockd is unable to acquire locks for LVM commands. LVM commands
  that only read the VG will generally be allowed to continue without locks in
  this case (with a warning). Commands to modify or activate the VG will fail
  without the necessary locks.
<div style="height: 1.00em;">&#x00A0;</div>
<i>local VG</i>
<div style="height: 1.00em;">&#x00A0;</div>
A &quot;local VG&quot; is meant to be used by a single host. It has no lock type
  or lock type &quot;none&quot;. LVM commands and lvmlockd do not perform
  locking for these VGs. A local VG typically exists on local (non-shared)
  devices and cannot be used concurrently from different hosts.
<div style="height: 1.00em;">&#x00A0;</div>
If a local VG does exist on shared devices, it should be owned by a single host
  by having its system ID set, see <b>lvmsystemid</b>(7). Only the host with a
  matching system ID can use the local VG. A VG with no lock type and no system
  ID should be excluded from all but one host using lvm.conf filters. Without
  any of these protections, a local VG on shared devices can be easily damaged
  or destroyed.
<div style="height: 1.00em;">&#x00A0;</div>
<i>clvm VG</i>
<div style="height: 1.00em;">&#x00A0;</div>
A &quot;clvm VG&quot; is a VG on shared storage (like a lockd VG) that requires
  clvmd for clustering. See below for converting a clvm VG to a lockd VG.
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="lockd_VGs_from_hosts_not_using_lvmlockd"><a class="selflink" href="#lockd_VGs_from_hosts_not_using_lvmlockd">lockd
  VGs from hosts not using lvmlockd</a></h2>
Only hosts that use lockd VGs should be configured to run lvmlockd. However,
  shared devices used by lockd VGs may be visible from hosts not using lvmlockd.
  From a host not using lvmlockd, visible lockd VGs are ignored in the same way
  as foreign VGs (see <b>lvmsystemid</b>(7).)
<div style="height: 1.00em;">&#x00A0;</div>
The --shared option for reporting and display commands causes lockd VGs to be
  displayed on a host not using lvmlockd, like the --foreign option does for
  foreign VGs.
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="vgcreate_comparison"><a class="selflink" href="#vgcreate_comparison">vgcreate
  comparison</a></h2>
The type of VG access control is specified in the vgcreate command. See
  <b>vgcreate</b>(8) for all vgcreate options.
<div style="height: 1.00em;">&#x00A0;</div>
<b>vgcreate &lt;vgname&gt; &lt;devices&gt;</b>
<div style="height: 1.00em;">&#x00A0;</div>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Creates a local VG with the local system ID when neither
      lvmlockd nor clvm are configured.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Creates a local VG with the local system ID when lvmlockd
      is configured.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Creates a clvm VG when clvm is configured.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<div class="Pp"></div>
<b>vgcreate --shared &lt;vgname&gt; &lt;devices&gt;</b>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Requires lvmlockd to be configured and running.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Creates a lockd VG with lock type sanlock|dlm depending on
      which lock manager is running.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">LVM commands request locks from lvmlockd to use the
    VG.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">lvmlockd obtains locks from the selected lock manager.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<div class="Pp"></div>
<b>vgcreate -c|--clustered y &lt;vgname&gt; &lt;devices&gt;</b>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Requires clvm to be configured and running.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Creates a clvm VG with the &quot;clustered&quot; flag.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">LVM commands request locks from clvmd to use the VG.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<h2 class="Ss" title="Ss" id="creating_the_first_sanlock_VG"><a class="selflink" href="#creating_the_first_sanlock_VG">creating
  the first sanlock VG</a></h2>
Creating the first sanlock VG is not protected by locking and requires special
  attention. This is because sanlock locks exist within the VG, so they are not
  available until the VG exists. The first sanlock VG will contain the
  &quot;global lock&quot;.
<div style="height: 1.00em;">&#x00A0;</div>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">The first vgcreate command needs to be given the path to a
      device that has not yet been initialized with pvcreate. The pvcreate
      initialization will be done by vgcreate. This is because the pvcreate
      command requires the global lock, which will not be available until after
      the first sanlock VG is created.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">While running vgcreate for the first sanlock VG, ensure
      that the device being used is not used by another LVM command. Allocation
      of shared devices is usually protected by the global lock, but this cannot
      be done for the first sanlock VG which will hold the global lock.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">While running vgcreate for the first sanlock VG, ensure
      that the VG name being used is not used by another LVM command. Uniqueness
      of VG names is usually ensured by the global lock.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Because the first sanlock VG will contain the global lock,
      this VG needs to be accessible to all hosts that will use sanlock shared
      VGs. All hosts will need to use the global lock from the first sanlock VG.
    <div style="height: 1.00em;">&#x00A0;</div>
    See below for more information about managing the sanlock global lock.
    <div style="height: 1.00em;">&#x00A0;</div>
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<h2 class="Ss" title="Ss" id="using_lockd_VGs"><a class="selflink" href="#using_lockd_VGs">using
  lockd VGs</a></h2>
There are some special considerations when using lockd VGs.
<div style="height: 1.00em;">&#x00A0;</div>
When use_lvmlockd is first enabled in lvm.conf, and before the first lockd VG is
  created, no global lock will exist. In this initial state, LVM commands try
  and fail to acquire the global lock, producing a warning, and some commands
  are disallowed. Once the first lockd VG is created, the global lock will be
  available, and LVM will be fully operational.
<div style="height: 1.00em;">&#x00A0;</div>
When a new lockd VG is created, its lockspace is automatically started on the
  host that creates it. Other hosts need to run 'vgchange --lock-start' to start
  the new VG before they can use it.
<div style="height: 1.00em;">&#x00A0;</div>
From the 'vgs' command, lockd VGs are indicated by &quot;s&quot; (for shared) in
  the sixth attr field. The specific lock type and lock args for a lockd VG can
  be displayed with 'vgs -o+locktype,lockargs'.
<div style="height: 1.00em;">&#x00A0;</div>
lockd VGs need to be &quot;started&quot; and &quot;stopped&quot;, unlike other
  types of VGs. See the following section for a full description of starting and
  stopping.
<div style="height: 1.00em;">&#x00A0;</div>
vgremove of a lockd VG will fail if other hosts have the VG started. Run
  vgchange --lock-stop &lt;vgname&gt; on all other hosts before vgremove. (It
  may take several seconds before vgremove recognizes that all hosts have
  stopped a sanlock VG.)
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="starting_and_stopping_VGs"><a class="selflink" href="#starting_and_stopping_VGs">starting
  and stopping VGs</a></h2>
Starting a lockd VG (vgchange --lock-start) causes the lock manager to start
  (join) the lockspace for the VG on the host where it is run. This makes locks
  for the VG available to LVM commands on the host. Before a VG is started, only
  LVM commands that read/display the VG are allowed to continue without locks
  (and with a warning).
<div style="height: 1.00em;">&#x00A0;</div>
Stopping a lockd VG (vgchange --lock-stop) causes the lock manager to stop
  (leave) the lockspace for the VG on the host where it is run. This makes locks
  for the VG inaccessible to the host. A VG cannot be stopped while it has
  active LVs.
<div style="height: 1.00em;">&#x00A0;</div>
When using the lock type sanlock, starting a VG can take a long time
  (potentially minutes if the host was previously shut down without cleanly
  stopping the VG.)
<div style="height: 1.00em;">&#x00A0;</div>
A lockd VG can be started after all the following are true:
<div>&#x00A0;</div>
&#x2022; lvmlockd is running
<div>&#x00A0;</div>
&#x2022; the lock manager is running
<div>&#x00A0;</div>
&#x2022; the VG is visible to the system
<div>&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
A lockd VG can be stopped if all LVs are deactivated.
<div style="height: 1.00em;">&#x00A0;</div>
All lockd VGs can be started/stopped using:
<div>&#x00A0;</div>
vgchange --lock-start
<div>&#x00A0;</div>
vgchange --lock-stop
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
Individual VGs can be started/stopped using:
<div>&#x00A0;</div>
vgchange --lock-start &lt;vgname&gt; ...
<div>&#x00A0;</div>
vgchange --lock-stop &lt;vgname&gt; ...
<div style="height: 1.00em;">&#x00A0;</div>
To make vgchange not wait for start to complete:
<div>&#x00A0;</div>
vgchange --lock-start --lock-opt nowait ...
<div style="height: 1.00em;">&#x00A0;</div>
lvmlockd can be asked directly to stop all lockspaces:
<div>&#x00A0;</div>
lvmlockctl --stop-lockspaces
<div style="height: 1.00em;">&#x00A0;</div>
To start only selected lockd VGs, use the lvm.conf activation/lock_start_list.
  When defined, only VG names in this list are started by vgchange. If the list
  is not defined (the default), all visible lockd VGs are started. To start only
  &quot;vg1&quot;, use the following lvm.conf configuration:
<div style="height: 1.00em;">&#x00A0;</div>
<pre>
activation {
    lock_start_list = [ &quot;vg1&quot; ]
    ...
}
</pre>
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="automatic_starting_and_automatic_activation"><a class="selflink" href="#automatic_starting_and_automatic_activation">automatic
  starting and automatic activation</a></h2>
Scripts or programs on a host that automatically start VGs will use the
  &quot;auto&quot; option to indicate that the command is being run
  automatically by the system:
<div style="height: 1.00em;">&#x00A0;</div>
vgchange --lock-start --lock-opt auto [&lt;vgname&gt; ...]
<div style="height: 1.00em;">&#x00A0;</div>
Without any additional configuration, including the &quot;auto&quot; option has
  no effect; all VGs are started unless restricted by lock_start_list.
<div style="height: 1.00em;">&#x00A0;</div>
However, when the lvm.conf activation/auto_lock_start_list is defined, the auto
  start command performs an additional filtering phase to all VGs being started,
  testing each VG name against the auto_lock_start_list. The
  auto_lock_start_list defines lockd VGs that will be started by the auto start
  command. Visible lockd VGs not included in the list are ignored by the auto
  start command. If the list is undefined, all VG names pass this filter. (The
  lock_start_list is also still used to filter all VGs.)
<div style="height: 1.00em;">&#x00A0;</div>
The auto_lock_start_list allows a user to select certain lockd VGs that should
  be automatically started by the system (or indirectly, those that should not).
<div style="height: 1.00em;">&#x00A0;</div>
To use auto activation of lockd LVs (see auto_activation_volume_list), auto
  starting of the corresponding lockd VGs is necessary.
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="internal_command_locking"><a class="selflink" href="#internal_command_locking">internal
  command locking</a></h2>
To optimize the use of LVM with lvmlockd, be aware of the three kinds of locks
  and when they are used:
<div style="height: 1.00em;">&#x00A0;</div>
<i>GL lock</i>
<div style="height: 1.00em;">&#x00A0;</div>
The global lock (GL lock) is associated with global information, which is
  information not isolated to a single VG. This includes:
<div style="height: 1.00em;">&#x00A0;</div>
&#x2022; The global VG namespace.
<div>&#x00A0;</div>
&#x2022; The set of orphan PVs and unused devices.
<div>&#x00A0;</div>
&#x2022; The properties of orphan PVs, e.g. PV size.
<div>&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
The global lock is used in shared mode by commands that read this information,
  or in exclusive mode by commands that change it.
<div style="height: 1.00em;">&#x00A0;</div>
The command 'vgs' acquires the global lock in shared mode because it reports the
  list of all VG names.
<div style="height: 1.00em;">&#x00A0;</div>
The vgcreate command acquires the global lock in exclusive mode because it
  creates a new VG name, and it takes a PV from the list of unused PVs.
<div style="height: 1.00em;">&#x00A0;</div>
When an LVM command is given a tag argument, or uses select, it must read all
  VGs to match the tag or selection, which causes the global lock to be
  acquired.
<div style="height: 1.00em;">&#x00A0;</div>
<i>VG lock</i>
<div style="height: 1.00em;">&#x00A0;</div>
A VG lock is associated with each VG. The VG lock is acquired in shared mode to
  read the VG and in exclusive mode to change the VG (modify the VG metadata or
  activate LVs). This lock serializes access to a VG with all other LVM commands
  accessing the VG from all hosts.
<div style="height: 1.00em;">&#x00A0;</div>
The command 'vgs' will not only acquire the GL lock to read the list of all VG
  names, but will acquire the VG lock for each VG prior to reading it.
<div style="height: 1.00em;">&#x00A0;</div>
The command 'vgs &lt;vgname&gt;' does not acquire the GL lock (it does not need
  the list of all VG names), but will acquire the VG lock on each VG name
  argument.
<div style="height: 1.00em;">&#x00A0;</div>
<i>LV lock</i>
<div style="height: 1.00em;">&#x00A0;</div>
An LV lock is acquired before the LV is activated, and is released after the LV
  is deactivated. If the LV lock cannot be acquired, the LV is not activated. LV
  locks are persistent and remain in place after the activation command is done.
  GL and VG locks are transient, and are held only while an LVM command is
  running.
<div style="height: 1.00em;">&#x00A0;</div>
<i>lock retries</i>
<div style="height: 1.00em;">&#x00A0;</div>
If a request for a GL or VG lock fails due to a lock conflict with another host,
  lvmlockd automatically retries for a short time before returning a failure to
  the LVM command. If those retries are insufficient, the LVM command will retry
  the entire lock request a number of times specified by
  global/lvmlockd_lock_retries before failing. If a request for an LV lock fails
  due to a lock conflict, the command fails immediately.
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="managing_the_global_lock_in_sanlock_VGs"><a class="selflink" href="#managing_the_global_lock_in_sanlock_VGs">managing
  the global lock in sanlock VGs</a></h2>
The global lock exists in one of the sanlock VGs. The first sanlock VG created
  will contain the global lock. Subsequent sanlock VGs will each contain
  disabled global locks that can be enabled later if necessary.
<div style="height: 1.00em;">&#x00A0;</div>
The VG containing the global lock must be visible to all hosts using sanlock
  VGs. This can be a reason to create a small sanlock VG, visible to all hosts,
  and dedicated to just holding the global lock. While not required, this
  strategy can help to avoid difficulty in the future if VGs are moved or
  removed.
<div style="height: 1.00em;">&#x00A0;</div>
The vgcreate command typically acquires the global lock, but in the case of the
  first sanlock VG, there will be no global lock to acquire until the first
  vgcreate is complete. So, creating the first sanlock VG is a special case that
  skips the global lock.
<div style="height: 1.00em;">&#x00A0;</div>
vgcreate for a sanlock VG determines it is the first one to exist if no other
  sanlock VGs are visible. It is possible that other sanlock VGs do exist but
  are not visible on the host running vgcreate. In this case, vgcreate would
  create a new sanlock VG with the global lock enabled. When the other VG
  containing a global lock appears, lvmlockd will see more than one VG with a
  global lock enabled, and LVM commands will report that there are duplicate
  global locks.
<div style="height: 1.00em;">&#x00A0;</div>
If the situation arises where more than one sanlock VG contains a global lock,
  the global lock should be manually disabled in all but one of them with the
  command:
<div style="height: 1.00em;">&#x00A0;</div>
lvmlockctl --gl-disable &lt;vgname&gt;
<div style="height: 1.00em;">&#x00A0;</div>
(The one VG with the global lock enabled must be visible to all hosts.)
<div style="height: 1.00em;">&#x00A0;</div>
An opposite problem can occur if the VG holding the global lock is removed. In
  this case, no global lock will exist following the vgremove, and subsequent
  LVM commands will fail to acquire it. In this case, the global lock needs to
  be manually enabled in one of the remaining sanlock VGs with the command:
<div style="height: 1.00em;">&#x00A0;</div>
lvmlockctl --gl-enable &lt;vgname&gt;
<div style="height: 1.00em;">&#x00A0;</div>
A small sanlock VG dedicated to holding the global lock can avoid the case where
  the GL lock must be manually enabled after a vgremove.
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="internal_lvmlock_LV"><a class="selflink" href="#internal_lvmlock_LV">internal
  lvmlock LV</a></h2>
A sanlock VG contains a hidden LV called &quot;lvmlock&quot; that holds the
  sanlock locks. vgreduce cannot yet remove the PV holding the lvmlock LV. To
  remove this PV, change the VG lock type to &quot;none&quot;, run vgreduce,
  then change the VG lock type back to &quot;sanlock&quot;. Similarly, pvmove
  cannot be used on a PV used by the lvmlock LV.
<div style="height: 1.00em;">&#x00A0;</div>
To place the lvmlock LV on a specific device, create the VG with only that
  device, then use vgextend to add other devices.
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="shared_LVs"><a class="selflink" href="#shared_LVs">shared
  LVs</a></h2>
When an LV is used concurrently from multiple hosts (e.g. by a
  multi-host/cluster application or file system), the LV can be activated on
  multiple hosts concurrently using a shared lock.
<div style="height: 1.00em;">&#x00A0;</div>
To activate the LV with a shared lock: lvchange -asy vg/lv.
<div style="height: 1.00em;">&#x00A0;</div>
With lvmlockd, an unspecified activation mode is always exclusive, i.e. -ay
  defaults to -aey.
<div style="height: 1.00em;">&#x00A0;</div>
If the LV type does not allow the LV to be used concurrently from multiple
  hosts, then a shared activation lock is not allowed and the lvchange command
  will report an error. LV types that cannot be used concurrently from multiple
  hosts include thin, cache, raid, mirror, and snapshot.
<div style="height: 1.00em;">&#x00A0;</div>
lvextend on LV with shared locks is not yet allowed. The LV must be deactivated,
  or activated exclusively to run lvextend.
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="recover_from_lost_PV_holding_sanlock_locks"><a class="selflink" href="#recover_from_lost_PV_holding_sanlock_locks">recover
  from lost PV holding sanlock locks</a></h2>
The general approach is to change the VG lock type to &quot;none&quot;, and then
  change the lock type back to &quot;sanlock&quot;. This recreates the internal
  lvmlock LV and the necessary locks on it. Additional steps may be required to
  deal with the missing PV.
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="locking_system_failures"><a class="selflink" href="#locking_system_failures">locking
  system failures</a></h2>
<b>lvmlockd failure</b>
<div style="height: 1.00em;">&#x00A0;</div>
If lvmlockd fails or is killed while holding locks, the locks are orphaned in
  the lock manager. lvmlockd can be restarted with an option to adopt locks in
  the lock manager that had been held by the previous instance.
<div style="height: 1.00em;">&#x00A0;</div>
<b>dlm/corosync failure</b>
<div style="height: 1.00em;">&#x00A0;</div>
If dlm or corosync fail, the clustering system will fence the host using a
  method configured within the dlm/corosync clustering environment.
<div style="height: 1.00em;">&#x00A0;</div>
LVM commands on other hosts will be blocked from acquiring any locks until the
  dlm/corosync recovery process is complete.
<div style="height: 1.00em;">&#x00A0;</div>
<b>sanlock lease storage failure</b>
<div style="height: 1.00em;">&#x00A0;</div>
If the PV under a sanlock VG's lvmlock LV is disconnected, unresponsive or too
  slow, sanlock cannot renew the lease for the VG's locks. After some time, the
  lease will expire, and locks that the host owns in the VG can be acquired by
  other hosts. The VG must be forcibly deactivated on the host with the expiring
  lease before other hosts can acquire its locks.
<div style="height: 1.00em;">&#x00A0;</div>
When the sanlock daemon detects that the lease storage is lost, it runs the
  command lvmlockctl --kill &lt;vgname&gt;. This command emits a syslog message
  stating that lease storage is lost for the VG and LVs must be immediately
  deactivated.
<div style="height: 1.00em;">&#x00A0;</div>
If no LVs are active in the VG, then the lockspace with an expiring lease will
  be removed, and errors will be reported when trying to use the VG. Use the
  lvmlockctl --drop command to clear the stale lockspace from lvmlockd.
<div style="height: 1.00em;">&#x00A0;</div>
If the VG has active LVs when the lock storage is lost, the LVs must be quickly
  deactivated before the lockspace lease expires. After all LVs are deactivated,
  run lvmlockctl --drop &lt;vgname&gt; to clear the expiring lockspace from
  lvmlockd. If all LVs in the VG are not deactivated within about 40 seconds,
  sanlock will reset the host using the local watchdog. The machine reset is
  effectively a severe form of &quot;deactivating&quot; LVs before they can be
  activated on other hosts. The reset is considered a better alternative than
  having LVs used by multiple hosts at once, which could easily damage or
  destroy their content.
<div style="height: 1.00em;">&#x00A0;</div>
In the future, the lvmlockctl kill command may automatically attempt to forcibly
  deactivate LVs before the sanlock lease expires. Until then, the user must
  notice the syslog message and manually deactivate the VG before sanlock resets
  the machine.
<div style="height: 1.00em;">&#x00A0;</div>
<b>sanlock daemon failure</b>
<div style="height: 1.00em;">&#x00A0;</div>
If the sanlock daemon fails or exits while a lockspace is started, the local
  watchdog will reset the host. This is necessary to protect any application
  resources that depend on sanlock leases which will be lost without sanlock
  running.
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="changing_dlm_cluster_name"><a class="selflink" href="#changing_dlm_cluster_name">changing
  dlm cluster name</a></h2>
When a dlm VG is created, the cluster name is saved in the VG metadata. To use
  the VG, a host must be in the named dlm cluster. If the dlm cluster name
  changes, or the VG is moved to a new cluster, the dlm cluster name saved in
  the VG must also be changed.
<div style="height: 1.00em;">&#x00A0;</div>
To see the dlm cluster name saved in the VG, use the command:
<div>&#x00A0;</div>
vgs -o+locktype,lockargs &lt;vgname&gt;
<div style="height: 1.00em;">&#x00A0;</div>
To change the dlm cluster name in the VG when the VG is still used by the
  original cluster:
<div style="height: 1.00em;">&#x00A0;</div>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Stop the VG on all hosts:
    <div>&#x00A0;</div>
    vgchange --lock-stop &lt;vgname&gt;
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Change the VG lock type to none:
    <div>&#x00A0;</div>
    vgchange --lock-type none &lt;vgname&gt;
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Change the dlm cluster name on the host or move the VG to
      the new cluster. The new dlm cluster must now be active on the host.
      Verify the new name by:
    <div>&#x00A0;</div>
    cat /sys/kernel/config/dlm/cluster/cluster_name
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Change the VG lock type back to dlm which sets the new
      cluster name:
    <div>&#x00A0;</div>
    vgchange --lock-type dlm &lt;vgname&gt;
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Start the VG on hosts to use it:
    <div>&#x00A0;</div>
    vgchange --lock-start &lt;vgname&gt;
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<div class="Pp"></div>
To change the dlm cluster name in the VG when the dlm cluster name has already
  changed, or the VG has already moved to a different cluster:
<div style="height: 1.00em;">&#x00A0;</div>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Ensure the VG is not being used by any hosts.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">The new dlm cluster must be active on the host making the
      change. The current dlm cluster name can be seen by:
    <div>&#x00A0;</div>
    cat /sys/kernel/config/dlm/cluster/cluster_name
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Change the VG lock type to none:
    <div>&#x00A0;</div>
    vgchange --lock-type none --force &lt;vgname&gt;
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Change the VG lock type back to dlm which sets the new
      cluster name:
    <div>&#x00A0;</div>
    vgchange --lock-type dlm &lt;vgname&gt;
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Start the VG on hosts to use it:
    <div>&#x00A0;</div>
    vgchange --lock-start &lt;vgname&gt;
    <div style="height: 1.00em;">&#x00A0;</div>
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<h2 class="Ss" title="Ss" id="changing_a_local_VG_to_a_lockd_VG"><a class="selflink" href="#changing_a_local_VG_to_a_lockd_VG">changing
  a local VG to a lockd VG</a></h2>
All LVs must be inactive to change the lock type.
<div style="height: 1.00em;">&#x00A0;</div>
lvmlockd must be configured and running as described in USAGE.
<div style="height: 1.00em;">&#x00A0;</div>
Change a local VG to a lockd VG with the command:
<div>&#x00A0;</div>
vgchange --lock-type sanlock|dlm &lt;vgname&gt;
<div style="height: 1.00em;">&#x00A0;</div>
Start the VG on hosts to use it:
<div>&#x00A0;</div>
vgchange --lock-start &lt;vgname&gt;
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="changing_a_lockd_VG_to_a_local_VG"><a class="selflink" href="#changing_a_lockd_VG_to_a_local_VG">changing
  a lockd VG to a local VG</a></h2>
Stop the lockd VG on all hosts, then run:
<div>&#x00A0;</div>
vgchange --lock-type none &lt;vgname&gt;
<div style="height: 1.00em;">&#x00A0;</div>
To change a VG from one lockd type to another (i.e. between sanlock and dlm),
  first change it to a local VG, then to the new type.
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="changing_a_clvm_VG_to_a_lockd_VG"><a class="selflink" href="#changing_a_clvm_VG_to_a_lockd_VG">changing
  a clvm VG to a lockd VG</a></h2>
All LVs must be inactive to change the lock type.
<div style="height: 1.00em;">&#x00A0;</div>
First change the clvm VG to a local VG. Within a running clvm cluster, change a
  clvm VG to a local VG with the command:
<div style="height: 1.00em;">&#x00A0;</div>
vgchange -cn &lt;vgname&gt;
<div style="height: 1.00em;">&#x00A0;</div>
If the clvm cluster is no longer running on any nodes, then extra options can be
  used to forcibly make the VG local. Caution: this is only safe if all nodes
  have stopped using the VG:
<div style="height: 1.00em;">&#x00A0;</div>
vgchange --config 'global/locking_type=0 global/use_lvmlockd=0'
<div style="margin-left: 5.00ex;">-cn &lt;vgname&gt;</div>
<div style="height: 1.00em;">&#x00A0;</div>
After the VG is local, follow the steps described in &quot;changing a local VG
  to a lockd VG&quot;.
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="limitations_of_lockd_VGs"><a class="selflink" href="#limitations_of_lockd_VGs">limitations
  of lockd VGs</a></h2>
Things that do not yet work in lockd VGs:
<div>&#x00A0;</div>
&#x2022; creating a new thin pool and a new thin LV in a single command
<div>&#x00A0;</div>
&#x2022; using lvcreate to create cache pools or cache LVs (use lvconvert)
<div>&#x00A0;</div>
&#x2022; using external origins for thin LVs
<div>&#x00A0;</div>
&#x2022; splitting mirrors and snapshots from LVs
<div>&#x00A0;</div>
&#x2022; vgsplit
<div>&#x00A0;</div>
&#x2022; vgmerge
<div>&#x00A0;</div>
&#x2022; resizing an LV that is active in the shared mode on multiple hosts
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h2 class="Ss" title="Ss" id="lvmlockd_changes_from_clvmd"><a class="selflink" href="#lvmlockd_changes_from_clvmd">lvmlockd
  changes from clvmd</a></h2>
(See above for converting an existing clvm VG to a lockd VG.)
<div style="height: 1.00em;">&#x00A0;</div>
While lvmlockd and clvmd are entirely different systems, LVM command usage
  remains similar. Differences are more notable when using lvmlockd's sanlock
  option.
<div style="height: 1.00em;">&#x00A0;</div>
Visible usage differences between lockd VGs with lvmlockd and clvm VGs with
  clvmd:
<div style="height: 1.00em;">&#x00A0;</div>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">lvm.conf must be configured to use either lvmlockd
      (use_lvmlockd=1) or clvmd (locking_type=3), but not both.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">vgcreate --shared creates a lockd VG, and vgcreate
      --clustered y creates a clvm VG.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">lvmlockd adds the option of using sanlock for locking,
      avoiding the need for network clustering.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">lvmlockd defaults to the exclusive activation mode whenever
      the activation mode is unspecified, i.e. -ay means -aey, not -asy.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">lvmlockd commands always apply to the local host, and never
      have an effect on a remote host. (The activation option 'l' is not used.)
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">lvmlockd works with thin and cache pools and LVs.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">lvmlockd works with lvmetad.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">lvmlockd saves the cluster name for a lockd VG using dlm.
      Only hosts in the matching cluster can use the VG.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">lvmlockd requires starting/stopping lockd VGs with vgchange
      --lock-start and --lock-stop.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">vgremove of a sanlock VG may fail indicating that all hosts
      have not stopped the VG lockspace. Stop the VG on all hosts using vgchange
      --lock-stop.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">vgreduce or pvmove of a PV in a sanlock VG will fail if it
      holds the internal &quot;lvmlock&quot; LV that holds the sanlock locks.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">lvmlockd uses lock retries instead of lock queueing, so
      high lock contention may require increasing global/lvmlockd_lock_retries
      to avoid transient lock failures.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">lvmlockd includes VG reporting options lock_type and
      lock_args, and LV reporting option lock_args to view the corresponding
      metadata fields.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">In the 'vgs' command's sixth VG attr field, &quot;s&quot;
      for &quot;shared&quot; is displayed for lockd VGs.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">If lvmlockd fails or is killed while in use, locks it held
      remain but are orphaned in the lock manager. lvmlockd can be restarted
      with an option to adopt the orphan locks from the previous instance of
      lvmlockd.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
</div>
<table class="foot">
  <tr>
    <td class="foot-date">LVM TOOLS 2.02.166(2)-RHEL7 (2016-11-16)</td>
    <td class="foot-os">Red Hat, Inc</td>
  </tr>
</table>
</body>
</html>
