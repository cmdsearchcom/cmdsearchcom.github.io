<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 19:11:09 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>filetop(8) System Manager&rsquo;s Manual filetop(8)</p>

<p style="margin-top: 1em">NAME <br>
filetop - File reads and writes by filename and process. Top
for files.</p>

<p style="margin-top: 1em">SYNOPSIS <br>
filetop [-h] [-C] [-r MAXROWS] [-p PID] [interval]
[count]</p>

<p style="margin-top: 1em">DESCRIPTION <br>
This is top for files.</p>

<p style="margin-top: 1em">This traces file reads and
writes, and prints a per-file summary every interval (by
default, 1 second). The summary is sorted on the highest
read throughput (Kbytes). By default <br>
only IO on regular files is shown. The -a option will list
all file types (sokets, FIFOs, etc).</p>

<p style="margin-top: 1em">This uses in-kernel eBPF maps to
store per process summaries for efficiency.</p>

<p style="margin-top: 1em">This script works by tracing the
__vfs_read() and __vfs_write() functions using kernel
dynamic tracing, which instruments explicit read and write
calls. If files are read or <br>
written using another means (eg, via mmap()), then they will
not be visible using this tool. Also, this tool will need
updating to match any code changes to those vfs
functions.</p>

<p style="margin-top: 1em">This should be useful for file
system workload characterization when analyzing the
performance of applications.</p>

<p style="margin-top: 1em">Note that tracing VFS level
reads and writes can be a frequent activity, and this tool
can begin to cost measurable overhead at high I/O rates.</p>

<p style="margin-top: 1em">Since this uses BPF, only the
root user can use this tool.</p>

<p style="margin-top: 1em">REQUIREMENTS <br>
CONFIG_BPF and bcc.</p>

<p style="margin-top: 1em">OPTIONS <br>
-a Include non-regular file types (sockets, FIFOs, etc).</p>

<p style="margin-top: 1em">-C Don&rsquo;t clear the
screen.</p>

<p style="margin-top: 1em">-r MAXROWS <br>
Maximum number of rows to print. Default is 20.</p>

<p style="margin-top: 1em">-p PID Trace this PID only.</p>

<p style="margin-top: 1em">interval <br>
Interval between updates, seconds.</p>

<p style="margin-top: 1em">count Number of interval
summaries.</p>

<p style="margin-top: 1em">EXAMPLES <br>
Summarize block device I/O by process, 1 second screen
refresh: <br>
# filetop</p>

<p style="margin-top: 1em">Don&rsquo;t clear the screen,
and top 8 rows only: <br>
# filetop -Cr 8</p>

<p style="margin-top: 1em">5 second summaries, 10 times
only: <br>
# filetop 5 10</p>

<p style="margin-top: 1em">FIELDS <br>
loadavg: <br>
The contents of /proc/loadavg</p>

<p style="margin-top: 1em">PID Process ID.</p>

<p style="margin-top: 1em">COMM Process name.</p>

<p style="margin-top: 1em">READS Count of reads during
interval.</p>

<p style="margin-top: 1em">WRITES Count of writes during
interval.</p>

<p style="margin-top: 1em">R_Kb Total read Kbytes during
interval.</p>

<p style="margin-top: 1em">W_Kb Total write Kbytes during
interval.</p>

<p style="margin-top: 1em">T Type of file: R == regular, S
== socket, O == other (pipe, etc).</p>

<p style="margin-top: 1em">OVERHEAD <br>
Depending on the frequency of application reads and writes,
overhead can become significant, in the worst case slowing
applications by over 50%. Hopefully for real world
work&acirc; <br>
loads the overhead is much less -- test before use. The
reason for the high overhead is that VFS reads and writes
can be a frequent event, and despite the eBPF overhead being
<br>
very small per event, if you multiply this small overhead by
a million events per second, it becomes a million times
worse. Literally. You can gauge the number of reads and <br>
writes using the vfsstat(8) tool, also from bcc.</p>

<p style="margin-top: 1em">SOURCE <br>
This is from bcc.</p>


<p style="margin-top: 1em">https://github.com/iovisor/bcc</p>

<p style="margin-top: 1em">Also look in the bcc
distribution for a companion _examples.txt file containing
example usage, output, and commentary for this tool.</p>

<p style="margin-top: 1em">OS <br>
Linux</p>

<p style="margin-top: 1em">STABILITY <br>
Unstable - in development.</p>

<p style="margin-top: 1em">AUTHOR <br>
Brendan Gregg</p>

<p style="margin-top: 1em">INSPIRATION <br>
top(1) by William LeFebvre</p>

<p style="margin-top: 1em">SEE ALSO <br>
vfsstat(8), vfscount(8), fileslower(8)</p>

<p style="margin-top: 1em">USER COMMANDS 2016-02-08
filetop(8)</p>
<hr>
</body>
</html>
