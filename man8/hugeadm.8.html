<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 19:11:52 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>HUGEADM(8) System Manager&rsquo;s Manual HUGEADM(8)</p>

<p style="margin-top: 1em">NAME <br>
hugeadm - Configure the system huge page pools</p>

<p style="margin-top: 1em">SYNOPSIS <br>
hugeadm [options]</p>

<p style="margin-top: 1em">DESCRIPTION <br>
hugeadm displays and configures the systems huge page pools.
The size of the pools is set as a minimum and maximum
threshold. The minimum value is allocated up front by the
ker&acirc; <br>
nel and guaranteed to remain as hugepages until the pool is
shrunk. If a maximum is set, the system will dynamically
allocate pages if applications request more hugepages than
<br>
the minimum size of the pool. There is no guarantee that
more pages than this minimum pool size can be allocated.</p>

<p style="margin-top: 1em">The following options create
mounts hugetlbfs mount points.</p>

<p style="margin-top: 1em">--create-mounts</p>

<p style="margin-top: 1em">This creates mount points for
each supported huge page size under /var/lib/hugetlbfs.
After creation they are mounts and are owned by root:root
with permissions set to <br>
770. Each mount point is named pagesize-&lt;size in
bytes&gt;.</p>


<p style="margin-top: 1em">--create-user-mounts=&lt;user&gt;</p>

<p style="margin-top: 1em">This creates mount points for
each supported huge page size under
/var/lib/hugetlbfs/user/&lt;user&gt;. Mount point naming is
the same as --create-mounts. After creation they <br>
are mounted and are owned by &lt;user&gt;:root with
permissions set to 700.</p>


<p style="margin-top: 1em">--create-group-mounts=&lt;group&gt;</p>

<p style="margin-top: 1em">This creates mount points for
each supported huge page size under
/var/lib/hugetlbfs/group/&lt;group&gt;. Mount point naming
is the same as --create-mounts. After creation <br>
they are mounted and are owned by root:&lt;group&gt; with
permissions set to 070.</p>

<p style="margin-top: 1em">--create-global-mounts</p>

<p style="margin-top: 1em">This creates mount points for
each supported huge page size under
/var/lib/hugetlbfs/global. Mount point naming is the same as
--create-mounts. After creation they are <br>
mounted and are owned by root:root with permissions set to
1777.</p>

<p style="margin-top: 1em">The following options affect how
mount points are created.</p>

<p style="margin-top: 1em">--max-size</p>

<p style="margin-top: 1em">This option is used in
conjunction with --create-*-mounts. It limits the maximum
amount of memory used by files within the mount point
rounded up to the nearest huge page <br>
size. This can be used for example to grant different huge
page quotas to individual users or groups.</p>

<p style="margin-top: 1em">--max-inodes</p>

<p style="margin-top: 1em">This option is used in
conjunction with --create-*-mounts. It limits the number of
inodes (e.g. files) that can be created on the new mount
points. This limits the number <br>
of mappings that can be created on a mount point. It could
be used for example to limit the number of application
instances that used a mount point as long as it was known
<br>
how many inodes each application instance required.</p>

<p style="margin-top: 1em">The following options display
information about the pools.</p>

<p style="margin-top: 1em">--pool-list</p>

<p style="margin-top: 1em">This displays the Minimum,
Current and Maximum number of huge pages in the pool for
each pagesize supported by the system. The
&quot;Minimum&quot; value is the size of the static <br>
pool and there will always be at least this number of
hugepages in use by the system, either by applications or
kept by the kernel in a reserved pool. The
&quot;Current&quot; value <br>
is the number of hugepages currently in use, either by
applications or stored on the kernels free list. The
&quot;Maximum&quot; value is the largest number of hugepages
that can be <br>
in use at any given time.</p>


<p style="margin-top: 1em">--set-recommended-min_free_kbytes</p>

<p style="margin-top: 1em">Fragmentation avoidance in the
kernel depends on avoiding pages of different mobility types
being mixed with a pageblock arena - typically the size of
the default huge <br>
page size. The more mixing that occurs, the less likely the
huge page pool will be able to dynamically resize. The
easiest means of avoiding mixing is to increase <br>
/proc/sys/vm/min_free_kbytes. This parameter sets
min_free_kbytes to a recommended value to aid fragmentation
avoidance.</p>

<p style="margin-top: 1em">--set-recommended-shmmax</p>

<p style="margin-top: 1em">The maximum shared memory
segment size should be set to at least the size of the
largest shared memory segment size you want available for
applications using huge pages, <br>
via /proc/sys/kernel/shmmax. Optionally, it can be set
automatically to match the maximum possible size of all huge
page allocations and thus the maximum possible shared <br>
memory segment size, using this switch.</p>


<p style="margin-top: 1em">--set-shm-group=&lt;gid|groupname&gt;</p>

<p style="margin-top: 1em">Users in the group specified in
/proc/sys/vm/hugetlb_shm_group are granted full access to
huge pages. The sysctl takes a numeric gid, but this hugeadm
option can set it <br>
for you, using either a gid or group name.</p>

<p style="margin-top: 1em">--page-sizes</p>

<p style="margin-top: 1em">This displays every page size
supported by the system and has a pool configured.</p>

<p style="margin-top: 1em">--page-sizes-all</p>

<p style="margin-top: 1em">This displays all page sizes
supported by the system, even if no pool is available.</p>

<p style="margin-top: 1em">--list-all-mounts</p>

<p style="margin-top: 1em">This displays all active mount
points for hugetlbfs.</p>

<p style="margin-top: 1em">The following options configure
the pool.</p>


<p style="margin-top: 1em">--pool-pages-min=&lt;size|DEFAULT&gt;:[+|-]&lt;pagecount|memsize&lt;G|M|K&gt;&gt;</p>

<p style="margin-top: 1em">This option sets or adjusts the
Minimum number of hugepages in the pool for pagesize size.
size may be specified in bytes or in kilobytes, megabytes,
or gigabytes by <br>
appending K, M, or G respectively, or as DEFAULT, which uses
the system&rsquo;s default huge page size for size. The pool
size adjustment can be specified by pagecount pages or <br>
by memsize, if postfixed with G, M, or K, for gigabytes,
megabytes, or kilobytes, respectively. If the adjustment is
specified via memsize, then the pagecount will be cal&acirc;
<br>
culated for you, based on page size size. The pool is set to
pagecount pages if + or - are not specified. If + or - are
specified, then the size of the pool will adjust <br>
by that amount. Note that there is no guarantee that the
system can allocate the hugepages requested for the Minimum
pool. The size of the pools should be checked after <br>
executing this command to ensure they were successful.</p>

<p style="margin-top: 1em">--obey-numa-mempol</p>

<p style="margin-top: 1em">This option requests that
allocation of huge pages to the static pool with
--pool-pages-min obey the NUMA memory policy of the current
process. This policy can be explic&acirc; <br>
itly specified using numactl or inherited from a parent
process.</p>


<p style="margin-top: 1em">--pool-pages-max=&lt;size|DEFAULT&gt;:[+|-]&lt;pagecount|memsize&lt;G|M|K&gt;&gt;</p>

<p style="margin-top: 1em">This option sets or adjusts the
Maximum number of hugepages. Note that while the Minimum
number of pages are guaranteed to be available to
applications, there is not guar&acirc; <br>
antee that the system can allocate the pages on demand when
the number of huge pages requested by applications is
between the Minimum and Maximum pool sizes. See --pool- <br>
pages-min for usage syntax.</p>

<p style="margin-top: 1em">--enable-zone-movable</p>

<p style="margin-top: 1em">This option enables the use of
the MOVABLE zone for the allocation of hugepages. This zone
is created when kernelcore= or movablecore= are specified on
the kernel command <br>
line but the zone is not used for the allocation of huge
pages by default as the intended use for the zone may be to
guarantee that memory can be off-lined and hot- <br>
removed. The kernel guarantees that the pages within this
zone can be reclaimed unlike some kernel buffers for
example. Unless pages are locked with mlock(), the hugepage
<br>
pool can grow to at least the size of the movable zone once
this option is set. Use sysctl to permanently enable the use
of the MOVABLE zone for the allocation of huge <br>
pages.</p>

<p style="margin-top: 1em">--disable-zone-movable</p>

<p style="margin-top: 1em">This option disables the use of
the MOVABLE zone for the future allocation of huge pages.
Note that existing huge pages are not reclaimed from the
zone. Use sysctl to <br>
permanently disable the use of the MOVABLE zone for the
allocation of huge pages.</p>

<p style="margin-top: 1em">--hard</p>

<p style="margin-top: 1em">This option is specified with
--pool-pages-min to retry allocations multiple times on
failure to allocate the desired count of pages. It initially
tries to resize the pool <br>
up to 5 times and continues to try if progress is being made
towards the resize.</p>


<p style="margin-top: 1em">--add-temp-swap&lt;=count&gt;</p>

<p style="margin-top: 1em">This options is specified with
--pool-pages-min to initialize a temporary swap file for the
duration of the pool resize. When increasing the size of the
pool, it can be <br>
necessary to reclaim pages so that contiguous memory is
freed and this often requires swap to be successful. Swap is
only created for a positive resize, and is then <br>
removed once the resize operation is completed. The default
swap size is 5 huge pages, the optional argument
&lt;count&gt; sets the swap size to &lt;count&gt; huge
pages.</p>

<p style="margin-top: 1em">--add-ramdisk-swap</p>

<p style="margin-top: 1em">This option is specified with
--pool-pages-min to initialize swap in memory on ram disks.
When increasing the size of the pool, it can be necessary to
reclaim pages so <br>
that contiguous memory is freed and this often requires swap
to be successful. If there isn&rsquo;t enough free disk
space, swap can be initialized in RAM using this option.
<br>
If the size of one ramdisk is not greater than the huge page
size, then swap is initialized on multiple ramdisks. Swap is
only created for a positive resize, and by <br>
default is removed once the resize operation is
completed.</p>

<p style="margin-top: 1em">--persist</p>

<p style="margin-top: 1em">This option is specified with
the --add-temp-swap or --add-ramdisk-swap to make the swap
space persist after the resize operation is completed. The
swap spaces can later <br>
be removed manually using the swapoff command.</p>

<p style="margin-top: 1em">The following options tune the
transparent huge page usage</p>

<p style="margin-top: 1em">--thp-always</p>

<p style="margin-top: 1em">Enable transparent huge pages
always</p>

<p style="margin-top: 1em">--thp-madvise</p>

<p style="margin-top: 1em">Enable transparent huge pages
only on madvised regions</p>

<p style="margin-top: 1em">--thp-never</p>

<p style="margin-top: 1em">Disable transparent huge
pages</p>

<p style="margin-top: 1em">--thp-khugepaged-pages &lt;pages
to scan&gt;</p>

<p style="margin-top: 1em">Configure the number of pages
that khugepaged should scan on each pass</p>

<p style="margin-top: 1em">--thp-khugepaged-scan-sleep
&lt;milliseconds&gt;</p>

<p style="margin-top: 1em">Configure how many milliseconds
khugepaged should wait between passes</p>

<p style="margin-top: 1em">--thp-khugepages-alloc-sleep
&lt;milliseconds&gt;</p>

<p style="margin-top: 1em">Configure how many milliseconds
khugepaged should wait after failing to allocate a huge page
to throttle the next attempt.</p>

<p style="margin-top: 1em">The following options affect the
verbosity of libhugetlbfs.</p>

<p style="margin-top: 1em">--verbose &lt;level&gt;, -v</p>

<p style="margin-top: 1em">The default value for the
verbosity level is 1 and the range of the value can be set
with --verbose from 0 to 99. The higher the value, the more
verbose the library will <br>
be. 0 is quiet and 3 will output much debugging information.
The verbosity level is increased by one each time -v is
specified.</p>

<p style="margin-top: 1em">SEE ALSO <br>
oprofile(1), pagesize(1), libhugetlbfs(7), hugectl(8),</p>

<p style="margin-top: 1em">AUTHORS <br>
libhugetlbfs was written by various people on the
libhugetlbfs-devel mailing list.</p>

<p style="margin-top: 1em">October 1, 2009 HUGEADM(8)</p>
<hr>
</body>
</html>
