<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 16:26:08 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>MPIEXEC(1) LAM COMMANDS MPIEXEC(1)</p>

<p style="margin-top: 1em">NAME <br>
mpiexec - Run MPI programs on LAM nodes.</p>

<p style="margin-top: 1em">SYNOPSIS <br>
mpiexec [global_args] local_args1 [: local_args2 [...]]</p>

<p style="margin-top: 1em">mpiexec [global_args]
-configfile filename</p>

<p style="margin-top: 1em">OPTIONS <br>
Global arguments apply to all commands that will be launched
by mpiexec. They come at the beginning of the command
line.</p>

<p style="margin-top: 1em">-boot Boot the LAM run-time
environment before running the MPI program. If -machinefile
is not specified, use the default boot schema. When the MPI
processes finish, the <br>
LAM run-time environment will be shut down.</p>

<p style="margin-top: 1em">-boot-args args <br>
Pass arguments to the back-end lamboot command when booting
the LAM run-time environment. Implies -boot.</p>

<p style="margin-top: 1em">-d Enable lots of debugging
output. Implies -v.</p>

<p style="margin-top: 1em">-machinefile hostfile <br>
Enable &quot;one shot&quot; MPI executions; boot the LAM
run-time environment with the boot schema specified by
hostfile (see bhost(5)), run the MPI program, and then shut
down <br>
the LAM run-time environment. Implies -boot.</p>

<p style="margin-top: 1em">-prefix lam/install/path <br>
Use the LAM installation specified in /lam/install/path/.
Not compatible with LAM/MPI versions prior to 7.1.</p>

<p style="margin-top: 1em">-ssi key value <br>
Set the SSI parameter key to the value value.</p>

<p style="margin-top: 1em">-tv Launch the MPI processes
under the TotalView debugger.</p>

<p style="margin-top: 1em">-v Be verbose</p>

<p style="margin-top: 1em">One or more sets of local
arguments must be specified (or a config file; see below).
Local arguments essentially include everything allowed in an
appschema(5) as well as the <br>
following options specified by the MPI-2 standard (note that
the options listed below must be specified before appschema
arguments):</p>

<p style="margin-top: 1em">-n numprocs <br>
Number of copies of the process to start.</p>

<p style="margin-top: 1em">-host hostname <br>
Specify the hostname to start the MPI process on. The
hostname must be resolvable by the lamnodes command after
the LAM run-time environment is booted (see <br>
lamnodes(1)).</p>

<p style="margin-top: 1em">-arch architecture <br>
Specify the architecture to start the MPI process on.
mpiexec essentially uses the provided architecture as a
pattern match against the output of the GNU config.guess
<br>
utility on each machine in the LAM run-time environment. Any
subset will match. See EXAMPLES, below.</p>

<p style="margin-top: 1em">-wdir directory <br>
Set the working directory of the executable.</p>

<p style="margin-top: 1em">-soft Not yet supported.</p>

<p style="margin-top: 1em">-path Not yet supported.</p>

<p style="margin-top: 1em">-file Not yet supported.</p>

<p style="margin-top: 1em">other_arguments <br>
When mpiexec first encounters an argument that it
doesn&rsquo;t recognize (such as an appschema(5) argument,
or the MPI executable name), the remainder of the arguments
will <br>
be passed back to mpirun to actually start the process. As
such, all of mpiexec&rsquo;s arguments that are described
above must come before appschema arguments and/or the <br>
MPI executable name. Similarly, all arguments after the MPI
executable name will be transparently passed as command line
argument to the MPI process and will be will <br>
be effectively ignored by mpirun.</p>

<p style="margin-top: 1em">DESCRIPTION <br>
mpiexec is loosely defined in the Miscellany chapter of the
MPI-2 standard (see http://www.mpi-forum.org/). It is meant
to be a portable mechanism for starting MPI processes. <br>
The MPI-2 standard recommends several command line options,
but does not mandate any. LAM&rsquo;s mpiexec currently
supports several of these options, but not all.</p>

<p style="margin-top: 1em">LAM&rsquo;s mpiexec is actually
a perl script that is a wrapper around several underlying
LAM commands, most notably lamboot, mpirun, and lamhalt. As
such, the functionality provided <br>
by mpiexec can always be performed manually. Unless
otherwise specified in arguments that are passed back to
mpirun, mpiexec will use the per-CPU scheduling as described
in <br>
mpirun(1) (i.e., the &quot;cX&quot; and &quot;C&quot;
notation).</p>

<p style="margin-top: 1em">mpiexec can either use an
already-existing LAM universe (i.e., a booted LAM run-time
environment), similar to mpirun, or can be used for
&quot;one-shot&quot; MPI executions where it boots <br>
the LAM run-time environment, runs the MPI executable(s),
and then shuts down the LAM run-time environment.</p>

<p style="margin-top: 1em">mpiexec can also be used to
launch MPMD MPI jobs from the command line. mpirun also
supports launching MPMD MPI jobs, but the user must make a
text file appschema(5) first.</p>

<p style="margin-top: 1em">Perhaps one of mpiexec&rsquo;s
most useful features is the command-line ability to launch
different executables on different architectures using the
-arch flag (see EXAMPLES, below). <br>
Essentially, the string argument that is given to -arch is
used as a pattern match against the output of the GNU
config.guess utility on each node. If the user-provided <br>
architecture string matches any subset of the output of
config.guess, it is ruled a match. Wildcards are not
possible. The GNU config.guess utility is available both in
the <br>
LAM/MPI source code distribution (in the config
subdirectory) and at
ftp://ftp.gnu.org/gnu/config/config.guess.</p>

<p style="margin-top: 1em">Some sample outputs from
config.guess include:</p>

<p style="margin-top: 1em">sparc-sun-solaris2.8 <br>
Solaris 2.8 running on a SPARC platform.</p>

<p style="margin-top: 1em">i686-pc-linux-gnu <br>
Linux running on an i686 architecture.</p>

<p style="margin-top: 1em">mips-sgi-irix6.5 <br>
IRIX 6.5 running on an SGI/MIPS architecture.</p>

<p style="margin-top: 1em">You might want to run the
laminfo command on your available platforms to see what
string config.guess reported. See laminfo(1) for more
details (e.g., the -arch flag to <br>
laminfo).</p>

<p style="margin-top: 1em">Configfile option <br>
It is possible to specify any set of local parameters in a
configuration file rather than on the command line using the
-configfile option. This option is typically used when <br>
the number of command line options is too large for some
shells, or when automated processes generate the command
line arguments and it is simply more convenient to put them
in a <br>
file for later processing by mpiexec.</p>

<p style="margin-top: 1em">The config file can contain both
comments and one or more sets of local arguments. Lines
beginning with &quot;#&quot; are considered comments and are
ignored. Other lines are considered <br>
to be one or more groups of local arguments. Each group must
be separated by either a newline or a colon (&quot;:&quot;).
For example:</p>

<p style="margin-top: 1em"># Sample mpiexec config file
<br>
# Launch foo on two nodes <br>
-host node1.example.com foo : -host node2.example.com foo
<br>
# Launch two copies of bar on a third node <br>
-host node3.example.com -np 2 bar</p>

<p style="margin-top: 1em">ERRORS <br>
In the event of an error, mpiexec will do its best to shut
everything down and return to the state before it was
executed. For example, if mpiexec was used to boot a LAM
run- <br>
time environment, mpiexec will do its best to take down
whatever successfully booted of the run-time environment (to
include invoking lamhalt and/or lamwipe).</p>

<p style="margin-top: 1em">EXAMPLES <br>
The following are some examples of how to use mpiexec. Note
that all examples assume the CPU-based scheduling (which
does NOT map to physical CPUs) as described in
mpirun(1).</p>

<p style="margin-top: 1em">mpiexec -n 4 my_mpi_program <br>
Launch 4 copies of my_mpi_program in an already-existing LAM
universe.</p>

<p style="margin-top: 1em">mpiexec -n 4 my_mpi_program arg1
arg2 <br>
Similar to the previous example, but pass &quot;arg1&quot;
and &quot;arg2&quot; as command line arguments to each copy
of my_mpi_program.</p>

<p style="margin-top: 1em">mpiexec -ssi rpi gm -n 4
my_mpi_program <br>
Similar to the previous example, but pass &quot;-ssi rpi
gm&quot; back to mpirun to tell the MPI processes to use the
Myrinet (gm) RPI for MPI message passing.</p>

<p style="margin-top: 1em">mpiexec -n 4 program1 : -n 4
program2 <br>
Launch 4 copies of program1 and 4 copies of program2 in an
already-existing LAM universe. All 8 resulting processes
will share a common MPI_COMM_WORLD.</p>

<p style="margin-top: 1em">mpiexec -machinefile hostfile -n
4 my_mpi_program <br>
Boot the LAM run-time environment with the nodes listed in
the hostfile, run 4 copies of my_mpi_program in the
resulting LAM universe, and then shut down the LAM <br>
universe.</p>

<p style="margin-top: 1em">mpiexec -machinefile hostfile
my_mpi_program <br>
Similar to above, but run my_mpi_program on all available
CPUs in the LAM universe.</p>

<p style="margin-top: 1em">mpiexec -arch solaris2.8
sol_program : -arch linux linux_program <br>
Run as many copies of sol_program as there are CPUs on
Solaris machines in the current LAM universe, and as many
copies of linux_program as there are CPUs on linux <br>
machines in the current LAM universe. All resulting
processes will share a common MPI_COMM_WORLD.</p>

<p style="margin-top: 1em">mpiexec -arch solaris2.8
sol2.8_prog : -arch solaris2.9 sol2.9_program <br>
Similar to the above example, except distinguish between
Solaris 2.8 and 2.9 (since they may have different shared
libraries, etc.).</p>

<p style="margin-top: 1em">SEE ALSO <br>
appschema(5), bhost(5), lamboot(1), lamexec(1),
mpirun(1)</p>

<p style="margin-top: 1em">LAM 7.1.4 July, 2007
MPIEXEC(1)</p>
<hr>
</body>
</html>
