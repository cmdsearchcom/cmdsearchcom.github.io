<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8"/>
  <style>
    table.head, table.foot { width: 100%; }
    td.head-rtitle, td.foot-os { text-align: right; }
    td.head-vol { text-align: center; }
    div.Pp { margin: 1ex 0ex; }
  </style>
  <title>CHECKBOT(1p)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">CHECKBOT(1p)</td>
    <td class="head-vol">User Contributed Perl Documentation</td>
    <td class="head-rtitle">CHECKBOT(1p)</td>
  </tr>
</table>
<div class="manual-text">
<h1 class="Sh" title="Sh" id="NAME"><a class="selflink" href="#NAME">NAME</a></h1>
Checkbot - WWW Link Verifier
<h1 class="Sh" title="Sh" id="SYNOPSIS"><a class="selflink" href="#SYNOPSIS">SYNOPSIS</a></h1>
checkbot [ <b>--cookies</b>] [<b>--debug</b>] [<b>--file</b> file name]
  [<b>--help</b>]
<br/>
 [ <b>--mailto</b> email addresses] [<b>--noproxy</b> list of domains]
<br/>
 [ <b>--verbose</b>]
<br/>
 [ <b>--url</b> start URL]
<br/>
 [ <b>--match</b> match string] [<b>--exclude</b> exclude string]
<br/>
 [ <b>--proxy</b> proxy URL] [<b>--internal-only</b>]
<br/>
 [ <b>--ignore</b> ignore string]
<br/>
 [ <b>--filter</b> substitution regular expression]
<br/>
 [ <b>--style</b> style file URL]
<br/>
 [ <b>--note</b> note] [<b>--sleep</b> seconds] [<b>--timeout</b> timeout]
<br/>
 [ <b>--interval</b> seconds] [<b>--dontwarn</b> HTTP responde codes]
<br/>
 [ <b>--enable-virtual</b>]
<br/>
 [ <b>--language</b> language code]
<br/>
 [ <b>--suppress</b> suppression file]
<br/>
 [start URLs]
<h1 class="Sh" title="Sh" id="DESCRIPTION"><a class="selflink" href="#DESCRIPTION">DESCRIPTION</a></h1>
Checkbot verifies the links in a specific portion of the World Wide Web. It
  creates HTML pages with diagnostics.
<div class="Pp"></div>
Checkbot uses LWP to find URLs on pages and to check them. It supports the same
  schemes as LWP does, and finds the same links that HTML::LinkExtor will find.
<div class="Pp"></div>
Checkbot considers links to be either 'internal' or 'external'. Internal links
  are links within the web space that needs to be checked. If an internal link
  points to a web document this document is retrieved, and its links are
  extracted and processed. External links are only checked to be working.
  Checkbot checks links as it finds them, so internal and external links are
  checked at the same time, even though they are treated differently.
<div class="Pp"></div>
Options for Checkbot are:
<dl class="Bl-tag">
  <dt class="It-tag">--cookies</dt>
  <dd class="It-tag">Accept cookies from the server and offer them again at
      later requests. This may be useful for servers that use cookies to handle
      sessions. By default Checkbot does not accept any cookies.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--debug</dt>
  <dd class="It-tag">Enable debugging mode. Not really supported anymore, but it
      will keep some files around that otherwise would be deleted.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--file &lt;file name&gt;</dt>
  <dd class="It-tag">Use the file <i>file name</i> as the basis for the summary
      file names. The summary page will get the <i>file name</i> given, and the
      server pages are based on the <i>file name</i> without the .html
      extension. For example, setting this option to &quot;index.html&quot; will
      create a summary page called index.html and server pages called
      index-server1.html and index-server2.html.
    <div style="height: 1.00em;">&#x00A0;</div>
    The default value for this option is &quot;checkbot.html&quot;.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--help</dt>
  <dd class="It-tag">Shows brief help message on the standard output.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--mailto &lt;email address&gt;[,&lt;email address&gt;]</dt>
  <dd class="It-tag">Send mail to the <i>email address</i> when Checkbot is done
      checking. You can give more than one address separated by commas. The
      notification email includes a small summary of the results. As of Checkbot
      1.76 email is only sent if problems have been found during the Checkbot
      run.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--noproxy &lt;list of domains&gt;</dt>
  <dd class="It-tag">Do not proxy requests to the given domains. The list of
      domains must be a comma-separated list. For example, so avoid using the
      proxy for the localhost and someserver.xyz, you can use &quot;--noproxy
      localhost,someserver.xyz&quot;.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--verbose</dt>
  <dd class="It-tag">Show verbose output while running. Includes all links
      checked, results from the checks, etc.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--url &lt;start URL&gt;</dt>
  <dd class="It-tag">Set the start URL. Checkbot starts checking at this URL,
      and then recursively checks all links found on this page. The start URL
      takes precedence over additional URLs specified on the command line.
    <div style="height: 1.00em;">&#x00A0;</div>
    If no scheme is specified for the URL, the file protocol is assumed.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--match &lt;match string&gt;</dt>
  <dd class="It-tag">This option selects which pages Checkbot considers local.
      If the <i>match string</i> is contained within the URL, then Checkbot
      considers the page local, retrieves it, and will check all the links
      contained on it. Otherwise the page is considered external and it is only
      checked with a HEAD request.
    <div style="height: 1.00em;">&#x00A0;</div>
    If no explicit <i>match string</i> is given, the start URLs (See option
      &quot;--url&quot;) will be used as a match string instead. In this case
      the last page name, if any, will be trimmed. For example, a start URL like
      &quot;http://some.site/index.html&quot; will result in a default
      <i>match</i> <i>string</i> of &quot;http://some.site/&quot;.
    <div style="height: 1.00em;">&#x00A0;</div>
    The <i>match string</i> can be a perl regular expression. For example, to
      check the main server page and all HTML pages directly underneath it, but
      not the HTML pages in the subdirectories of the server, the <i>match
      string</i> would be &quot;www.someserver.xyz/($|[^/]+.html)&quot;.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--exclude &lt;exclude string&gt;</dt>
  <dd class="It-tag">URLs matching the <i>exclude string</i> are considered to
      be external, even if they happen to match the <i>match string</i> (See
      option &quot;--match&quot;). URLs matching the --exclude string are still
      being checked and will be reported if problems are found, but they will
      not be checked for further links into the site.
    <div style="height: 1.00em;">&#x00A0;</div>
    The <i>exclude string</i> can be a perl regular expression. For example, to
      consider all URLs with a query string external, use &quot;[=\?]&quot;.
      This can be useful when a URL with a query string unlocks the path to a
      huge database which will be checked.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--filter &lt;filter string&gt;</dt>
  <dd class="It-tag">This option defines a <i>filter string</i>, which is a perl
      regular expression. This filter is run on each URL found, thus rewriting
      the URL before it enters the queue to be checked. It can be used to remove
      elements from a URL. This option can be useful when symbolic links point
      to the same directory, or when a content management system adds session
      IDs to URLs.
    <div style="height: 1.00em;">&#x00A0;</div>
    For example &quot;/old/new/&quot; would replace occurrences of 'old' with
      'new' in each URL.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--ignore &lt;ignore string&gt;</dt>
  <dd class="It-tag">URLs matching the <i>ignore string</i> are not checked at
      all, they are completely ignored by Checkbot. This can be useful to ignore
      known problem links, or to ignore links leading into databases. The
      <i>ignore</i> <i>string</i> is matched after the <i>filter string</i> has
      been applied.
    <div style="height: 1.00em;">&#x00A0;</div>
    The <i>ignore string</i> can be a perl regular expression.
    <div style="height: 1.00em;">&#x00A0;</div>
    For example &quot;www.server.com\/(one|two)&quot; would match all URLs
      starting with either www.server.com/one or www.server.com/two.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--proxy &lt;proxy URL&gt;</dt>
  <dd class="It-tag">This attribute specifies the URL of a proxy server. Only
      the HTTP and FTP requests will be sent to that proxy server.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--internal-only</dt>
  <dd class="It-tag">Skip the checking of external links at the end of the
      Checkbot run. Only matching links are checked. Note that some redirections
      may still cause external links to be checked.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--note &lt;note&gt;</dt>
  <dd class="It-tag">The <i>note</i> is included verbatim in the mail message
      (See option &quot;--mailto&quot;). This can be useful to include the URL
      of the summary HTML page for easy reference, for instance.
    <div style="height: 1.00em;">&#x00A0;</div>
    Only meaningful in combination with the &quot;--mailto&quot; option.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--sleep &lt;seconds&gt;</dt>
  <dd class="It-tag">Number of <i>seconds</i> to sleep in between requests.
      Default is 0 seconds, i.e. do not sleep at all between requests. Setting
      this option can be useful to keep the load on the web server down while
      running Checkbot. This option can also be set to a fractional number, i.e.
      a value of 0.1 will sleep one tenth of a second between requests.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--timeout &lt;timeout&gt;</dt>
  <dd class="It-tag">Default timeout for the requests, specified in seconds. The
      default is 2 minutes.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--interval &lt;seconds&gt;</dt>
  <dd class="It-tag">The maximum interval between updates of the results web
      pages in seconds. Default is 3 hours (10800 seconds). Checkbot will start
      the interval at one minute, and gradually extend it towards the maximum
      interval.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--style &lt;URL of style file&gt;</dt>
  <dd class="It-tag">When this option is used, Checkbot embeds this URL as a
      link to a style file on each page it writes. This makes it easy to
      customize the layout of pages generated by Checkbot.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--dontwarn &lt;HTTP response codes regular
    expression&gt;</dt>
  <dd class="It-tag">Do not include warnings on the result pages for those HTTP
      response codes which match the regular expression. For instance,
      --dontwarn &quot;(301|404)&quot; would not include 301 and 404 response
      codes.
    <div style="height: 1.00em;">&#x00A0;</div>
    Checkbot uses the response codes generated by the server, even if this
      response code is not defined in RFC 2616 (HTTP/1.1). In addition to the
      normal HTTP response code, Checkbot defines a few response codes for
      situations which are not technically a problem, but which causes problems
      in many cases anyway. These codes are:
    <div style="height: 1.00em;">&#x00A0;</div>
    <pre>
  901 Host name expected but not found
      In this case the URL supports a host name, but non was found
      in the URL. This usually indicates a mistake in the URL. An
      exception is that this check is not applied to news: URLs.
  902 Unqualified host name found
      In this case the host name does not contain the domain part.
      This usually means that the pages work fine when viewed within
      the original domain, but not when viewed from outside it.
  903 Double slash in URL path
      The URL has a double slash in it. This is legal, but some web
      servers cannot handle it very well and may cause Checkbot to
      run away. See also the comments below.
  904 Unknown scheme in URL
      The URL starts with a scheme that Checkbot does not know
      about. This is often caused by mistyping the scheme of the URL,
      but the scheme can also be a legal one. In that case please let
      me know so that it can be added to Checkbot.
    </pre>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--enable-virtual</dt>
  <dd class="It-tag">This option enables dealing with virtual servers. Checkbot
      then assumes that all hostnames for internal servers are unique, even
      though their IP addresses may be the same. Normally Checkbot uses the IP
      address to distinguish servers. This has the advantage that if a server
      has two names (e.g. www and bamboozle) its pages only get checked once.
      When you want to check multiple virtual servers this causes problems,
      which this feature works around by using the hostname to distinguish the
      server.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--language</dt>
  <dd class="It-tag">The argument for this option is a two-letter language code.
      Checkbot will use language negotiation to request files in that language.
      The default is to request English language (language code 'en').</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--suppress</dt>
  <dd class="It-tag">The argument for this option is a file which contains
      combinations of error codes and URLs for which to suppress warnings. This
      can be used to avoid reporting of known and unfixable URL errors or
      warnings.
    <div style="height: 1.00em;">&#x00A0;</div>
    The format of the suppression file is a simple whitespace delimited format,
      first listing the error code followed by the URL. Each error code and URL
      combination is listed on a new line. Comments can be added to the file by
      starting the line with a &quot;#&quot; character.
    <div style="height: 1.00em;">&#x00A0;</div>
    <pre>
  # 301 Moved Permanently
  301   http://www.w3.org/P3P
  
  # 403 Forbidden
  403   http://www.herring.com/
    </pre>
    <div style="height: 1.00em;">&#x00A0;</div>
    For further flexibility a regular expression can be used instead of a normal
      URL. The regular expression must be enclosed with forward slashes. For
      example, to suppress all 403 errors on wikipedia:
    <div style="height: 1.00em;">&#x00A0;</div>
    <pre>
  403   /http:\/\/wikipedia.org\/.*/
    </pre>
  </dd>
</dl>
<div class="Pp"></div>
Deprecated options which will disappear in a future release:
<dl class="Bl-tag">
  <dt class="It-tag">--allow-simple-hosts (deprecated)</dt>
  <dd class="It-tag">This option turns off warnings about URLs which contain
      unqualified host names. This is useful for intranet sites which often use
      just a simple host name or even &quot;localhost&quot; in their links.
    <div style="height: 1.00em;">&#x00A0;</div>
    Use of this option is deprecated. Please use the --dontwarn mechanism for
      error 902 instead.</dd>
</dl>
<h1 class="Sh" title="Sh" id="HINTS_AND_TIPS"><a class="selflink" href="#HINTS_AND_TIPS">HINTS
  AND TIPS</a></h1>
<dl class="Bl-tag">
  <dt class="It-tag">Problems with checking FTP links</dt>
  <dd class="It-tag">Some users may experience consistent problems with checking
      FTP links. In these cases it may be useful to instruct Net::FTP to use
      passive FTP mode to check files. This can be done by setting the
      environment variable FTP_PASSIVE to 1. For example, using the bash shell:
      &quot;FTP_PASSIVE=1 checkbot ...&quot;. See the Net::FTP documentation for
      more details.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">Run-away Checkbot</dt>
  <dd class="It-tag">In some cases Checkbot literally takes forever to finish.
      There are two common causes for this problem.
    <div style="height: 1.00em;">&#x00A0;</div>
    First, there might be a database application as part of the web site which
      generates a new page based on links on another page. Since Checkbot tries
      to travel through all links this will create an infinite number of pages.
      This kind of run-away effect is usually predictable. It can be avoided by
      using the --exclude option.
    <div style="height: 1.00em;">&#x00A0;</div>
    Second, a server configuration problem can cause a loop in generating URLs
      for pages that really do not exist. This will result in URLs of the form
      http://some.server/images/images/images/logo.png, with ever more 'images'
      included. Checkbot cannot check for this because the server should have
      indicated that the requested pages do not exist. There is no easy way to
      solve this other than fixing the offending web server or the broken
    links.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">Problems with https:// links</dt>
  <dd class="It-tag">The error message
    <div style="height: 1.00em;">&#x00A0;</div>
    <pre>
  Can't locate object method &quot;new&quot; via package &quot;LWP::Protocol::https::Socket&quot;
    </pre>
    <div style="height: 1.00em;">&#x00A0;</div>
    usually means that the current installation of LWP does not support checking
      of SSL links (i.e. links starting with https://). This problem can be
      solved by installing the Crypt::SSLeay module.</dd>
</dl>
<h1 class="Sh" title="Sh" id="EXAMPLES"><a class="selflink" href="#EXAMPLES">EXAMPLES</a></h1>
The most simple use of Checkbot is to check a set of pages on a server. To check
  my checkbot pages I would use:
<div class="Pp"></div>
<pre>
    checkbot http://degraaff.org/checkbot/
</pre>
<div class="Pp"></div>
Checkbot runs can take some time so Checkbot can send a notification mail when
  the run is done:
<div class="Pp"></div>
<pre>
    checkbot --mailto hans@degraaff.org http://degraaff.org/checkbot/
</pre>
<div class="Pp"></div>
It is possible to check a set of local file without using a web server. This
  only works for static files but may be useful in some cases.
<div class="Pp"></div>
<pre>
    checkbot file:///var/www/documents/
</pre>
<h1 class="Sh" title="Sh" id="PREREQUISITES"><a class="selflink" href="#PREREQUISITES">PREREQUISITES</a></h1>
This script uses the &quot;LWP&quot; modules.
<h1 class="Sh" title="Sh" id="COREQUISITES"><a class="selflink" href="#COREQUISITES">COREQUISITES</a></h1>
This script can send mail when &quot;Mail::Send&quot; is present.
<h1 class="Sh" title="Sh" id="AUTHOR"><a class="selflink" href="#AUTHOR">AUTHOR</a></h1>
Hans de Graaff &lt;hans@degraaff.org&gt;
<div class="Pp"></div>
any</div>
<table class="foot">
  <tr>
    <td class="foot-date">2008-10-15</td>
    <td class="foot-os">perl v5.14.2</td>
  </tr>
</table>
</body>
</html>
