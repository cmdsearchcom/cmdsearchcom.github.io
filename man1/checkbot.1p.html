<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 15:58:13 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>CHECKBOT(1p) User Contributed Perl Documentation
CHECKBOT(1p)</p>

<p style="margin-top: 1em">NAME <br>
Checkbot - WWW Link Verifier</p>

<p style="margin-top: 1em">SYNOPSIS <br>
checkbot [--cookies] [--debug] [--file file name] [--help]
<br>
[--mailto email addresses] [--noproxy list of domains] <br>
[--verbose] <br>
[--url start URL] <br>
[--match match string] [--exclude exclude string] <br>
[--proxy proxy URL] [--internal-only] <br>
[--ignore ignore string] <br>
[--filter substitution regular expression] <br>
[--style style file URL] <br>
[--note note] [--sleep seconds] [--timeout timeout] <br>
[--interval seconds] [--dontwarn HTTP responde codes] <br>
[--enable-virtual] <br>
[--language language code] <br>
[--suppress suppression file] <br>
[start URLs]</p>

<p style="margin-top: 1em">DESCRIPTION <br>
Checkbot verifies the links in a specific portion of the
World Wide Web. It creates HTML pages with diagnostics.</p>

<p style="margin-top: 1em">Checkbot uses LWP to find URLs
on pages and to check them. It supports the same schemes as
LWP does, and finds the same links that HTML::LinkExtor will
find.</p>

<p style="margin-top: 1em">Checkbot considers links to be
either &rsquo;internal&rsquo; or &rsquo;external&rsquo;.
Internal links are links within the web space that needs to
be checked. If an internal link points to a web <br>
document this document is retrieved, and its links are
extracted and processed. External links are only checked to
be working. Checkbot checks links as it finds them, so <br>
internal and external links are checked at the same time,
even though they are treated differently.</p>

<p style="margin-top: 1em">Options for Checkbot are:</p>

<p style="margin-top: 1em">--cookies <br>
Accept cookies from the server and offer them again at later
requests. This may be useful for servers that use cookies to
handle sessions. By default Checkbot does not accept <br>
any cookies.</p>

<p style="margin-top: 1em">--debug <br>
Enable debugging mode. Not really supported anymore, but it
will keep some files around that otherwise would be
deleted.</p>

<p style="margin-top: 1em">--file &lt;file name&gt; <br>
Use the file file name as the basis for the summary file
names. The summary page will get the file name given, and
the server pages are based on the file name without the <br>
.html extension. For example, setting this option to
&quot;index.html&quot; will create a summary page called
index.html and server pages called index-server1.html and
<br>
index-server2.html.</p>

<p style="margin-top: 1em">The default value for this
option is &quot;checkbot.html&quot;.</p>

<p style="margin-top: 1em">--help <br>
Shows brief help message on the standard output.</p>

<p style="margin-top: 1em">--mailto &lt;email
address&gt;[,&lt;email address&gt;] <br>
Send mail to the email address when Checkbot is done
checking. You can give more than one address separated by
commas. The notification email includes a small summary of
the <br>
results. As of Checkbot 1.76 email is only sent if problems
have been found during the Checkbot run.</p>

<p style="margin-top: 1em">--noproxy &lt;list of
domains&gt; <br>
Do not proxy requests to the given domains. The list of
domains must be a comma-separated list. For example, so
avoid using the proxy for the localhost and someserver.xyz,
<br>
you can use &quot;--noproxy
localhost,someserver.xyz&quot;.</p>

<p style="margin-top: 1em">--verbose <br>
Show verbose output while running. Includes all links
checked, results from the checks, etc.</p>

<p style="margin-top: 1em">--url &lt;start URL&gt; <br>
Set the start URL. Checkbot starts checking at this URL, and
then recursively checks all links found on this page. The
start URL takes precedence over additional URLs <br>
specified on the command line.</p>

<p style="margin-top: 1em">If no scheme is specified for
the URL, the file protocol is assumed.</p>

<p style="margin-top: 1em">--match &lt;match string&gt;
<br>
This option selects which pages Checkbot considers local. If
the match string is contained within the URL, then Checkbot
considers the page local, retrieves it, and will <br>
check all the links contained on it. Otherwise the page is
considered external and it is only checked with a HEAD
request.</p>

<p style="margin-top: 1em">If no explicit match string is
given, the start URLs (See option &quot;--url&quot;) will be
used as a match string instead. In this case the last page
name, if any, will be trimmed. <br>
For example, a start URL like
&quot;http://some.site/index.html&quot; will result in a
default match string of &quot;http://some.site/&quot;.</p>

<p style="margin-top: 1em">The match string can be a perl
regular expression. For example, to check the main server
page and all HTML pages directly underneath it, but not the
HTML pages in the <br>
subdirectories of the server, the match string would be
&quot;www.someserver.xyz/($|[^/]+.html)&quot;.</p>

<p style="margin-top: 1em">--exclude &lt;exclude string&gt;
<br>
URLs matching the exclude string are considered to be
external, even if they happen to match the match string (See
option &quot;--match&quot;). URLs matching the --exclude
string are <br>
still being checked and will be reported if problems are
found, but they will not be checked for further links into
the site.</p>

<p style="margin-top: 1em">The exclude string can be a perl
regular expression. For example, to consider all URLs with a
query string external, use &quot;[= string unlocks the path
to a huge database which will be checked.</p>

<p style="margin-top: 1em">--filter &lt;filter string&gt;
<br>
This option defines a filter string, which is a perl regular
expression. This filter is run on each URL found, thus
rewriting the URL before it enters the queue to be <br>
checked. It can be used to remove elements from a URL. This
option can be useful when symbolic links point to the same
directory, or when a content management system adds <br>
session IDs to URLs.</p>

<p style="margin-top: 1em">For example
&quot;/old/new/&quot; would replace occurrences of
&rsquo;old&rsquo; with &rsquo;new&rsquo; in each URL.</p>

<p style="margin-top: 1em">--ignore &lt;ignore string&gt;
<br>
URLs matching the ignore string are not checked at all, they
are completely ignored by Checkbot. This can be useful to
ignore known problem links, or to ignore links leading <br>
into databases. The ignore string is matched after the
filter string has been applied.</p>

<p style="margin-top: 1em">The ignore string can be a perl
regular expression.</p>

<p style="margin-top: 1em">For example
&quot;www.server.com(one|two)&quot; would match all URLs
starting with either www.server.com/one or
www.server.com/two.</p>

<p style="margin-top: 1em">--proxy &lt;proxy URL&gt; <br>
This attribute specifies the URL of a proxy server. Only the
HTTP and FTP requests will be sent to that proxy server.</p>

<p style="margin-top: 1em">--internal-only <br>
Skip the checking of external links at the end of the
Checkbot run. Only matching links are checked. Note that
some redirections may still cause external links to be
checked.</p>

<p style="margin-top: 1em">--note &lt;note&gt; <br>
The note is included verbatim in the mail message (See
option &quot;--mailto&quot;). This can be useful to include
the URL of the summary HTML page for easy reference, for
instance.</p>

<p style="margin-top: 1em">Only meaningful in combination
with the &quot;--mailto&quot; option.</p>

<p style="margin-top: 1em">--sleep &lt;seconds&gt; <br>
Number of seconds to sleep in between requests. Default is 0
seconds, i.e. do not sleep at all between requests. Setting
this option can be useful to keep the load on the web <br>
server down while running Checkbot. This option can also be
set to a fractional number, i.e. a value of 0.1 will sleep
one tenth of a second between requests.</p>

<p style="margin-top: 1em">--timeout &lt;timeout&gt; <br>
Default timeout for the requests, specified in seconds. The
default is 2 minutes.</p>

<p style="margin-top: 1em">--interval &lt;seconds&gt; <br>
The maximum interval between updates of the results web
pages in seconds. Default is 3 hours (10800 seconds).
Checkbot will start the interval at one minute, and
gradually <br>
extend it towards the maximum interval.</p>

<p style="margin-top: 1em">--style &lt;URL of style
file&gt; <br>
When this option is used, Checkbot embeds this URL as a link
to a style file on each page it writes. This makes it easy
to customize the layout of pages generated by <br>
Checkbot.</p>

<p style="margin-top: 1em">--dontwarn &lt;HTTP response
codes regular expression&gt; <br>
Do not include warnings on the result pages for those HTTP
response codes which match the regular expression. For
instance, --dontwarn &quot;(301|404)&quot; would not include
301 and <br>
404 response codes.</p>

<p style="margin-top: 1em">Checkbot uses the response codes
generated by the server, even if this response code is not
defined in RFC 2616 (HTTP/1.1). In addition to the normal
HTTP response code, <br>
Checkbot defines a few response codes for situations which
are not technically a problem, but which causes problems in
many cases anyway. These codes are:</p>

<p style="margin-top: 1em">901 Host name expected but not
found <br>
In this case the URL supports a host name, but non was found
<br>
in the URL. This usually indicates a mistake in the URL. An
<br>
exception is that this check is not applied to news:
URLs.</p>

<p style="margin-top: 1em">902 Unqualified host name found
<br>
In this case the host name does not contain the domain part.
<br>
This usually means that the pages work fine when viewed
within <br>
the original domain, but not when viewed from outside
it.</p>

<p style="margin-top: 1em">903 Double slash in URL path
<br>
The URL has a double slash in it. This is legal, but some
web <br>
servers cannot handle it very well and may cause Checkbot to
<br>
run away. See also the comments below.</p>

<p style="margin-top: 1em">904 Unknown scheme in URL <br>
The URL starts with a scheme that Checkbot does not know
<br>
about. This is often caused by mistyping the scheme of the
URL, <br>
but the scheme can also be a legal one. In that case please
let <br>
me know so that it can be added to Checkbot.</p>

<p style="margin-top: 1em">--enable-virtual <br>
This option enables dealing with virtual servers. Checkbot
then assumes that all hostnames for internal servers are
unique, even though their IP addresses may be the same. <br>
Normally Checkbot uses the IP address to distinguish
servers. This has the advantage that if a server has two
names (e.g. www and bamboozle) its pages only get checked
once. <br>
When you want to check multiple virtual servers this causes
problems, which this feature works around by using the
hostname to distinguish the server.</p>

<p style="margin-top: 1em">--language <br>
The argument for this option is a two-letter language code.
Checkbot will use language negotiation to request files in
that language. The default is to request English <br>
language (language code &rsquo;en&rsquo;).</p>

<p style="margin-top: 1em">--suppress <br>
The argument for this option is a file which contains
combinations of error codes and URLs for which to suppress
warnings. This can be used to avoid reporting of known and
<br>
unfixable URL errors or warnings.</p>

<p style="margin-top: 1em">The format of the suppression
file is a simple whitespace delimited format, first listing
the error code followed by the URL. Each error code and URL
combination is listed on <br>
a new line. Comments can be added to the file by starting
the line with a &quot;#&quot; character.</p>

<p style="margin-top: 1em"># 301 Moved Permanently <br>
301 http://www.w3.org/P3P</p>

<p style="margin-top: 1em"># 403 Forbidden <br>
403 http://www.herring.com/</p>

<p style="margin-top: 1em">For further flexibility a
regular expression can be used instead of a normal URL. The
regular expression must be enclosed with forward slashes.
For example, to suppress all <br>
403 errors on wikipedia:</p>

<p style="margin-top: 1em">403 /http:wikipedia.org.*/</p>

<p style="margin-top: 1em">Deprecated options which will
disappear in a future release:</p>

<p style="margin-top: 1em">--allow-simple-hosts
(deprecated) <br>
This option turns off warnings about URLs which contain
unqualified host names. This is useful for intranet sites
which often use just a simple host name or even
&quot;localhost&quot; <br>
in their links.</p>

<p style="margin-top: 1em">Use of this option is
deprecated. Please use the --dontwarn mechanism for error
902 instead.</p>

<p style="margin-top: 1em">HINTS AND TIPS <br>
Problems with checking FTP links <br>
Some users may experience consistent problems with checking
FTP links. In these cases it may be useful to instruct
Net::FTP to use passive FTP mode to check files. This can
<br>
be done by setting the environment variable FTP_PASSIVE to
1. For example, using the bash shell: &quot;FTP_PASSIVE=1
checkbot ...&quot;. See the Net::FTP documentation for more
<br>
details.</p>

<p style="margin-top: 1em">Run-away Checkbot <br>
In some cases Checkbot literally takes forever to finish.
There are two common causes for this problem.</p>

<p style="margin-top: 1em">First, there might be a database
application as part of the web site which generates a new
page based on links on another page. Since Checkbot tries to
travel through all <br>
links this will create an infinite number of pages. This
kind of run-away effect is usually predictable. It can be
avoided by using the --exclude option.</p>

<p style="margin-top: 1em">Second, a server configuration
problem can cause a loop in generating URLs for pages that
really do not exist. This will result in URLs of the form
<br>
http://some.server/images/images/images/logo.png, with ever
more &rsquo;images&rsquo; included. Checkbot cannot check
for this because the server should have indicated that the
requested <br>
pages do not exist. There is no easy way to solve this other
than fixing the offending web server or the broken
links.</p>

<p style="margin-top: 1em">Problems with https:// links
<br>
The error message</p>

<p style="margin-top: 1em">Can&rsquo;t locate object method
&quot;new&quot; via package
&quot;LWP::Protocol::https::Socket&quot;</p>

<p style="margin-top: 1em">usually means that the current
installation of LWP does not support checking of SSL links
(i.e. links starting with https://). This problem can be
solved by installing the <br>
Crypt::SSLeay module.</p>

<p style="margin-top: 1em">EXAMPLES <br>
The most simple use of Checkbot is to check a set of pages
on a server. To check my checkbot pages I would use:</p>

<p style="margin-top: 1em">checkbot
http://degraaff.org/checkbot/</p>

<p style="margin-top: 1em">Checkbot runs can take some time
so Checkbot can send a notification mail when the run is
done:</p>

<p style="margin-top: 1em">checkbot --mailto
hans@degraaff.org http://degraaff.org/checkbot/</p>

<p style="margin-top: 1em">It is possible to check a set of
local file without using a web server. This only works for
static files but may be useful in some cases.</p>

<p style="margin-top: 1em">checkbot
file:///var/www/documents/</p>

<p style="margin-top: 1em">PREREQUISITES <br>
This script uses the &quot;LWP&quot; modules.</p>

<p style="margin-top: 1em">COREQUISITES <br>
This script can send mail when &quot;Mail::Send&quot; is
present.</p>

<p style="margin-top: 1em">AUTHOR <br>
Hans de Graaff &lt;hans@degraaff.org&gt;</p>

<p style="margin-top: 1em">any</p>

<p style="margin-top: 1em">perl v5.14.2 2008-10-15
CHECKBOT(1p)</p>
<hr>
</body>
</html>
