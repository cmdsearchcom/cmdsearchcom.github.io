<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 16:05:51 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>DUPEMAP(1) Magic Rescue DUPEMAP(1)</p>

<p style="margin-top: 1em">NAME <br>
dupemap - Creates a database of file checksums and uses it
to eliminate duplicates</p>

<p style="margin-top: 1em">SYNOPSIS <br>
dupemap [ options ] [ -d database ] operation path...</p>

<p style="margin-top: 1em">DESCRIPTION <br>
dupemap recursively scans each path to find checksums of
file contents. Directories are searched through in no
particular order. Its actions depend on whether the -d
option is <br>
given, and on the operation parameter, which must be a
comma-seperated list of scan, report, delete:</p>

<p style="margin-top: 1em">Without -d <br>
dupemap will take action when it sees the same checksum
repeated more than once, i.e. it simply finds duplicates
recursively. The action depends on operation:</p>

<p style="margin-top: 1em">report Report what files are
encountered more than once, printing their names to standard
output.</p>

<p style="margin-top: 1em">delete[,report] <br>
Delete files that are encountered more than once. Print
their names if report is also given.</p>

<p style="margin-top: 1em">WARNING: use the report
operation first to see what will be deleted.</p>

<p style="margin-top: 1em">WARNING: You are advised to make
a backup of the target first, e.g. with &quot;cp -al&quot;
(for GNU cp) to create hard links recursively.</p>

<p style="margin-top: 1em">With -d <br>
The database argument to -d will denote a database file (see
the &quot;DATABASE&quot; section in this manual for details)
to read from or write to. In this mode, the scan operation
should <br>
be run on one path, followed by the report or delete
operation on another (not the same!) path.</p>

<p style="margin-top: 1em">scan Add the checksum of each
file to database. This operation must be run initially to
create the database. To start over, you must manually delete
the database file(s) (see <br>
the &quot;DATABASE&quot; section).</p>

<p style="margin-top: 1em">report Print each file name if
its checksum is found in database.</p>

<p style="margin-top: 1em">delete[,report] <br>
Delete each file if its checksum is found in database. If
report is also present, print the name of each deleted
file.</p>

<p style="margin-top: 1em">WARNING: if you run dupemap
delete on the same path you just ran dupemap scan on, it
will delete every file! The idea of these options is to scan
one path and delete files <br>
in a second path.</p>

<p style="margin-top: 1em">WARNING: use the report
operation first to see what will be deleted.</p>

<p style="margin-top: 1em">WARNING: You are advised to make
a backup of the target first, e.g. with &quot;cp -al&quot;
(for GNU cp) to create hard links recursively.</p>

<p style="margin-top: 1em">OPTIONS <br>
-d database <br>
Use database as an on-disk database to read from or write
to. See the &quot;DESCRIPTION&quot; section above about how
this influences the operation of dupemap.</p>

<p style="margin-top: 1em">-I file <br>
Reads input files from file in addition to those listed on
the command line. If file is &quot;-&quot;, read from
standard input. Each line will be interpreted as a file
name.</p>

<p style="margin-top: 1em">The paths given here will NOT be
scanned recursively. Directories will be ignored and
symlinks will be followed.</p>

<p style="margin-top: 1em">-m minsize <br>
Ignore files below this size.</p>

<p style="margin-top: 1em">-M maxsize <br>
Ignore files above this size.</p>

<p style="margin-top: 1em">USAGE <br>
General usage <br>
The easiest operations to understand is when the -d option
is not given. To delete all duplicate files in
/tmp/recovered-files, do:</p>

<p style="margin-top: 1em">$ dupemap delete
/tmp/recovered-files</p>

<p style="margin-top: 1em">Often, dupemap scan is run to
produce a checksum database of all files in a directory
tree. Then dupemap delete is run on another directory,
possibly following dupemap report. <br>
For example, to delete all files in /tmp/recovered-files
that already exist in $HOME, do this:</p>

<p style="margin-top: 1em">$ dupemap -d homedir.map scan
$HOME <br>
$ dupemap -d homedir.map delete,report
/tmp/recovered-files</p>

<p style="margin-top: 1em">Usage with magicrescue <br>
The main application for dupemap is to take some pain out of
performing undelete operations with magicrescue(1). The
reason is that magicrescue will extract every single file of
<br>
the specified type on the block device, so undeleting files
requires you to find a few files out of hundreds, which can
take a long time if done manually. What we want to do is
<br>
to only extract the documents that don&rsquo;t exist on the
file system already.</p>

<p style="margin-top: 1em">In the following scenario, you
have accidentally deleted some important Word documents in
Windows. If this were a real-world scenario, then by all
means use The Sleuth Kit. <br>
However, magicrescue will work even when the directory
entries were overwritten, i.e. more files were stored in the
same folder later.</p>

<p style="margin-top: 1em">You boot into Linux and change
to a directory with lots of space. Mount the Windows
partition, preferably read-only (especially with NTFS), and
create the directories we will <br>
use.</p>

<p style="margin-top: 1em">$ mount -o ro /dev/hda1
/mnt/windows <br>
$ mkdir healthy_docs rescued_docs</p>

<p style="margin-top: 1em">Extract all the healthy Word
documents with magicrescue and build a database of their
checksums. It may seem a little redundant to send all the
documents through magicrescue <br>
first, but the reason is that this process may modify them
(e.g. stripping trailing garbage), and therefore their
checksum will not be the same as the original documents.
Also, <br>
it will find documents embedded inside other files, such as
uncompressed zip archives or files with the wrong
extension.</p>

<p style="margin-top: 1em">$ find /mnt/windows -type f
|magicrescue -I- -r msoffice -d healthy_docs <br>
$ dupemap -d healthy_docs.map scan healthy_docs <br>
$ rm -rf healthy_docs</p>

<p style="margin-top: 1em">Now rescue all
&quot;msoffice&quot; documents from the block device and get
rid of everything that&rsquo;s not a *.doc.</p>

<p style="margin-top: 1em">$ magicrescue -Mo -r msoffice -d
rescued_docs /dev/hda1 |grep -v &rsquo;.doc$&rsquo;|xargs rm
-f</p>

<p style="margin-top: 1em">Remove all the rescued documents
that also appear on the file system, and remove
duplicates.</p>

<p style="margin-top: 1em">$ dupemap -d healthy_docs.map
delete,report rescued_docs <br>
$ dupemap delete,report rescued_docs</p>

<p style="margin-top: 1em">The rescued_docs folder should
now contain only a few files. This will be the undeleted
files and some documents that were not stored in contiguous
blocks (use that defragger <br>
;-)).</p>

<p style="margin-top: 1em">Usage with fsck <br>
In this scenario (based on a true story), you have a hard
disk that&rsquo;s gone bad. You have managed to dd about 80%
of the contents into the file diskimage, and you have an old
<br>
backup from a few months ago. The disk is using reiserfs on
Linux.</p>

<p style="margin-top: 1em">First, use fsck to make the file
system usable again. It will find many nameless files and
put them in lost+found. You need to make sure there is some
free space on the disk <br>
image, so fsck has something to work with.</p>

<p style="margin-top: 1em">$ cp diskimage diskimage.bak
<br>
$ dd if=/dev/zero bs=1M count=2048 &gt;&gt; diskimage <br>
$ reiserfsck --rebuild-tree diskimage <br>
$ mount -o loop diskimage /mnt <br>
$ ls /mnt/lost+found <br>
(tons of files)</p>

<p style="margin-top: 1em">Our strategy will be to restore
the system with the old backup as a base and merge the two
other sets of files (/mnt/lost+found and /mnt) into the
backup after eliminating <br>
duplicates. Therefore we create a checksum database of the
directory we have unpacked the backup in.</p>

<p style="margin-top: 1em">$ dupemap -d backup.map scan
~/backup</p>

<p style="margin-top: 1em">Next, we eliminate all the files
from the rescued image that are also present in the
backup.</p>

<p style="margin-top: 1em">$ dupemap -d backup.map
delete,report /mnt</p>

<p style="margin-top: 1em">We also want to remove
duplicates from lost+found, and we want to get rid of any
files that are also present in the other directories in
/mnt.</p>

<p style="margin-top: 1em">$ dupemap delete,report
/mnt/lost+found <br>
$ ls /mnt|grep -v lost+found|xargs dupemap -d mnt.map scan
<br>
$ dupemap -d mnt.map delete,report /mnt/lost+found</p>

<p style="margin-top: 1em">This should leave only the files
in /mnt that have changed since the last backup or got
corrupted. Particularly, the contents of /mnt/lost+found
should now be reduced enough to <br>
manually sort through them (or perhaps use
magicsort(1)).</p>

<p style="margin-top: 1em">Primitive intrusion detection
<br>
You can use dupemap to see what files change on your system.
This is one of the more exotic uses, and it&rsquo;s only
included for inspiration.</p>

<p style="margin-top: 1em">First, you map the whole file
system.</p>

<p style="margin-top: 1em">$ dupemap -d old.map scan /</p>

<p style="margin-top: 1em">Then you come back a few
days/weeks later and run dupemap report. This will give you
a view of what has not changed. To see what has changed, you
need a list of the whole file <br>
system. You can get this list along with preparing a new map
easily. Both lists need to be sorted to be compared.</p>

<p style="margin-top: 1em">$ dupemap -d old.map report
/|sort &gt; unchanged_files <br>
$ dupemap -d current.map scan /|sort &gt; current_files</p>

<p style="margin-top: 1em">All that&rsquo;s left to do is
comparing these files and preparing for next week. This
assumes that the dbm appends the &quot;.db&quot; extension
to database files.</p>

<p style="margin-top: 1em">$ diff unchanged_files
current_files &gt; changed_files <br>
$ mv current.map.db old.map.db</p>

<p style="margin-top: 1em">DATABASE <br>
The actual database file(s) written by dupecheck will have
some relation to the database argument, but most
implementations append an extension. For example, Berkeley
DB names <br>
the files database.db, while Solaris and GDBM creates both a
database.dir and database.pag file.</p>

<p style="margin-top: 1em">dupecheck depends on a database
library for storing the checksums. It currently requires the
POSIX-standardized ndbm library, which must be present on
XSI-compliant UNIXes. <br>
Implementations are not required to handle hash key
collisions, and a failure to do that could make dupecheck
delete too many files. I haven&rsquo;t heard of such an
implementation, <br>
though.</p>

<p style="margin-top: 1em">The current checksum algorithm
is the file&rsquo;s CRC32 combined with its size. Both
values are stored in native byte order, and because of
varying type sizes the database is not <br>
portable across architectures, compilers and operating
systems.</p>

<p style="margin-top: 1em">SEE ALSO <br>
magicrescue(1), weeder(1)</p>

<p style="margin-top: 1em">This tool does the same thing
weeder does, except that weeder cannot seem to handle many
files without crashing, and it has no largefile support.</p>

<p style="margin-top: 1em">BUGS <br>
There is a tiny chance that two different files can have the
same checksum and size. The probability of this happening is
around 1 to 10^14, and since dupemap is part of the <br>
Magic Rescue package, which deals with disaster recovery,
that chance becomes an insignificant part of the game. You
should consider this if you apply dupemap to other <br>
applications, especially if they are security-related (see
next paragraph).</p>

<p style="margin-top: 1em">It is possible to craft a file
to have a known CRC32. You need to keep this in mind if you
use dupemap on untrusted data. A solution to this could be
to implement an option for <br>
using MD5 checksums instead.</p>

<p style="margin-top: 1em">AUTHOR <br>
Jonas Jensen &lt;jbj@knef.dk&gt;</p>

<p style="margin-top: 1em">LATEST VERSION <br>
This tool is part of Magic Rescue. You can find the latest
version at &lt;http://jbj.rapanden.dk/magicrescue/&gt;</p>

<p style="margin-top: 1em">1.1.9 2008-06-26 DUPEMAP(1)</p>
<hr>
</body>
</html>
