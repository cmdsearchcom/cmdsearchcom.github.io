<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8"/>
  <style>
    table.head, table.foot { width: 100%; }
    td.head-rtitle, td.foot-os { text-align: right; }
    td.head-vol { text-align: center; }
    div.Pp { margin: 1ex 0ex; }
  </style>
  <title>DUPEMAP(1)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">DUPEMAP(1)</td>
    <td class="head-vol">Magic Rescue</td>
    <td class="head-rtitle">DUPEMAP(1)</td>
  </tr>
</table>
<div class="manual-text">
<h1 class="Sh" title="Sh" id="NAME"><a class="selflink" href="#NAME">NAME</a></h1>
dupemap - Creates a database of file checksums and uses it to eliminate
  duplicates
<h1 class="Sh" title="Sh" id="SYNOPSIS"><a class="selflink" href="#SYNOPSIS">SYNOPSIS</a></h1>
<b>dupemap</b> [ <i>options</i> ] [ <b>-d</b> <i>database</i> ] <i>operation</i>
  <i>path...</i>
<h1 class="Sh" title="Sh" id="DESCRIPTION"><a class="selflink" href="#DESCRIPTION">DESCRIPTION</a></h1>
<b>dupemap</b> recursively scans each <i>path</i> to find checksums of file
  contents. Directories are searched through in no particular order. Its actions
  depend on whether the <b>-d</b> option is given, and on the <i>operation</i>
  parameter, which must be a comma-seperated list of <b>scan</b>, <b>report</b>,
  <b>delete</b>:
<h2 class="Ss" title="Ss" id="Without_-d"><a class="selflink" href="#Without_-d">Without
  <b>-d</b></a></h2>
<b>dupemap</b> will take action when it sees the same checksum repeated more
  than once, i.e. it simply finds duplicates recursively. The action depends on
  <i>operation</i>:
<dl class="Bl-tag">
  <dt class="It-tag"><b>report</b></dt>
  <dd class="It-tag">Report what files are encountered more than once, printing
      their names to standard output.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>delete</b>[<b>,report</b>]</dt>
  <dd class="It-tag">Delete files that are encountered more than once. Print
      their names if <b>report</b> is also given.
    <div style="height: 1.00em;">&#x00A0;</div>
     <i>WARNING:</i> use the <b>report</b> operation first to see what will be
      deleted.
    <div style="height: 1.00em;">&#x00A0;</div>
     <i>WARNING:</i> You are advised to make a backup of the target first, e.g.
      with &quot;cp -al&quot; (for GNU cp) to create hard links
    recursively.</dd>
</dl>
<h2 class="Ss" title="Ss" id="With_-d"><a class="selflink" href="#With_-d">With
  <b>-d</b></a></h2>
The <i>database</i> argument to <b>-d</b> will denote a database file (see the
  &quot;DATABASE&quot; section in this manual for details) to read from or write
  to. In this mode, the <b>scan</b> operation should be run on one <i>path</i>,
  followed by the <b>report</b> or <b>delete</b> operation on another (<i>not
  the same!</i>) <i>path</i>.
<dl class="Bl-tag">
  <dt class="It-tag"><b>scan</b></dt>
  <dd class="It-tag">Add the checksum of each file to <i>database</i>. This
      operation must be run initially to create the database. To start over, you
      must manually delete the database file(s) (see the &quot;DATABASE&quot;
      section).</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>report</b></dt>
  <dd class="It-tag">Print each file name if its checksum is found in
      <i>database</i>.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>delete</b>[<b>,report</b>]</dt>
  <dd class="It-tag">Delete each file if its checksum is found in
      <i>database</i>. If <b>report</b> is also present, print the name of each
      deleted file.
    <div style="height: 1.00em;">&#x00A0;</div>
     <i>WARNING:</i> if you run <b>dupemap delete</b> on the same <i>path</i>
      you just ran <b>dupemap scan</b> on, it will <i>delete every file!</i> The
      idea of these options is to scan one <i>path</i> and delete files in a
      second <i>path</i>.
    <div style="height: 1.00em;">&#x00A0;</div>
     <i>WARNING:</i> use the <b>report</b> operation first to see what will be
      deleted.
    <div style="height: 1.00em;">&#x00A0;</div>
     <i>WARNING:</i> You are advised to make a backup of the target first, e.g.
      with &quot;cp -al&quot; (for GNU cp) to create hard links
    recursively.</dd>
</dl>
<h1 class="Sh" title="Sh" id="OPTIONS"><a class="selflink" href="#OPTIONS">OPTIONS</a></h1>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-d</b> <i>database</i></dt>
  <dd class="It-tag">Use <i>database</i> as an on-disk database to read from or
      write to. See the &quot;DESCRIPTION&quot; section above about how this
      influences the operation of <b>dupemap</b>.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-I</b> <i>file</i></dt>
  <dd class="It-tag">Reads input files from <i>file</i> in addition to those
      listed on the command line. If <i>file</i> is &quot;-&quot;, read from
      standard input. Each line will be interpreted as a file name.
    <div style="height: 1.00em;">&#x00A0;</div>
    The paths given here will NOT be scanned recursively. Directories will be
      ignored and symlinks will be followed.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-m</b> <i>minsize</i></dt>
  <dd class="It-tag">Ignore files below this size.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-M</b> <i>maxsize</i></dt>
  <dd class="It-tag">Ignore files above this size.</dd>
</dl>
<h1 class="Sh" title="Sh" id="USAGE"><a class="selflink" href="#USAGE">USAGE</a></h1>
<h2 class="Ss" title="Ss" id="General_usage"><a class="selflink" href="#General_usage">General
  usage</a></h2>
The easiest operations to understand is when the <b>-d</b> option is not given.
  To delete all duplicate files in <i>/tmp/recovered-files</i>, do:
<div class="Pp"></div>
<pre>
    $ dupemap delete /tmp/recovered-files
</pre>
<div class="Pp"></div>
Often, <b>dupemap scan</b> is run to produce a checksum database of all files in
  a directory tree. Then <b>dupemap delete</b> is run on another directory,
  possibly following <b>dupemap report</b>. For example, to delete all files in
  <i>/tmp/recovered-files</i> that already exist in <i></i><i>$HOME</i><i></i>,
  do this:
<div class="Pp"></div>
<pre>
    $ dupemap -d homedir.map scan $HOME
    $ dupemap -d homedir.map delete,report /tmp/recovered-files
</pre>
<h2 class="Ss" title="Ss" id="Usage_with_magicrescue"><a class="selflink" href="#Usage_with_magicrescue">Usage
  with magicrescue</a></h2>
The main application for <b>dupemap</b> is to take some pain out of performing
  undelete operations with <b>magicrescue</b>(1). The reason is that
  <b>magicrescue</b> will extract every single file of the specified type on the
  block device, so undeleting files requires you to find a few files out of
  hundreds, which can take a long time if done manually. What we want to do is
  to only extract the documents that don't exist on the file system already.
<div class="Pp"></div>
In the following scenario, you have accidentally deleted some important Word
  documents in Windows. If this were a real-world scenario, then by all means
  use The Sleuth Kit. However, <b>magicrescue</b> will work even when the
  directory entries were overwritten, i.e. more files were stored in the same
  folder later.
<div class="Pp"></div>
You boot into Linux and change to a directory with lots of space. Mount the
  Windows partition, preferably read-only (especially with NTFS), and create the
  directories we will use.
<div class="Pp"></div>
<pre>
    $ mount -o ro /dev/hda1 /mnt/windows
    $ mkdir healthy_docs rescued_docs
</pre>
<div class="Pp"></div>
Extract all the healthy Word documents with <b>magicrescue</b> and build a
  database of their checksums. It may seem a little redundant to send all the
  documents through <b>magicrescue</b> first, but the reason is that this
  process may modify them (e.g. stripping trailing garbage), and therefore their
  checksum will not be the same as the original documents. Also, it will find
  documents embedded inside other files, such as uncompressed zip archives or
  files with the wrong extension.
<div class="Pp"></div>
<pre>
    $ find /mnt/windows -type f \
      |magicrescue -I- -r msoffice -d healthy_docs
    $ dupemap -d healthy_docs.map scan healthy_docs
    $ rm -rf healthy_docs
</pre>
<div class="Pp"></div>
Now rescue all &quot;msoffice&quot; documents from the block device and get rid
  of everything that's not a *.doc.
<div class="Pp"></div>
<pre>
    $ magicrescue -Mo -r msoffice -d rescued_docs /dev/hda1 \
      |grep -v '\.doc$'|xargs rm -f
</pre>
<div class="Pp"></div>
Remove all the rescued documents that also appear on the file system, and remove
  duplicates.
<div class="Pp"></div>
<pre>
    $ dupemap -d healthy_docs.map delete,report rescued_docs
    $ dupemap delete,report rescued_docs
</pre>
<div class="Pp"></div>
The <i>rescued_docs</i> folder should now contain only a few files. This will be
  the undeleted files and some documents that were not stored in contiguous
  blocks (use that defragger ;-)).
<h2 class="Ss" title="Ss" id="Usage_with_fsck"><a class="selflink" href="#Usage_with_fsck">Usage
  with fsck</a></h2>
In this scenario (based on a true story), you have a hard disk that's gone bad.
  You have managed to <i>dd</i> about 80% of the contents into the file
  <i>diskimage</i>, and you have an old backup from a few months ago. The disk
  is using reiserfs on Linux.
<div class="Pp"></div>
First, use fsck to make the file system usable again. It will find many nameless
  files and put them in <i>lost+found</i>. You need to make sure there is some
  free space on the disk image, so fsck has something to work with.
<div class="Pp"></div>
<pre>
    $ cp diskimage diskimage.bak
    $ dd if=/dev/zero bs=1M count=2048 &gt;&gt; diskimage
    $ reiserfsck --rebuild-tree diskimage
    $ mount -o loop diskimage /mnt
    $ ls /mnt/lost+found
    (tons of files)
</pre>
<div class="Pp"></div>
Our strategy will be to restore the system with the old backup as a base and
  merge the two other sets of files ( <i>/mnt/lost+found</i> and <i>/mnt</i>)
  into the backup after eliminating duplicates. Therefore we create a checksum
  database of the directory we have unpacked the backup in.
<div class="Pp"></div>
<pre>
    $ dupemap -d backup.map scan ~/backup
</pre>
<div class="Pp"></div>
Next, we eliminate all the files from the rescued image that are also present in
  the backup.
<div class="Pp"></div>
<pre>
    $ dupemap -d backup.map delete,report /mnt
</pre>
<div class="Pp"></div>
We also want to remove duplicates from <i>lost+found</i>, and we want to get rid
  of any files that are also present in the other directories in <i>/mnt</i>.
<div class="Pp"></div>
<pre>
    $ dupemap delete,report /mnt/lost+found
    $ ls /mnt|grep -v lost+found|xargs dupemap -d mnt.map scan
    $ dupemap -d mnt.map delete,report /mnt/lost+found
</pre>
<div class="Pp"></div>
This should leave only the files in <i>/mnt</i> that have changed since the last
  backup or got corrupted. Particularly, the contents of <i>/mnt/lost+found</i>
  should now be reduced enough to manually sort through them (or perhaps use
  <b>magicsort</b>(1)).
<h2 class="Ss" title="Ss" id="Primitive_intrusion_detection"><a class="selflink" href="#Primitive_intrusion_detection">Primitive
  intrusion detection</a></h2>
You can use <b>dupemap</b> to see what files change on your system. This is one
  of the more exotic uses, and it's only included for inspiration.
<div class="Pp"></div>
First, you map the whole file system.
<div class="Pp"></div>
<pre>
    $ dupemap -d old.map scan /
</pre>
<div class="Pp"></div>
Then you come back a few days/weeks later and run <b>dupemap report</b>. This
  will give you a view of what <i>has not</i> changed. To see what <i>has</i>
  changed, you need a list of the whole file system. You can get this list along
  with preparing a new map easily. Both lists need to be sorted to be compared.
<div class="Pp"></div>
<pre>
    $ dupemap -d old.map report /|sort &gt; unchanged_files
    $ dupemap -d current.map scan /|sort &gt; current_files
</pre>
<div class="Pp"></div>
All that's left to do is comparing these files and preparing for next week. This
  assumes that the dbm appends the &quot;.db&quot; extension to database files.
<div class="Pp"></div>
<pre>
    $ diff unchanged_files current_files &gt; changed_files
    $ mv current.map.db old.map.db
</pre>
<h1 class="Sh" title="Sh" id="DATABASE"><a class="selflink" href="#DATABASE">DATABASE</a></h1>
The actual database file(s) written by <b>dupecheck</b> will have some relation
  to the <i>database</i> argument, but most implementations append an extension.
  For example, Berkeley DB names the files <i>database</i><b>.db</b>, while
  Solaris and GDBM creates both a <i>database</i><b>.dir</b> and
  <i>database</i><b>.pag</b> file.
<div class="Pp"></div>
<b>dupecheck</b> depends on a database library for storing the checksums. It
  currently requires the POSIX-standardized <b>ndbm</b> library, which must be
  present on XSI-compliant UNIXes. Implementations are not required to handle
  hash key collisions, and a failure to do that could make <b>dupecheck</b>
  delete too many files. I haven't heard of such an implementation, though.
<div class="Pp"></div>
The current checksum algorithm is the file's CRC32 combined with its size. Both
  values are stored in native byte order, and because of varying type sizes the
  database is <i>not</i> portable across architectures, compilers and operating
  systems.
<h1 class="Sh" title="Sh" id="SEE_ALSO"><a class="selflink" href="#SEE_ALSO">SEE
  ALSO</a></h1>
<b>magicrescue</b>(1), <b>weeder</b>(1)
<div class="Pp"></div>
This tool does the same thing <b>weeder</b> does, except that <b>weeder</b>
  cannot seem to handle many files without crashing, and it has no largefile
  support.
<h1 class="Sh" title="Sh" id="BUGS"><a class="selflink" href="#BUGS">BUGS</a></h1>
There is a tiny chance that two different files can have the same checksum and
  size. The probability of this happening is around 1 to 10^14, and since
  <b>dupemap</b> is part of the Magic Rescue package, which deals with disaster
  recovery, that chance becomes an insignificant part of the game. You should
  consider this if you apply <b>dupemap</b> to other applications, especially if
  they are security-related (see next paragraph).
<div class="Pp"></div>
It is possible to craft a file to have a known CRC32. You need to keep this in
  mind if you use <b>dupemap</b> on untrusted data. A solution to this could be
  to implement an option for using MD5 checksums instead.
<h1 class="Sh" title="Sh" id="AUTHOR"><a class="selflink" href="#AUTHOR">AUTHOR</a></h1>
Jonas Jensen &lt;jbj@knef.dk&gt;
<h1 class="Sh" title="Sh" id="LATEST_VERSION"><a class="selflink" href="#LATEST_VERSION">LATEST
  VERSION</a></h1>
This tool is part of Magic Rescue. You can find the latest version at
  &lt;http://jbj.rapanden.dk/magicrescue/&gt;</div>
<table class="foot">
  <tr>
    <td class="foot-date">2008-06-26</td>
    <td class="foot-os">1.1.9</td>
  </tr>
</table>
</body>
</html>
