<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 15:56:53 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>bup-split(1) bup-split(1)</p>

<p style="margin-top: 1em">NAME <br>
bup-split - save individual files to bup backup sets</p>

<p style="margin-top: 1em">SYNOPSIS <br>
bup split [-t] [-c] [-n name] COMMON_OPTIONS</p>

<p style="margin-top: 1em">bup split -b COMMON_OPTIONS</p>

<p style="margin-top: 1em">bup split &lt;--noop
[--copy]|--copy&gt; COMMON_OPTIONS</p>

<p style="margin-top: 1em">COMMON_OPTIONS <br>
[-r host:path] [-v] [-q] [-d seconds-since-epoch] [--bench]
[--max-pack-size=bytes] [-#] [--bwlimit=bytes]
[--max-pack-objects=n] [--fanout=count] [--keep-boundaries]
<br>
[--git-ids | filenames...]</p>

<p style="margin-top: 1em">DESCRIPTION <br>
bup split concatenates the contents of the given files (or
if no filenames are given, reads from stdin), splits the
content into chunks of around 8k using a rolling checksum
al&acirc; <br>
gorithm, and saves the chunks into a bup repository. Chunks
which have previously been stored are not stored again (ie.
they are &rsquo;deduplicated&rsquo;).</p>

<p style="margin-top: 1em">Because of the way the rolling
checksum works, chunks tend to be very stable across changes
to a given file, including adding, deleting, and changing
bytes.</p>

<p style="margin-top: 1em">For example, if you use bup
split to back up an XML dump of a database, and the XML file
changes slightly from one run to the next, nearly all the
data will still be deduplicated <br>
and the size of each backup after the first will typically
be quite small.</p>

<p style="margin-top: 1em">Another technique is to pipe the
output of the tar(1) or cpio(1) programs to bup split. When
individual files in the tarball change slightly or are added
or removed, bup still <br>
processes the remainder of the tarball efficiently. (Note
that bup save is usually a more efficient way to accomplish
this, however.)</p>

<p style="margin-top: 1em">To get the data back, use
bup-join(1).</p>

<p style="margin-top: 1em">MODES <br>
These options select the primary behavior of the command,
with -n being the most likely choice.</p>

<p style="margin-top: 1em">-n, --name=name <br>
after creating the dataset, create a git branch named name
so that it can be accessed using that name. If name already
exists, the new dataset will be considered a de&acirc; <br>
scendant of the old name. (Thus, you can continually create
new datasets with the same name, and later view the history
of that dataset to see how it has changed over <br>
time.) The original data will also be available as a
top-level file named &quot;data&quot; in the VFS, accessible
via bup fuse, bup ftp, etc.</p>

<p style="margin-top: 1em">-t, --tree <br>
output the git tree id of the resulting dataset.</p>

<p style="margin-top: 1em">-c, --commit <br>
output the git commit id of the resulting dataset.</p>

<p style="margin-top: 1em">-b, --blobs <br>
output a series of git blob ids that correspond to the
chunks in the dataset. Incompatible with -n, -t, and -c.</p>

<p style="margin-top: 1em">--noop read the data and split
it into blocks based on the &quot;bupsplit&quot; rolling
checksum algorithm, but don&rsquo;t do anything with the
blocks. This is mostly useful for benchmarking. <br>
Incompatible with -n, -t, -c, and -b.</p>

<p style="margin-top: 1em">--copy like --noop, but also
write the data to stdout. This can be useful for
benchmarking the speed of read+bupsplit+write for large
amounts of data. Incompatible with -n, -t, <br>
-c, and -b.</p>

<p style="margin-top: 1em">OPTIONS <br>
-r, --remote=host:path <br>
save the backup set to the given remote server. If path is
omitted, uses the default path on the remote server (you
still need to include the &rsquo;:&rsquo;). The connection
to the <br>
remote server is made with SSH. If you&rsquo;d like to
specify which port, user or private key to use for the SSH
connection, we recommend you use the ~/.ssh/config file.
Even <br>
though the destination is remote, a local bup repository is
still required.</p>

<p style="margin-top: 1em">-d, --date=seconds-since-epoch
<br>
specify the date inscribed in the commit (seconds since
1970-01-01).</p>

<p style="margin-top: 1em">-q, --quiet <br>
disable progress messages.</p>

<p style="margin-top: 1em">-v, --verbose <br>
increase verbosity (can be used more than once).</p>

<p style="margin-top: 1em">--git-ids <br>
stdin is a list of git object ids instead of raw data. bup
split will read the contents of each named git object (if it
exists in the bup repository) and split it. This <br>
might be useful for converting a git repository with large
binary files to use bup-style hashsplitting instead. This
option is probably most useful when combined with <br>
--keep-boundaries.</p>

<p style="margin-top: 1em">--keep-boundaries <br>
if multiple filenames are given on the command line, they
are normally concatenated together as if the content all
came from a single file. That is, the set of <br>
blobs/trees produced is identical to what it would have been
if there had been a single input file. However, if you use
--keep-boundaries, each file is split separately. <br>
You still only get a single tree or commit or series of
blobs, but each blob comes from only one of the files; the
end of one of the input files always ends a blob.</p>

<p style="margin-top: 1em">--bench <br>
print benchmark timings to stderr.</p>

<p style="margin-top: 1em">--max-pack-size=bytes <br>
never create git packfiles larger than the given number of
bytes. Default is 1 billion bytes. Usually there is no
reason to change this.</p>

<p style="margin-top: 1em">--max-pack-objects=numobjs <br>
never create git packfiles with more than the given number
of objects. Default is 200 thousand objects. Usually there
is no reason to change this.</p>

<p style="margin-top: 1em">--fanout=numobjs <br>
when splitting very large files, try and keep the number of
elements in trees to an average of numobjs.</p>

<p style="margin-top: 1em">--bwlimit=bytes/sec <br>
don&rsquo;t transmit more than bytes/sec bytes per second to
the server. This is good for making your backups not suck up
all your network bandwidth. Use a suffix like k, M, <br>
or G to specify multiples of 1024, 10241024, 10241024*1024
respectively.</p>

<p style="margin-top: 1em">-#, --compress=# <br>
set the compression level to # (a value from 0-9, where 9 is
the highest and 0 is no compression). The default is 1
(fast, loose compression)</p>

<p style="margin-top: 1em">EXAMPLES <br>
$ tar -cf - /etc | bup split -r myserver: -n mybackup-tar
<br>
tar: Removing leading /&rsquo; from member names <br>
Indexing objects: 100% (196/196), done.</p>

<p style="margin-top: 1em">$ bup join -r myserver:
mybackup-tar | tar -tf - | wc -l <br>
1961</p>

<p style="margin-top: 1em">SEE ALSO <br>
bup-join(1), bup-index(1), bup-save(1), bup-on(1),
ssh_config(5)</p>

<p style="margin-top: 1em">BUP <br>
Part of the bup(1) suite.</p>

<p style="margin-top: 1em">AUTHORS <br>
Avery Pennarun &lt;apenwarr@gmail.com&gt;.</p>

<p style="margin-top: 1em">Bup debian/0.29-2 2017-01-01
bup-split(1)</p>
<hr>
</body>
</html>
