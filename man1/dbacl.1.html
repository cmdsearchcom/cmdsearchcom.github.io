<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8"/>
  <style>
    table.head, table.foot { width: 100%; }
    td.head-rtitle, td.foot-os { text-align: right; }
    td.head-vol { text-align: center; }
    div.Pp { margin: 1ex 0ex; }
  </style>
  <title>DBACL(1)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">DBACL(1)</td>
    <td class="head-vol"></td>
    <td class="head-rtitle">DBACL(1)</td>
  </tr>
</table>
<div class="manual-text">
<h1 class="Sh" title="Sh" id="NAME"><a class="selflink" href="#NAME">NAME</a></h1>
dbacl - a digramic Bayesian classifier for text recognition.
<h1 class="Sh" title="Sh" id="SYNOPSIS"><a class="selflink" href="#SYNOPSIS">SYNOPSIS</a></h1>
<div class="Pp" style="margin-left: 5.00ex; text-indent: -5.00ex;"><b>dbacl</b>
  [-01dvnirmwMNDXW] [-T <i>type</i> ] -l <i>category</i> [-h <i>size</i>] [-H
  <i>gsize</i>] [-x <i>decim</i>] [-q <i>quality</i>] [-w <i>max_order</i>] [-e
  <i>deftok</i>] [-o <i>online</i>] [-L <i>measure</i>] [-g <i>regex</i>]...
  [FILE]...</div>
<div class="Pp"></div>
<div class="Pp" style="margin-left: 5.00ex; text-indent: -5.00ex;"><b>dbacl</b>
  [-vnimNRX] [-h <i>size</i>] [-T <i>type]</i> -c <i>category</i> [-c
  <i>category</i>]... [-f <i>keep</i>]... [FILE]...</div>
<div class="Pp"></div>
<div class="Pp" style="margin-left: 5.00ex; text-indent: -5.00ex;"><b>dbacl</b>
  -V</div>
<h1 class="Sh" title="Sh" id="OVERVIEW"><a class="selflink" href="#OVERVIEW">OVERVIEW</a></h1>
<b>dbacl</b> is a Bayesian text and email classifier. When using the <b>-l</b>
  switch, it learns a body of text and produce a file named <i>category</i>
  which summarizes the text. When using the <b>-c</b> switch, it compares an
  input text stream with any number of <i>category</i> files, and outputs the
  name of the closest match, or optionally various numerical scores explained
  below.
<div class="Pp"></div>
Whereas this manual page is intended as a reference, there are several tutorials
  and documents you can read to get specialized information. Specific
  documentation about the design of <b>dbacl</b> and the statistical models that
  it uses can be found in dbacl.ps. For a basic overview of text classification
  using <b>dbacl</b>, see tutorial.html. A companion tutorial geared towards
  email filtering is email.html. If you have trouble getting dbacl to classify
  reliably, read is_it_working.html. The USAGE section of this manual page also
  has some examples.
<dl class="Bl-tag">
  <dt class="It-tag">/usr/share/doc/dbacl/dbacl.ps</dt>
  <dd class="It-tag"></dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">/usr/share/doc/dbacl/tutorial.html</dt>
  <dd class="It-tag"></dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">/usr/share/doc/dbacl/email.html</dt>
  <dd class="It-tag"></dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">/usr/share/doc/dbacl/is_it_working.html</dt>
  <dd class="It-tag"></dd>
</dl>
<div class="Pp"></div>
<b>dbacl</b> uses a maximum entropy (minimum divergence) language model
  constructed with respect to a digramic reference measure (unknown tokens are
  predicted from digrams, i.e. pairs of letters). Practically, this means that a
  <i>category</i> is constructed from tokens in the training set, while
  previously unseen tokens can be predicted automatically from their letters. A
  token here is either a word (fragment) or a combination of words (fragments),
  selected according to various switches. Learning roughly works by tweaking
  token probabilities until the training data is least surprising.
<h1 class="Sh" title="Sh" id="EXIT_STATUS"><a class="selflink" href="#EXIT_STATUS">EXIT
  STATUS</a></h1>
The normal shell exit conventions aren't followed (sorry!). When using the
  <b>-l</b> command form, <b>dbacl</b> returns zero on success, nonzero if an
  error occurs. When using the <b>-c</b> form, <b>dbacl</b> returns a positive
  integer corresponding to the <i>category</i> with the highest posterior
  probability. In case of a tie, the first most probable category is chosen. If
  an error occurs, <b>dbacl</b> returns zero.
<h1 class="Sh" title="Sh" id="DESCRIPTION"><a class="selflink" href="#DESCRIPTION">DESCRIPTION</a></h1>
When using the <b>-l</b> command form, <b>dbacl</b> learns a category when given
  one or more FILE names, which should contain readable <small>ASCII</small>
  text. If no FILE is given, <b>dbacl</b> learns from STDIN. If FILE is a
  directory, it is opened and all its files are read, but not its
  subdirectories. The result is saved in the binary file named <i>category</i>,
  and completely replaces any previous contents. As a convenience, if the
  environment variable DBACL_PATH contains a directory, then that is prepended
  to the file path, unless <i>category</i> starts with a '/' or a '.'.
<div class="Pp"></div>
The input text for learning is assumed to be unstructured plain text by default.
  This is not suitable for learning email, because email contains various
  transport encodings and formatting instructions which can reduce
  classification effectiveness. You must use the <b>-T</b> switch in that case
  so that <b>dbacl</b> knows it should perform decoding and filtering of MIME
  and HTML as appropriate. Apropriate switch values are &quot;-T email&quot; for
  RFC2822 email input, &quot;-T html&quot; for HTML input, &quot;-T xml&quot;
  for generic XML style input and &quot;-T text&quot; is the default plain text
  format. There are other values of the <b>-T</b> switch that also allow fine
  tuning of the decoding capabilities.
<div class="Pp"></div>
When using the <b>-c</b> command form, <b>dbacl</b> attempts to classify the
  text found in FILE, or STDIN if no FILE is given. Each possible
  <i>category</i> must be given separately, and should be the file name of a
  previously learned text corpus. As a convenience, if the variable DBACL_PATH
  contains a directory, it is prepended to each file path which doesn't start
  with a '/' or a '.'. The visible output of the classification depends on the
  combination of extra switches used. If no switch is used, then no output is
  shown on STDOUT. However, <b>dbacl</b> always produces an exit code which can
  be tested.
<div class="Pp"></div>
To see an output for a classification, you must use at least one of the
  <b>-v</b>,<b>-U</b>,<b>-n</b>,<b>-N</b>,<b>-D</b>,<b>-d</b> switches.
  Sometimes, they can be used in combination to produce a natural variation of
  their individual outputs. Sometimes, <b>dbacl</b> also produces warnings on
  STDERR if applicable.
<div class="Pp"></div>
The <b>-v</b> switch outputs the name of the best category among all the choices
  given.
<div class="Pp"></div>
The <b>-U</b> switch outputs the name of the best category followed by a
  confidence percentage. Normally, this is the switch that you want to use. A
  percentage of 100% means that <b>dbacl</b> is sure of its choice, while a
  percentage of 0% means that some other category is equally likely. This is not
  the model probability, but measures how unambiguous the classification is, and
  can be used to tag unsure classifications (e.g. if the confidence is 25% or
  less).
<div class="Pp"></div>
The <b>-N</b> switch prints each category name followed by its (posterior)
  probability, expressed as a percentage. The percentages always sum to 100%.
  This is intuitive, but only valuable if the document being classified contains
  a handful of tokens (ten or less). In the common case with many more tokens,
  the probabilities are always extremely close to 100% and 0%.
<div class="Pp"></div>
The <b>-n</b> switch prints each category name followed by the negative
  logarithm of its probability. This is equivalent to using the <b>-N</b>
  switch, but much more useful. The smallest number gives the best category. A
  more convenient form is to use both <b>-n</b> and <b>-v</b> which prints each
  category name followed by the cross entropy and the number of tokens analyzed.
  The cross entropy measures (in bits) the average compression rate which is
  achievable, under the given category model, per token of input text. If you
  use all three of <b>-n</b>,<b>-v</b>,<b>-X</b> then an extra value is output
  for each category, representing a kind of p-value for each category score.
  This indicates how typical the score is compared to the training documents,
  but only works if the <b>-X</b> switch was used during learning, and only for
  some types of models (e.g. email). These p-values are uniformly distributed
  and independent (if the categories are independent), so can be combined using
  Fisher's chi squared test to obtain composite p-values for groupings of
  categories.
<div class="Pp"></div>
The <b>-v</b> and <b>-X</b> switches together print each category name followed
  by a detailed decomposition of the category score, factored into ( divergence
  rate + shannon entropy rate )* token count @ p-value. Again, this only works
  in some types of models.
<div class="Pp"></div>
The <b>-v</b> and <b>-U</b> switches print each category name followed by a
  decomposition of the category score into ( divergence rate + shannon entropy
  rate # score variance )* token count.
<div class="Pp"></div>
The <b>-D</b> switch prints out the input text as modified internally by
  <b>dbacl</b> prior to tokenization. For example, if a MIME encoded email
  document is classified, then this prints the decoded text that will be
  actually tokenized and classified. This switch is mainly useful for debugging.
<div class="Pp"></div>
The <b>-d</b> switch dumps tokens and scores while they are being read. It is
  useful for debugging, or if you want to create graphical representations of
  the classification. A detailed explanation of the output is beyond the scope
  of this manual page, but is straightforward if you've read dbacl.ps. Possible
  variations include <b>-d</b> together with <b>-n</b> or <b>-N</b>.
<div class="Pp"></div>
Classification can be done with one or several categories in principle. When two
  or more categories are used, the Bayesian posterior probability is used, given
  the input text, with a uniform prior distribution on categories. For other
  choices of prior, see the companion utility <b>bayesol</b>(1). When a single
  category is used, classification can be done by comparing the score with a
  treshold. In practice however, much better results are obtained with several
  categories.
<div class="Pp"></div>
Learning and classifying cannot be mixed on the same command invocation, however
  there are no locking issues and separate <b>dbacl</b> processes can operate
  simultaneously with obvious results, because file operations are designed to
  be atomic.
<div class="Pp"></div>
Finally, note that <b>dbacl</b> does not manage your document corpora or your
  computed categories, and in particular it does not allow you to extend an
  existing category file with new documents. This is unlike various current spam
  filters, which can learn new emails incrementally. This limitation of
  <b>dbacl</b> is partially due to the nonlinear procedure used in the learning
  algorithm, and partially a desire for increased flexibility.
<div class="Pp"></div>
You can simulate the effect of incremental learning by saving your training
  documents into archives and adding to these archives over time, relearning
  from scratch periodically. Learning is actually faster if these archives are
  compressed and decompressed on the fly when needed. By keeping control of your
  archives, you can never lose the information in your categories, and you can
  easily experiment with different switches or tokenizations or sets of training
  documents if you like.
<h1 class="Sh" title="Sh" id="SECONDARY_SWITCHES"><a class="selflink" href="#SECONDARY_SWITCHES">SECONDARY
  SWITCHES</a></h1>
By default, <b>dbacl</b> classifies the input text as a whole. However, when
  using the <b>-f</b> option, <b>dbacl</b> can be used to filter each input line
  separately, printing only those lines which match one or more models
  identified by <i>keep</i> (use the category name or number to refer to a
  category). This is useful if you want to filter out some lines, but note that
  if the lines are short, then the error rate can be high.
<div class="Pp"></div>
The <b>-e</b>,<b>-w</b>,<b>-g</b>,<b>-j</b> switches are used for selecting an
  appropriate tokenization scheme. A token is a word or word fragment or
  combination of words or fragments. The shape of tokens is important because it
  forms the basis of the language models used by <b>dbacl</b>. The <b>-e</b>
  switch selects a predefined tokenization scheme, which is speedy but limited.
  The <b>-w</b> switch specifies composite tokens derived from the <b>-e</b>
  switch. For example, &quot;-e alnum -w 2&quot; means that tokens should be
  alphanumeric word fragments combined into overlapping pairs (bigrams). When
  the <b>-j</b> switch is used, all tokens are converted to lowercase, which
  reduces the number of possible tokens and therefore memory consumption.
<div class="Pp"></div>
If the <b>-g</b> switch is used, you can completely specify what the tokens
  should look like using a regular expression. Several <b>-g</b> switches can be
  used to construct complex tokenization schemes, and parentheses within each
  expression can be used to select fragments and combine them into n-grams. The
  cost of such flexibility is reduced classification and learning speed. When
  experimenting with tokenization schemes, try using the <b>-d</b> or <b>-D</b>
  switches while learning or classifying, as they will print the tokens
  explicitly so you can see what text fragments are picked up or missed out. For
  regular exression syntax, see <b>regex</b>(7).
<div class="Pp"></div>
The <b>-h</b> and <b>-H</b> switches regulate how much memory <b>dbacl</b> may
  use for learning. Text classification can use a lot of memory, and by default
  <b>dbacl</b> limits itself even at the expense of learning accuracy. In many
  cases if a limit is reached, a warning message will be printed on STDERR with
  some advice.
<div class="Pp"></div>
When relearning the same category several times, a significant speedup can be
  obtained by using the <b>-1</b> switch, as this allows the previously learned
  probabilities to be read from the category and reused.
<div class="Pp"></div>
Note that classification accuracy depends foremost on the amount and quality of
  the training samples, and then only on amount of tweaking.
<h1 class="Sh" title="Sh" id="EXIT_STATUS"><a class="selflink" href="#EXIT_STATUS">EXIT
  STATUS</a></h1>
When using the <b>-l</b> command form, <b>dbacl</b> returns zero on success.
  When using the <b>-c</b> form, <b>dbacl</b> returns a positive integer
  (1,2,3...) corresponding to the <i>category</i> with the highest posterior
  probability. In case of a tie, the first most probable category is chosen. If
  an error occurs, <b>dbacl</b> returns zero.
<h1 class="Sh" title="Sh" id="OPTIONS"><a class="selflink" href="#OPTIONS">OPTIONS</a></h1>
<dl class="Bl-tag">
  <dt class="It-tag">-0</dt>
  <dd class="It-tag">When learning, prevents weight preloading. Normally,
      <b>dbacl</b> checks if the category file already exists, and if so, tries
      to use the existing weights as a starting point. This can dramatically
      speed up learning. If the <b>-0</b> (zero) switch is set, then
      <b>dbacl</b> behaves as if no category file already exists. This is mainly
      useful for testing. This switch is now enabled by default, to protect
      against weight drift which can reduce accuracy over many learning
      iterations. Use <b>-1</b> to force preloading.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-1</dt>
  <dd class="It-tag">Force weight preloading if the category file already
      exists. See discussion of the <b>-0</b> switch.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-a</dt>
  <dd class="It-tag">Append scores. Every input line is written to STDOUT and
      the dbacl scores are appended. This is useful for postprocessing with
      <b>bayesol</b>(1). For ease of processing, every original input line is
      indented by a single space (to distinguish them from the appended scores),
      and the line with the scores (if <b>-n</b> is used) is prefixed with the
      string &quot;scores &quot;. If a second copy of <b>dbacl</b> needs to read
      this output later, it should be invoked with the <b>-A</b> switch.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-d</dt>
  <dd class="It-tag">Dump the model parameters to STDOUT. In conjunction with
      the <b>-l</b> option, this produces a human-readable summary of the
      maximum entropy model. In conjunction with the <b>-c</b> option, displays
      the contribution of each token to the final score. Suppresses all other
      normal output.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-e</dt>
  <dd class="It-tag">Select character class for default (not regex-based)
      tokenization. By default, tokens are alphabetic strings only. This
      corresponds to the case when <i>deftok</i> is &quot;alpha&quot;. Possible
      values for <i>deftok</i> are &quot;alpha&quot;, &quot;alnum&quot;,
      &quot;graph&quot;, &quot;char&quot;, &quot;cef&quot; and &quot;adp&quot;.
      The last two are custom tokenizers intended for email messages. See also
      <b>isalpha</b>(3). The &quot;char&quot; tokenizer picks up single
      printable characters rather than bigger tokens, and is intended for
      testing only.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-f</dt>
  <dd class="It-tag">Filter each line of input separately, passing to STDOUT
      only lines which match the <i>category</i> identified as <i>keep</i>. This
      option should be used repeatedly for each <i>category</i> which must be
      kept. <i>keep</i> can be either the <i>category</i> file name, or a
      positive integer representing the required <i>category</i> in the same
      order it appears on the command line.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">Output lines are flushed as soon as they are written. If
      the input file is a pipe or character device, then an attempt is made to
      use line buffering mode, otherwise the more efficient block buffering is
      used.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-g</dt>
  <dd class="It-tag">Learn only features described by the extended regular
      expression <i>regex</i>. This overrides the default feature selection
      method (see <b>-w</b> option) and learns, for each line of input, only
      tokens constructed from the concatenation of strings which match the
      tagged subexpressions within the supplied <i>regex</i>. All substrings
      which match <i>regex</i> within a suffix of each input line are treated as
      features, even if they overlap on the input line.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">As an optional convenience, <i>regex</i> can include the
      suffix <i>||xyz</i> which indicates which parenthesized subexpressions
      should be tagged. In this case, <i>xyz</i> should consist exclusively of
      digits 1 to 9, numbering exactly those subexpressions which should be
      tagged. Alternatively, if no parentheses exist within <i>regex</i>, then
      it is assumed that the whole expression must be captured.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-h</dt>
  <dd class="It-tag">Set the size of the hash table to 2^<i>size</i> elements.
      When using the <b>-l</b> option, this refers to the total number of
      features allowed in the maximum entropy model being learned. When using
      the <b>-c</b> option toghether with the <b>-M</b> switch and multinomial
      type categories, this refers to the maximum number of features taken into
      account during classification. Without the <b>-M</b> switch, this option
      has no effect.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-i</dt>
  <dd class="It-tag">Fully internationalized mode. Forces the use of wide
      characters internally, which is necessary in some locales. This incurs a
      noticeable performance penalty.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-j</dt>
  <dd class="It-tag">Make features case sensitive. Normally, all features are
      converted to lower case during processing, which reduces storage
      requirements and improves statistical estimates for small datasets. With
      this option, the original capitalization is used for each feature. This
      can improve classification accuracy.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-m</dt>
  <dd class="It-tag">Aggressively maps categories into memory and locks them
      into RAM to prevent swapping, if possible. This is useful when speed is
      paramount and memory is plentiful, for example when testing the classifier
      on large datasets.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">Locking may require relaxing user limits with
      <b>ulimit</b>(1). Ask your system administrator. Beware when using the
      <b>-m</b> switch together with the <b>-o</b> switch, as only one dbacl
      process must learn or classify at a time to prevent file corruption. If no
      learning takes place, then the <b>-m</b> switch for classifying is always
      safe to use. See also the discussion for the <b>-o</b> switch.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-n</dt>
  <dd class="It-tag">Print scores for each <i>category</i>. Each score is the
      product of two numbers, the cross entropy and the complexity of the input
      text under each model. Multiplied together, they represent the log
      probability that the input resembles the model. To see these numbers
      separately, use also the <b>-v</b> option. In conjunction with the
      <b>-f</b> option, stops filtering but prints each input line prepended
      with a list of scores for that line.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-q</dt>
  <dd class="It-tag">Select <i>quality</i> of learning, where <i>quality</i> can
      be 1,2,3,4. Higher values take longer to learn, and should be slightly
      more accurate. The default <i>quality</i> is 1 if the category file
      doesn't exist or weights cannot be preloaded, and 2 otherwise.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-o</dt>
  <dd class="It-tag">When learning, reads/writes partial token counts so they
      can be reused. Normally, category files are learned from exactly the input
      data given, and don't contain extraneous information. When this option is
      in effect, some extra information is saved in the file <i>online</i>,
      after all input was read. This information can be reread the next time
      that learning occurs, to continue where the previous dataset left off. If
      <i>online</i> doesn't exist, it is created. If <i>online</i> exists, it is
      read before learning, and updated afterwards. The file is approximately 3
      times bigger (at least) than the learned <i>category</i>.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">In <b>dbacl</b>, file updates are atomic, but if using the
      <b>-o</b> switch, two or more processes should not learn simultaneously,
      as only one process will write a lasting category and memory dump. The
      <b>-m</b> switch can also speed up online learning, but beware of possible
      corruption. Only one process should read or write a file. This option is
      intended primarily for controlled test runs.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-r</dt>
  <dd class="It-tag">Learn the digramic reference model only. Skips the learning
      of extra features in the text corpus.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-v</dt>
  <dd class="It-tag">Verbose mode. When learning, print out details of the
      computation, when classifying, print out the name of the most probable
      <i>category</i>. In conjunction with the <b>-n</b> option, prints the
      scores as an explicit product of the cross entropy and the
    complexity.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-w</dt>
  <dd class="It-tag">Select default features to be n-grams up to
      <i>max_order</i>. This is incompatible with the <b>-g</b> option, which
      always takes precedence. If no <b>-w</b> or <b>-g</b> options are given,
      <b>dbacl</b> assumes <b>-w</b> 1. Note that n-grams for n greater than 1
      do not straddle line breaks by default. The <b>-S</b> switch enables line
      straddling.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-x</dt>
  <dd class="It-tag">Set decimation probability to 1 - 2^(<i>-decim</i>). To
      reduce memory requirements when learning, some inputs are randomly
      skipped, and only a few are added to the model. Exact behaviour depends on
      the applicable <b>-T</b> option (default is <b>-T</b> &quot;text&quot;).
      When the type is not &quot;email&quot; (eg &quot;text&quot;), then
      individual input features are added with probability 2^( <i>-decim</i>).
      When the type is &quot;email&quot;, then full input messages are added
      with probability 2^( <i>-decim</i>). Within each such message, all
      features are used.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-A</dt>
  <dd class="It-tag">Expect indented input and scores. With this switch,
      <b>dbacl</b> expects input lines to be indented by a single space
      character (which is then skipped). Lines starting with any other character
      are ignored. This is the counterpart to the <b>-a</b> switch above. When
      used together with the <b>-a</b> switch, <b>dbacl</b> outputs the skipped
      lines as they are, and reinserts the space at the front of each processed
      input line.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-D</dt>
  <dd class="It-tag">Print debug output. Do not use normally, but can be very
      useful for displaying the list features picked up while learning.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-H</dt>
  <dd class="It-tag">Allow hash table to grow up to a maximum of 2^<i>gsize</i>
      elements during learning. Initial size is given by <b>-h</b> option.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-L</dt>
  <dd class="It-tag">Select the digramic reference measure for character
      transitions. The <i>measure</i> can be one of &quot;uniform&quot;,
      &quot;dirichlet&quot; or &quot;maxent&quot;. Default is
      &quot;uniform&quot;.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-M</dt>
  <dd class="It-tag">Force multinomial calculations. When learning, forces the
      model features to be treated multinomially. When classifying, corrects
      entropy scores to reflect multinomial probabilities (only applicable to
      multinomial type models, if present). Scores will always be lower, because
      the ordering of features is lost.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-N</dt>
  <dd class="It-tag">Print posterior probabilities for each <i>category</i>.
      This assumes the supplied categories form an exhaustive list of
      possibilities. In conjunction with the <b>-f</b> option, stops filtering
      but prints each input line prepended with a summary of the posterior
      distribution for that line.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-R</dt>
  <dd class="It-tag">Include an extra category for purely random text. The
      category is called &quot;random&quot;. Only makes sense when using the
      <b>-c</b> option.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-S</dt>
  <dd class="It-tag">Enable line straddling. This is useful together with the
      <b>-w</b> option to allow n-grams for n &gt; 1 to ignore line breaks, so a
      complex token can continue past the end of the line. This is not
      recommended for email.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-T</dt>
  <dd class="It-tag">Specify nonstandard text format. By default, <b>dbacl</b>
      assumes that the input text is a purely <small>ASCII</small> text file.
      This corresponds to the case when <i>type</i> is &quot;text&quot;.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">There are several types and subtypes which can be used to
      clean the input text of extraneous tokens before actual learning or
      classifying takes place. Each (sub)type you wish to use must be indicated
      with a separate <b>-T</b> option on the command line, and automatically
      implies the corresponding type.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">The &quot;text&quot; type is for unstructured plain text.
      No cleanup is performed. This is the default if no types are given on the
      command line.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">The &quot;email&quot; type is for mbox format input files
      or single RFC822 emails. Headers are recognized and most are skipped. To
      include extra RFC822 standard headers (except for trace headers), use the
      &quot;email:headers&quot; subtype. To include trace headers, use the
      &quot;email:theaders&quot; subtype. To include all headers in the email,
      use the &quot;email:xheaders&quot; subtype. To skip all headers, except
      the subject, use &quot;email:noheaders&quot;. To scan binary attachments
      for strings, use the &quot;email:atts&quot; subtype.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">When the &quot;email&quot; type is in effect, HTML markup
      is automatically removed from text attachments except text/plain
      attachments. To also remove HTML markup from plain text attachments, use
      &quot;email:noplain&quot;. To prevent HTML markup removal in all text
      attachments, use &quot;email:plain&quot;.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">The &quot;html&quot; type is for removing HTML markup
      (between &lt;html&gt; and &lt;/html&gt; tags) and surrounding text. Note
      that if the &quot;email&quot; type is enabled, then &quot;html&quot; is
      automatically enabled for compatible message attachments only.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">The &quot;xml&quot; type is like &quot;html&quot;, but
      doesn't honour &lt;html&gt; and &lt;/html&gt;, and doesn't interpret tags
      (so this should be more properly called &quot;angle markup&quot; removal,
      and has nothing to do with actual XML semantics).</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">When &quot;html&quot; is enabled, most markup attributes
      are lost (for values of 'most' close to 'all'). The &quot;html:links&quot;
      subtype forces link urls to be parsed and learned, which would otherwise
      be ignored. The &quot;html:alt&quot; subtype forces parsing of alternative
      text in ALT attributes and various other tags. The
      &quot;html:scripts&quot; subtype forces parsing of scripts,
      &quot;html:styles&quot; forces parsing of styles, &quot;html:forms&quot;
      forces parsing of form values, while &quot;html:comments&quot; forces
      parsing of HTML comments.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-U</dt>
  <dd class="It-tag">Print (U)nambiguity. When used in conjunction with the
      <b>-v</b> switch, prints scores followed by their empirical standard
      deviations. When used alone, prints the best category, followed by an
      estimated probability that this category choice is unambiguous. More
      precisely, the probability measures lack of overlap of CLT confidence
      intervals for each category score (If there is overlap, then there is
      ambiguity).</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">This estimated probability can be used as an
      &quot;unsure&quot; flag, e.g. if the estimated probability is lower than
      50%. Formally, a score of 0% means another category is equally likely to
      apply to the input, and a score of 100% means no other category is likely
      to apply to the input. Note that this type of confidence is unrelated to
      the <b>-X</b> switch. Also, the probability estimate is usually low if the
      document is short, or if the message contains many tokens that have never
      been seen before (only applies to uniform digramic measure).</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-V</dt>
  <dd class="It-tag">Print the program version number and exit.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-W</dt>
  <dd class="It-tag">Like -w, but prevents features from straddling newlines.
      See the description of <b>-w</b>.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-X</dt>
  <dd class="It-tag">Print the confidence in the score calculated for each
      <i>category</i>, when used together with the <b>-n</b> or <b>-N</b>
      switch. Prepares the model for confidence scores, when used with the
      <b>-l</b> switch. The confidence is an estimate of the typicality of the
      score, assuming the null hypothesis that the given category is correct.
      When used with the <b>-v</b> switch alone, factorizes the score as the
      empirical divergence plus the shannon entropy, multiplied by complexity,
      in that order. The <b>-X</b> switch is not supported in all possible
      models, and displays a percentage of &quot;0.0&quot; if it can't be
      calculated. Note that for unknown documents, it is quite common to have
      confidences close to zero.</dd>
</dl>
<h1 class="Sh" title="Sh" id="USAGE"><a class="selflink" href="#USAGE">USAGE</a></h1>
To create two category files in the current directory from two
  <small>ASCII</small> text files named Mark_Twain.txt and
  William_Shakespeare.txt respectively, type:
<div class="Pp"></div>
% dbacl -l twain Mark_Twain.txt
<div>&#x00A0;</div>
% dbacl -l shake William_Shakespeare.txt
<div class="Pp"></div>
Now you can classify input text, for example:
<div class="Pp"></div>
% echo &quot;howdy&quot; | dbacl -v -c twain -c shake
<div>&#x00A0;</div>
twain
<div>&#x00A0;</div>
% echo &quot;to be or not to be&quot; | dbacl -v -c twain -c shake
<div>&#x00A0;</div>
shake
<div class="Pp"></div>
Note that the <b>-v</b> option at least is necessary, otherwise <b>dbacl</b>
  does not print anything. The return value is 1 in the first case, 2 in the
  second.
<div class="Pp"></div>
% echo &quot;to be or not to be&quot; | dbacl -v -N -c twain -c shake
<div>&#x00A0;</div>
twain 22.63% shake 77.37%
<div>&#x00A0;</div>
% echo &quot;to be or not to be&quot; | dbacl -v -n -c twain -c shake
<div>&#x00A0;</div>
twain 7.04 * 6.0 shake 6.74 * 6.0
<div class="Pp"></div>
These invocations are equivalent. The numbers 6.74 and 7.04 represent how close
  the average token is to each category, and 6.0 is the number of tokens
  observed. If you want to print a simple confidence value together with the
  best category, replace <b>-v</b> with <b>-U</b>.
<div class="Pp"></div>
% echo &quot;to be or not to be&quot; | dbacl -U -c twain -c shake
<div>&#x00A0;</div>
shake # 34%
<div class="Pp"></div>
Note that the true probability of category <i>shake</i> versus category
  <i>twain</i> is 77.37%, but the calculation is somewhat ambiguous, and 34% is
  the confidence out of 100% that the calculation is qualitatively correct.
<div class="Pp"></div>
Suppose a file document.txt contains English text lines interspersed with noise
  lines. To filter out the noise lines from the English lines, assuming you have
  an existing category shake say, type:
<div class="Pp"></div>
% dbacl -c shake -f shake -R document.txt &gt; document.txt_eng
<div>&#x00A0;</div>
% dbacl -c shake -f random -R document.txt &gt; document.txt_rnd
<div class="Pp"></div>
Note that the quality of the results will vary depending on how well the
  categories shake and random represent each input line. It is sometimes useful
  to see the posterior probabilities for each line without filtering:
<div class="Pp"></div>
% dbacl -c shake -f shake -RN document.txt &gt; document.txt_probs
<div class="Pp"></div>
You can now postprocess the posterior probabilities for each line of text with
  another script, to replicate an arbitrary Bayesian decision rule of your
  choice.
<div class="Pp"></div>
In the special case of exactly two categories, the optimal Bayesian decision
  procedure can be implemented for documents as follows: let <i>p1</i> be the
  prior probability that the input text is classified as <i>category1</i>.
  Consequently, the prior probability of classifying as <i>category2</i> is 1 -
  <i>p1</i>. Let <i>u12</i> be the cost of misclassifying a <i>category1</i>
  input text as belonging to <i>category2</i> and vice versa for <i>u21</i>. We
  assume there is no cost for classifying correctly. Then the following command
  implements the optimal Bayesian decision:
<div class="Pp"></div>
<div class="Pp" style="margin-left: 5.00ex; text-indent: -5.00ex;">% dbacl -n -c
  <i>category1</i> -c <i>category2</i> | awk '{ if($2 * <i>p1</i> * <i>u12</i>
  &gt; $4 * (1 - <i>p1</i>) * <i>u21</i>) { print $1; } else { print $3; }
  }'</div>
<div class="Pp"></div>
<b>dbacl</b> can also be used in conjunction with <b>procmail</b>(1) to
  implement a simple Bayesian email classification system. Assume that incoming
  mail should be automatically delivered to one of three mail folders located in
  $MAILDIR and named <i>work</i>, <i>personal</i>, and <i>spam</i>. Initially,
  these must be created and filled with appropriate sample emails. A
  <b>crontab</b>(1) file can be used to learn the three categories once a day,
  e.g.
<div class="Pp"></div>
CATS=$HOME/.dbacl
<div>&#x00A0;</div>
5 0 * * * dbacl -T email -l $CATS/work $MAILDIR/work
<div>&#x00A0;</div>
10 0 * * * dbacl -T email -l $CATS/personal $MAILDIR/personal
<div>&#x00A0;</div>
15 0 * * * dbacl -T email -l $CATS/spam $MAILDIR/spam
<div class="Pp"></div>
To automatically deliver each incoming email into the appropriate folder, the
  following <b>procmailrc</b>(5) recipe fragment could be used:
<div class="Pp"></div>
CATS=$HOME/.dbacl
<div class="Pp"></div>
# run the spam classifier
<div>&#x00A0;</div>
:0 c
<div>&#x00A0;</div>
YAY=| dbacl -vT email -c $CATS/work -c $CATS/personal -c $CATS/spam
<div class="Pp"></div>
# send to the appropriate mailbox
<div>&#x00A0;</div>
:0:
<div>&#x00A0;</div>
* ? test -n &quot;$YAY&quot;
<div>&#x00A0;</div>
$MAILDIR/$YAY
<div class="Pp"></div>
:0:
<div>&#x00A0;</div>
$DEFAULT
<div class="Pp"></div>
Sometimes, <b>dbacl</b> will send the email to the wrong mailbox. In that case,
  the misclassified message should be removed from its wrong destination and
  placed in the correct mailbox. The error will be corrected the next time your
  messages are learned. If it is left in the wrong category, <b>dbacl</b> will
  learn the wrong corpus statistics.
<div class="Pp"></div>
The default text features (tokens) read by <b>dbacl</b> are purely alphabetic
  strings, which minimizes memory requirements but can be unrealistic in some
  cases. To construct models based on alphanumeric tokens, use the <b>-e</b>
  switch. The example below also uses the optional <b>-D</b> switch, which
  prints a list of actual tokens found in the document:
<div class="Pp"></div>
% dbacl -e alnum -D -l twain Mark_Twain.txt | less
<div class="Pp"></div>
It is also possible to override the default feature selection method used to
  learn the category model by means of regular expressions. For example, the
  following duplicates the default feature selection method in the C locale,
  while being much slower:
<div class="Pp"></div>
<div class="Pp" style="margin-left: 5.00ex; text-indent: -5.00ex;">% dbacl -l
  twain -g '^([[:alpha:]]+)' -g '[^[:alpha:]]([[:alpha:]]+)'
  Mark_Twain.txt</div>
<div class="Pp"></div>
The category twain which is obtained depends only on single alphabetic words in
  the text file Mark_Twain.txt (and computed digram statistics for prediction).
  For a second example, the following command builds a smoothed Markovian (word
  bigram) model which depends on pairs of consecutive words within each line
  (but pairs cannot straddle a line break):
<div class="Pp"></div>
<div class="Pp" style="margin-left: 5.00ex; text-indent: -5.00ex;">% dbacl -l
  twain2 -g '(^|[^[:alpha:]])([[:alpha:]]+)||2' -g
  '(^|[^[:alpha:]])([[:alpha:]]+)[^[:alpha:]]+([[:alpha:]]+)||23'
  Mark_Twain.txt</div>
<div class="Pp"></div>
More general, line based, n-gram models of all orders (up to 7) can be built in
  a similar way. To construct paragraph based models, you should reformat the
  input corpora with <b>awk</b>(1) or <b>sed</b>(1) to obtain one paragraph per
  line. Line size is limited by available memory, but note that regex
  performance will degrade quickly for long lines.
<h1 class="Sh" title="Sh" id="PERFORMANCE"><a class="selflink" href="#PERFORMANCE">PERFORMANCE</a></h1>
The underlying assumption of statistical learning is that a relatively small
  number of training documents can represent a much larger set of input
  documents. Thus in the long run, learning can grind to a halt without serious
  impact on classification accuracy. While not true in reality, this assumption
  is surprisingly accurate for problems such as email filtering. In practice,
  this means that a well chosen corpus on the order of ten thousand documents is
  sufficient for highly accurate results for years. Continual learning after
  such a critical mass results in diminishing returns. Of course, when real
  world input document patterns change dramatically, the predictive power of the
  models can be lost. At the other end, a few hundred documents already give
  acceptable results in most cases.
<div class="Pp"></div>
<b>dbacl</b> is heavily optimized for the case of frequent classifications but
  infrequent batch learning. This is the long run optimum described above. Under
  ideal conditions, <b>dbacl</b> can classify a hundred emails per second on low
  end hardware (500Mhz Pentium III). Learning speed is not very much slower, but
  takes effectively much longer for large document collections for various
  reasons. When using the <b>-m</b> switch, data structures are aggressively
  mapped into memory if possible, reducing overheads for both I/O and memory
  allocations.
<div class="Pp"></div>
<b>dbacl</b> throws away its input as soon as possible, and has no limits on the
  input document size. Both classification and learning speed are directly
  proportional to the number of tokens in the input, but learning also needs a
  nonlinear optimization step which takes time proportional to the number of
  unique tokens discovered. At time of writing, <b>dbacl</b> is one of the
  fastest open source mail filters given its optimal usage scenario, but uses
  more memory for learning than other filters.
<h1 class="Sh" title="Sh" id="MULTIPLE_PROCESSES_AND_DATA_CORRUPTION"><a class="selflink" href="#MULTIPLE_PROCESSES_AND_DATA_CORRUPTION">MULTIPLE
  PROCESSES AND DATA CORRUPTION</a></h1>
When saving category files, <b>dbacl</b> first writes out a temporary file in
  the same location, and renames it afterwards. If a problem or crash occurs
  during learning, the old category file is therefore left untouched. This
  ensures that categories can never be corrupted, no matter how many processes
  try to simultaneously learn or classify, and means that valid categories are
  available for classification at any time.
<div class="Pp"></div>
When using the <b>-m</b> switch, file contents are memory mapped for speedy
  reading and writing. This, together with the <b>-o</b> switch, is intended
  mainly for testing purposes, when tens of thousands of messages must be
  learned and scored in a laboratory to measure <b>dbacl</b>'s accuracy. Because
  no file locking is attempted for performance reasons, corruptions are
  possible, unless you make sure that only one <b>dbacl</b> process reads or
  writes any file at any given time. This is the only case (-m and -o together)
  when corruption is possible.
<h1 class="Sh" title="Sh" id="MEMORY_USE"><a class="selflink" href="#MEMORY_USE">MEMORY
  USE</a></h1>
When classifying a document, <b>dbacl</b> loads all indicated categories into
  RAM, so the total memory needed is approximately the sum of the category file
  sizes plus a fixed small overhead. The input document is consumed while being
  read, so its size doesn't matter, but very long lines can take up space. When
  using the <b>-m</b> switch, the categories are read using <b>mmap</b>(2) as
  available.
<div class="Pp"></div>
When learning, <b>dbacl</b> keeps a large structure in memory which contains
  many objects which won't be saved into the output category. The size of this
  structure is proportional to the number of unique tokens read, but not the
  size of the input documents, since they are discarded while being read. As a
  rough guide, this structure is 4x-5x the size of the final category file that
  is produced.
<div class="Pp"></div>
To prevent unchecked memory growth, <b>dbacl</b> allocates by default a fixed
  smallish amount of memory for tokens. When this space is used up, further
  tokens are discarded which has the effect of skewing the learned category
  making it less usable as more tokens are dropped. A warning is printed on
  STDERR in such a case.
<div class="Pp"></div>
The <b>-h</b> switch lets you fix the initial size of the token space in powers
  of 2, ie &quot;-h 17&quot; means 2^17 = 131072 possible tokens. If you type
  &quot;dbacl -V&quot;, you can see the number of bytes needed for each token
  when either learning or classifying. Multiply this number by the maximum
  number of possible tokens to estimate the memory needed for learning. The
  <b>-H</b> switch lets <b>dbacl</b> grow its tables automatically if and when
  needed, up to a maximum specified. So if you type &quot;-H 21&quot;, then the
  initial size will be doubled repeatedly if necessary, up to approximately two
  million unique tokens.
<div class="Pp"></div>
When learning with the <b>-X</b> switch, a handful of input documents are also
  kept in RAM throughout.
<h1 class="Sh" title="Sh" id="ENVIRONMENT"><a class="selflink" href="#ENVIRONMENT">ENVIRONMENT</a></h1>
<dl class="Bl-tag">
  <dt class="It-tag">DBACL_PATH</dt>
  <dd class="It-tag">When this variable is set, its value is prepended to every
      <i>category</i> filename which doesn't start with a '/' or a '.'.</dd>
</dl>
<h1 class="Sh" title="Sh" id="SIGNALS"><a class="selflink" href="#SIGNALS">SIGNALS</a></h1>
<dl class="Bl-tag">
  <dt class="It-tag">INT</dt>
  <dd class="It-tag">If this signal is caught, <b>dbacl</b> simply exits without
      doing any cleanup or other operations. This signal can often be sent by
      pressing Ctrl-C on the keyboard. See <b>stty</b>(1).</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">HUP, QUIT, TERM</dt>
  <dd class="It-tag">If one of these signals is caught, <b>dbacl</b> stops
      reading input and continues its operation as if no more input was
      available. This is a way of quitting gracefully, but note that in learning
      mode, a category file will be written based on the incomplete input. The
      QUIT signal can often be sent by pressing Ctrl-&#x00A0;on the keyboard.
      See <b>stty</b>(1).</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">USR1</dt>
  <dd class="It-tag">If this signal is caught, <b>dbacl</b> reloads the current
      categories at the earliest feasible opportunity. This is not normally
      useful at all, but might be in special cases, such as if the <b>-f</b>
      switch is invoked together with input from a long running pipe.</dd>
</dl>
<h1 class="Sh" title="Sh" id="NOTES"><a class="selflink" href="#NOTES">NOTES</a></h1>
<b>dbacl</b> generated category files are in binary format, and may or may not
  be portable to systems using a different byte order architecture (this depends
  on how <b>dbacl</b> was compiled). The <b>-V</b> switch prints out whether
  categories are portable, or else you can just experiment.
<div class="Pp"></div>
<b>dbacl</b> does not recognize functionally equivalent regular expressions, and
  in this case duplicate features will be counted several times.
<div class="Pp"></div>
With every learned category, the command line options that were used are saved.
  When classifying, make sure that every relevant category was learned with the
  same set of options (regexes are allowed to differ), otherwise behaviour is
  undefined. There is no need to repeat all the switches when classifying.
<div class="Pp"></div>
If you get many digitization warnings, then you are trying to learn too much
  data at once, or your model is too complex. <b>dbacl</b> is compiled to save
  memory by digitizing final weights, but you can disable digitization by
  editing dbacl.h and recompiling.
<div class="Pp"></div>
<b>dbacl</b> offers several built-in tokenizers (see <b>-e</b> switch) with more
  to come in future versions, as the author invents them. While the default
  tokenizer may evolve, no tokenizer should ever be removed, so that you can
  always simulate previous <b>dbacl</b> behaviour subject to bug fixes and
  architectural changes.
<div class="Pp"></div>
The confidence estimates obtained through the <b>-X</b> switch are
  underestimates, ie are more conservative than they should be.
<h1 class="Sh" title="Sh" id="BUGS"><a class="selflink" href="#BUGS">BUGS</a></h1>
&quot;Ya know, some day scientists are gonna invent something that will outsmart
  a rabbit.&quot; (Robot Rabbit, 1953)
<h1 class="Sh" title="Sh" id="SOURCE"><a class="selflink" href="#SOURCE">SOURCE</a></h1>
The source code for the latest version of this program is available at the
  following locations:
<div class="Pp"></div>
http://www.lbreyer.com/gpl.html
<div>&#x00A0;</div>
http://dbacl.sourceforge.net
<h1 class="Sh" title="Sh" id="AUTHOR"><a class="selflink" href="#AUTHOR">AUTHOR</a></h1>
Laird A. Breyer &lt;laird@lbreyer.com&gt;
<h1 class="Sh" title="Sh" id="SEE_ALSO"><a class="selflink" href="#SEE_ALSO">SEE
  ALSO</a></h1>
<b>awk</b>(1), <b>bayesol</b>(1), <b>crontab</b>(1), <b>hmine</b>(1),
  <b>hypex</b>(1), <b>less</b>(1), <b>mailcross</b>(1), <b>mailfoot</b>(1),
  <b>mailinspect</b>(1), <b>mailtoe</b>(1), <b>procmailex</b>(5),
  <b>regex</b>(7), <b>stty</b>(1), <b>sed</b>(1)</div>
<table class="foot">
  <tr>
    <td class="foot-date">Bayesian Text Classification Tools</td>
    <td class="foot-os">Version 1.12</td>
  </tr>
</table>
</body>
</html>
