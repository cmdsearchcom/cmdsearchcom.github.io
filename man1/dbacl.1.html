<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 16:02:30 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p style="margin-top: 1em">DBACL(1) DBACL(1)</p>

<p style="margin-top: 1em">NAME <br>
dbacl - a digramic Bayesian classifier for text
recognition.</p>

<p style="margin-top: 1em">SYNOPSIS <br>
dbacl [-01dvnirmwMNDXW] [-T type ] -l category [-h size] [-H
gsize] [-x decim] [-q quality] [-w max_order] [-e deftok]
[-o online] [-L measure] [-g regex]... [FILE]...</p>

<p style="margin-top: 1em">dbacl [-vnimNRX] [-h size] [-T
type] -c category [-c category]... [-f keep]...
[FILE]...</p>

<p style="margin-top: 1em">dbacl -V</p>

<p style="margin-top: 1em">OVERVIEW <br>
dbacl is a Bayesian text and email classifier. When using
the -l switch, it learns a body of text and produce a file
named category which summarizes the text. When using the -c
<br>
switch, it compares an input text stream with any number of
category files, and outputs the name of the closest match,
or optionally various numerical scores explained below.</p>

<p style="margin-top: 1em">Whereas this manual page is
intended as a reference, there are several tutorials and
documents you can read to get specialized information.
Specific documentation about the <br>
design of dbacl and the statistical models that it uses can
be found in dbacl.ps. For a basic overview of text
classification using dbacl, see tutorial.html. A companion
tuto&acirc; <br>
rial geared towards email filtering is email.html. If you
have trouble getting dbacl to classify reliably, read
is_it_working.html. The USAGE section of this manual page
also <br>
has some examples.</p>


<p style="margin-top: 1em">/usr/share/doc/dbacl/dbacl.ps</p>


<p style="margin-top: 1em">/usr/share/doc/dbacl/tutorial.html</p>


<p style="margin-top: 1em">/usr/share/doc/dbacl/email.html</p>


<p style="margin-top: 1em">/usr/share/doc/dbacl/is_it_working.html</p>

<p style="margin-top: 1em">dbacl uses a maximum entropy
(minimum divergence) language model constructed with respect
to a digramic reference measure (unknown tokens are
predicted from digrams, i.e. pairs <br>
of letters). Practically, this means that a category is
constructed from tokens in the training set, while
previously unseen tokens can be predicted automatically from
their let&acirc; <br>
ters. A token here is either a word (fragment) or a
combination of words (fragments), selected according to
various switches. Learning roughly works by tweaking token
probabili&acirc; <br>
ties until the training data is least surprising.</p>

<p style="margin-top: 1em">EXIT STATUS <br>
The normal shell exit conventions aren&rsquo;t followed
(sorry!). When using the -l command form, dbacl returns zero
on success, nonzero if an error occurs. When using the -c
form, <br>
dbacl returns a positive integer corresponding to the
category with the highest posterior probability. In case of
a tie, the first most probable category is chosen. If an
error <br>
occurs, dbacl returns zero.</p>

<p style="margin-top: 1em">DESCRIPTION <br>
When using the -l command form, dbacl learns a category when
given one or more FILE names, which should contain readable
ASCII text. If no FILE is given, dbacl learns from STDIN.
<br>
If FILE is a directory, it is opened and all its files are
read, but not its subdirectories. The result is saved in the
binary file named category, and completely replaces any <br>
previous contents. As a convenience, if the environment
variable DBACL_PATH contains a directory, then that is
prepended to the file path, unless category starts with a
&rsquo;/&rsquo; or a <br>
&rsquo;.&rsquo;.</p>

<p style="margin-top: 1em">The input text for learning is
assumed to be unstructured plain text by default. This is
not suitable for learning email, because email contains
various transport encodings and <br>
formatting instructions which can reduce classification
effectiveness. You must use the -T switch in that case so
that dbacl knows it should perform decoding and filtering of
<br>
MIME and HTML as appropriate. Apropriate switch values are
&quot;-T email&quot; for RFC2822 email input, &quot;-T
html&quot; for HTML input, &quot;-T xml&quot; for generic
XML style input and &quot;-T text&quot; is <br>
the default plain text format. There are other values of the
-T switch that also allow fine tuning of the decoding
capabilities.</p>

<p style="margin-top: 1em">When using the -c command form,
dbacl attempts to classify the text found in FILE, or STDIN
if no FILE is given. Each possible category must be given
separately, and should be <br>
the file name of a previously learned text corpus. As a
convenience, if the variable DBACL_PATH contains a
directory, it is prepended to each file path which
doesn&rsquo;t start with a <br>
&rsquo;/&rsquo; or a &rsquo;.&rsquo;. The visible output of
the classification depends on the combination of extra
switches used. If no switch is used, then no output is shown
on STDOUT. However, dbacl <br>
always produces an exit code which can be tested.</p>

<p style="margin-top: 1em">To see an output for a
classification, you must use at least one of the
-v,-U,-n,-N,-D,-d switches. Sometimes, they can be used in
combination to produce a natural variation of <br>
their individual outputs. Sometimes, dbacl also produces
warnings on STDERR if applicable.</p>

<p style="margin-top: 1em">The -v switch outputs the name
of the best category among all the choices given.</p>

<p style="margin-top: 1em">The -U switch outputs the name
of the best category followed by a confidence percentage.
Normally, this is the switch that you want to use. A
percentage of 100% means that dbacl <br>
is sure of its choice, while a percentage of 0% means that
some other category is equally likely. This is not the model
probability, but measures how unambiguous the
classifica&acirc; <br>
tion is, and can be used to tag unsure classifications (e.g.
if the confidence is 25% or less).</p>

<p style="margin-top: 1em">The -N switch prints each
category name followed by its (posterior) probability,
expressed as a percentage. The percentages always sum to
100%. This is intuitive, but only valu&acirc; <br>
able if the document being classified contains a handful of
tokens (ten or less). In the common case with many more
tokens, the probabilities are always extremely close to 100%
<br>
and 0%.</p>

<p style="margin-top: 1em">The -n switch prints each
category name followed by the negative logarithm of its
probability. This is equivalent to using the -N switch, but
much more useful. The smallest num&acirc; <br>
ber gives the best category. A more convenient form is to
use both -n and -v which prints each category name followed
by the cross entropy and the number of tokens analyzed. The
<br>
cross entropy measures (in bits) the average compression
rate which is achievable, under the given category model,
per token of input text. If you use all three of -n,-v,-X
then <br>
an extra value is output for each category, representing a
kind of p-value for each category score. This indicates how
typical the score is compared to the training documents,
<br>
but only works if the -X switch was used during learning,
and only for some types of models (e.g. email). These
p-values are uniformly distributed and independent (if the
cate&acirc; <br>
gories are independent), so can be combined using
Fisher&rsquo;s chi squared test to obtain composite p-values
for groupings of categories.</p>

<p style="margin-top: 1em">The -v and -X switches together
print each category name followed by a detailed
decomposition of the category score, factored into (
divergence rate + shannon entropy rate )* <br>
token count @ p-value. Again, this only works in some types
of models.</p>

<p style="margin-top: 1em">The -v and -U switches print
each category name followed by a decomposition of the
category score into ( divergence rate + shannon entropy rate
# score variance )* token count.</p>

<p style="margin-top: 1em">The -D switch prints out the
input text as modified internally by dbacl prior to
tokenization. For example, if a MIME encoded email document
is classified, then this prints the <br>
decoded text that will be actually tokenized and classified.
This switch is mainly useful for debugging.</p>

<p style="margin-top: 1em">The -d switch dumps tokens and
scores while they are being read. It is useful for
debugging, or if you want to create graphical
representations of the classification. A detailed <br>
explanation of the output is beyond the scope of this manual
page, but is straightforward if you&rsquo;ve read dbacl.ps.
Possible variations include -d together with -n or -N.</p>

<p style="margin-top: 1em">Classification can be done with
one or several categories in principle. When two or more
categories are used, the Bayesian posterior probability is
used, given the input text, <br>
with a uniform prior distribution on categories. For other
choices of prior, see the companion utility bayesol(1). When
a single category is used, classification can be done by
<br>
comparing the score with a treshold. In practice however,
much better results are obtained with several
categories.</p>

<p style="margin-top: 1em">Learning and classifying cannot
be mixed on the same command invocation, however there are
no locking issues and separate dbacl processes can operate
simultaneously with obvious <br>
results, because file operations are designed to be
atomic.</p>

<p style="margin-top: 1em">Finally, note that dbacl does
not manage your document corpora or your computed
categories, and in particular it does not allow you to
extend an existing category file with new <br>
documents. This is unlike various current spam filters,
which can learn new emails incrementally. This limitation of
dbacl is partially due to the nonlinear procedure used in
<br>
the learning algorithm, and partially a desire for increased
flexibility.</p>

<p style="margin-top: 1em">You can simulate the effect of
incremental learning by saving your training documents into
archives and adding to these archives over time, relearning
from scratch periodically. <br>
Learning is actually faster if these archives are compressed
and decompressed on the fly when needed. By keeping control
of your archives, you can never lose the information in <br>
your categories, and you can easily experiment with
different switches or tokenizations or sets of training
documents if you like.</p>

<p style="margin-top: 1em">SECONDARY SWITCHES <br>
By default, dbacl classifies the input text as a whole.
However, when using the -f option, dbacl can be used to
filter each input line separately, printing only those lines
which <br>
match one or more models identified by keep (use the
category name or number to refer to a category). This is
useful if you want to filter out some lines, but note that
if the <br>
lines are short, then the error rate can be high.</p>

<p style="margin-top: 1em">The -e,-w,-g,-j switches are
used for selecting an appropriate tokenization scheme. A
token is a word or word fragment or combination of words or
fragments. The shape of tokens <br>
is important because it forms the basis of the language
models used by dbacl. The -e switch selects a predefined
tokenization scheme, which is speedy but limited. The -w
switch <br>
specifies composite tokens derived from the -e switch. For
example, &quot;-e alnum -w 2&quot; means that tokens should
be alphanumeric word fragments combined into overlapping
pairs <br>
(bigrams). When the -j switch is used, all tokens are
converted to lowercase, which reduces the number of possible
tokens and therefore memory consumption.</p>

<p style="margin-top: 1em">If the -g switch is used, you
can completely specify what the tokens should look like
using a regular expression. Several -g switches can be used
to construct complex tokeniza&acirc; <br>
tion schemes, and parentheses within each expression can be
used to select fragments and combine them into n-grams. The
cost of such flexibility is reduced classification and <br>
learning speed. When experimenting with tokenization
schemes, try using the -d or -D switches while learning or
classifying, as they will print the tokens explicitly so you
can <br>
see what text fragments are picked up or missed out. For
regular exression syntax, see regex(7).</p>

<p style="margin-top: 1em">The -h and -H switches regulate
how much memory dbacl may use for learning. Text
classification can use a lot of memory, and by default dbacl
limits itself even at the expense of <br>
learning accuracy. In many cases if a limit is reached, a
warning message will be printed on STDERR with some
advice.</p>

<p style="margin-top: 1em">When relearning the same
category several times, a significant speedup can be
obtained by using the -1 switch, as this allows the
previously learned probabilities to be read from <br>
the category and reused.</p>

<p style="margin-top: 1em">Note that classification
accuracy depends foremost on the amount and quality of the
training samples, and then only on amount of tweaking.</p>

<p style="margin-top: 1em">EXIT STATUS <br>
When using the -l command form, dbacl returns zero on
success. When using the -c form, dbacl returns a positive
integer (1,2,3...) corresponding to the category with the
highest <br>
posterior probability. In case of a tie, the first most
probable category is chosen. If an error occurs, dbacl
returns zero.</p>

<p style="margin-top: 1em">OPTIONS <br>
-0 When learning, prevents weight preloading. Normally,
dbacl checks if the category file already exists, and if so,
tries to use the existing weights as a starting point. <br>
This can dramatically speed up learning. If the -0 (zero)
switch is set, then dbacl behaves as if no category file
already exists. This is mainly useful for testing. <br>
This switch is now enabled by default, to protect against
weight drift which can reduce accuracy over many learning
iterations. Use -1 to force preloading.</p>

<p style="margin-top: 1em">-1 Force weight preloading if
the category file already exists. See discussion of the -0
switch.</p>

<p style="margin-top: 1em">-a Append scores. Every input
line is written to STDOUT and the dbacl scores are appended.
This is useful for postprocessing with bayesol(1). For ease
of processing, every <br>
original input line is indented by a single space (to
distinguish them from the appended scores), and the line
with the scores (if -n is used) is prefixed with the string
<br>
&quot;scores &quot;. If a second copy of dbacl needs to read
this output later, it should be invoked with the -A
switch.</p>

<p style="margin-top: 1em">-d Dump the model parameters to
STDOUT. In conjunction with the -l option, this produces a
human-readable summary of the maximum entropy model. In
conjunction with the -c <br>
option, displays the contribution of each token to the final
score. Suppresses all other normal output.</p>

<p style="margin-top: 1em">-e Select character class for
default (not regex-based) tokenization. By default, tokens
are alphabetic strings only. This corresponds to the case
when deftok is &quot;alpha&quot;. <br>
Possible values for deftok are &quot;alpha&quot;,
&quot;alnum&quot;, &quot;graph&quot;, &quot;char&quot;,
&quot;cef&quot; and &quot;adp&quot;. The last two are custom
tokenizers intended for email messages. See also isalpha(3).
<br>
The &quot;char&quot; tokenizer picks up single printable
characters rather than bigger tokens, and is intended for
testing only.</p>

<p style="margin-top: 1em">-f Filter each line of input
separately, passing to STDOUT only lines which match the
category identified as keep. This option should be used
repeatedly for each category <br>
which must be kept. keep can be either the category file
name, or a positive integer representing the required
category in the same order it appears on the command
line.</p>

<p style="margin-top: 1em">Output lines are flushed as soon
as they are written. If the input file is a pipe or
character device, then an attempt is made to use line
buffering mode, otherwise the <br>
more efficient block buffering is used.</p>

<p style="margin-top: 1em">-g Learn only features described
by the extended regular expression regex. This overrides the
default feature selection method (see -w option) and learns,
for each line of <br>
input, only tokens constructed from the concatenation of
strings which match the tagged subexpressions within the
supplied regex. All substrings which match regex within <br>
a suffix of each input line are treated as features, even if
they overlap on the input line.</p>

<p style="margin-top: 1em">As an optional convenience,
regex can include the suffix ||xyz which indicates which
parenthesized subexpressions should be tagged. In this case,
xyz should consist exclu&acirc; <br>
sively of digits 1 to 9, numbering exactly those
subexpressions which should be tagged. Alternatively, if no
parentheses exist within regex, then it is assumed that the
<br>
whole expression must be captured.</p>

<p style="margin-top: 1em">-h Set the size of the hash
table to 2^size elements. When using the -l option, this
refers to the total number of features allowed in the
maximum entropy model being <br>
learned. When using the -c option toghether with the -M
switch and multinomial type categories, this refers to the
maximum number of features taken into account during <br>
classification. Without the -M switch, this option has no
effect.</p>

<p style="margin-top: 1em">-i Fully internationalized mode.
Forces the use of wide characters internally, which is
necessary in some locales. This incurs a noticeable
performance penalty.</p>

<p style="margin-top: 1em">-j Make features case sensitive.
Normally, all features are converted to lower case during
processing, which reduces storage requirements and improves
statistical estimates <br>
for small datasets. With this option, the original
capitalization is used for each feature. This can improve
classification accuracy.</p>

<p style="margin-top: 1em">-m Aggressively maps categories
into memory and locks them into RAM to prevent swapping, if
possible. This is useful when speed is paramount and memory
is plentiful, for <br>
example when testing the classifier on large datasets.</p>

<p style="margin-top: 1em">Locking may require relaxing
user limits with ulimit(1). Ask your system administrator.
Beware when using the -m switch together with the -o switch,
as only one dbacl <br>
process must learn or classify at a time to prevent file
corruption. If no learning takes place, then the -m switch
for classifying is always safe to use. See also the <br>
discussion for the -o switch.</p>

<p style="margin-top: 1em">-n Print scores for each
category. Each score is the product of two numbers, the
cross entropy and the complexity of the input text under
each model. Multiplied together, <br>
they represent the log probability that the input resembles
the model. To see these numbers separately, use also the -v
option. In conjunction with the -f option, stops <br>
filtering but prints each input line prepended with a list
of scores for that line.</p>

<p style="margin-top: 1em">-q Select quality of learning,
where quality can be 1,2,3,4. Higher values take longer to
learn, and should be slightly more accurate. The default
quality is 1 if the cate&acirc; <br>
gory file doesn&rsquo;t exist or weights cannot be
preloaded, and 2 otherwise.</p>

<p style="margin-top: 1em">-o When learning, reads/writes
partial token counts so they can be reused. Normally,
category files are learned from exactly the input data
given, and don&rsquo;t contain extrane&acirc; <br>
ous information. When this option is in effect, some extra
information is saved in the file online, after all input was
read. This information can be reread the next time <br>
that learning occurs, to continue where the previous dataset
left off. If online doesn&rsquo;t exist, it is created. If
online exists, it is read before learning, and updated <br>
afterwards. The file is approximately 3 times bigger (at
least) than the learned category.</p>

<p style="margin-top: 1em">In dbacl, file updates are
atomic, but if using the -o switch, two or more processes
should not learn simultaneously, as only one process will
write a lasting category and <br>
memory dump. The -m switch can also speed up online
learning, but beware of possible corruption. Only one
process should read or write a file. This option is intended
<br>
primarily for controlled test runs.</p>

<p style="margin-top: 1em">-r Learn the digramic reference
model only. Skips the learning of extra features in the text
corpus.</p>

<p style="margin-top: 1em">-v Verbose mode. When learning,
print out details of the computation, when classifying,
print out the name of the most probable category. In
conjunction with the -n option, <br>
prints the scores as an explicit product of the cross
entropy and the complexity.</p>

<p style="margin-top: 1em">-w Select default features to be
n-grams up to max_order. This is incompatible with the -g
option, which always takes precedence. If no -w or -g
options are given, dbacl <br>
assumes -w 1. Note that n-grams for n greater than 1 do not
straddle line breaks by default. The -S switch enables line
straddling.</p>

<p style="margin-top: 1em">-x Set decimation probability to
1 - 2^(-decim). To reduce memory requirements when learning,
some inputs are randomly skipped, and only a few are added
to the model. Exact <br>
behaviour depends on the applicable -T option (default is -T
&quot;text&quot;). When the type is not &quot;email&quot;
(eg &quot;text&quot;), then individual input features are
added with probability <br>
2^(-decim). When the type is &quot;email&quot;, then full
input messages are added with probability 2^(-decim). Within
each such message, all features are used.</p>

<p style="margin-top: 1em">-A Expect indented input and
scores. With this switch, dbacl expects input lines to be
indented by a single space character (which is then
skipped). Lines starting with any <br>
other character are ignored. This is the counterpart to the
-a switch above. When used together with the -a switch,
dbacl outputs the skipped lines as they are, and rein&acirc;
<br>
serts the space at the front of each processed input
line.</p>

<p style="margin-top: 1em">-D Print debug output. Do not
use normally, but can be very useful for displaying the list
features picked up while learning.</p>

<p style="margin-top: 1em">-H Allow hash table to grow up
to a maximum of 2^gsize elements during learning. Initial
size is given by -h option.</p>

<p style="margin-top: 1em">-L Select the digramic reference
measure for character transitions. The measure can be one of
&quot;uniform&quot;, &quot;dirichlet&quot; or
&quot;maxent&quot;. Default is &quot;uniform&quot;.</p>

<p style="margin-top: 1em">-M Force multinomial
calculations. When learning, forces the model features to be
treated multinomially. When classifying, corrects entropy
scores to reflect multinomial <br>
probabilities (only applicable to multinomial type models,
if present). Scores will always be lower, because the
ordering of features is lost.</p>

<p style="margin-top: 1em">-N Print posterior probabilities
for each category. This assumes the supplied categories form
an exhaustive list of possibilities. In conjunction with the
-f option, stops <br>
filtering but prints each input line prepended with a
summary of the posterior distribution for that line.</p>

<p style="margin-top: 1em">-R Include an extra category for
purely random text. The category is called
&quot;random&quot;. Only makes sense when using the -c
option.</p>

<p style="margin-top: 1em">-S Enable line straddling. This
is useful together with the -w option to allow n-grams for n
&gt; 1 to ignore line breaks, so a complex token can
continue past the end of the <br>
line. This is not recommended for email.</p>

<p style="margin-top: 1em">-T Specify nonstandard text
format. By default, dbacl assumes that the input text is a
purely ASCII text file. This corresponds to the case when
type is &quot;text&quot;.</p>

<p style="margin-top: 1em">There are several types and
subtypes which can be used to clean the input text of
extraneous tokens before actual learning or classifying
takes place. Each (sub)type you <br>
wish to use must be indicated with a separate -T option on
the command line, and automatically implies the
corresponding type.</p>

<p style="margin-top: 1em">The &quot;text&quot; type is for
unstructured plain text. No cleanup is performed. This is
the default if no types are given on the command line.</p>

<p style="margin-top: 1em">The &quot;email&quot; type is
for mbox format input files or single RFC822 emails. Headers
are recognized and most are skipped. To include extra RFC822
standard headers (except for <br>
trace headers), use the &quot;email:headers&quot; subtype.
To include trace headers, use the &quot;email:theaders&quot;
subtype. To include all headers in the email, use the
&quot;email:xheaders&quot; <br>
subtype. To skip all headers, except the subject, use
&quot;email:noheaders&quot;. To scan binary attachments for
strings, use the &quot;email:atts&quot; subtype.</p>

<p style="margin-top: 1em">When the &quot;email&quot; type
is in effect, HTML markup is automatically removed from text
attachments except text/plain attachments. To also remove
HTML markup from plain text <br>
attachments, use &quot;email:noplain&quot;. To prevent HTML
markup removal in all text attachments, use
&quot;email:plain&quot;.</p>

<p style="margin-top: 1em">The &quot;html&quot; type is for
removing HTML markup (between &lt;html&gt; and &lt;/html&gt;
tags) and surrounding text. Note that if the
&quot;email&quot; type is enabled, then &quot;html&quot; is
automatically <br>
enabled for compatible message attachments only.</p>

<p style="margin-top: 1em">The &quot;xml&quot; type is like
&quot;html&quot;, but doesn&rsquo;t honour &lt;html&gt; and
&lt;/html&gt;, and doesn&rsquo;t interpret tags (so this
should be more properly called &quot;angle markup&quot;
removal, and has <br>
nothing to do with actual XML semantics).</p>

<p style="margin-top: 1em">When &quot;html&quot; is
enabled, most markup attributes are lost (for values of
&rsquo;most&rsquo; close to &rsquo;all&rsquo;). The
&quot;html:links&quot; subtype forces link urls to be parsed
and learned, which <br>
would otherwise be ignored. The &quot;html:alt&quot; subtype
forces parsing of alternative text in ALT attributes and
various other tags. The &quot;html:scripts&quot; subtype
forces parsing <br>
of scripts, &quot;html:styles&quot; forces parsing of
styles, &quot;html:forms&quot; forces parsing of form
values, while &quot;html:comments&quot; forces parsing of
HTML comments.</p>

<p style="margin-top: 1em">-U Print (U)nambiguity. When
used in conjunction with the -v switch, prints scores
followed by their empirical standard deviations. When used
alone, prints the best cate&acirc; <br>
gory, followed by an estimated probability that this
category choice is unambiguous. More precisely, the
probability measures lack of overlap of CLT confidence
intervals <br>
for each category score (If there is overlap, then there is
ambiguity).</p>

<p style="margin-top: 1em">This estimated probability can
be used as an &quot;unsure&quot; flag, e.g. if the estimated
probability is lower than 50%. Formally, a score of 0% means
another category is equally <br>
likely to apply to the input, and a score of 100% means no
other category is likely to apply to the input. Note that
this type of confidence is unrelated to the -X switch. <br>
Also, the probability estimate is usually low if the
document is short, or if the message contains many tokens
that have never been seen before (only applies to uniform
<br>
digramic measure).</p>

<p style="margin-top: 1em">-V Print the program version
number and exit.</p>

<p style="margin-top: 1em">-W Like -w, but prevents
features from straddling newlines. See the description of
-w.</p>

<p style="margin-top: 1em">-X Print the confidence in the
score calculated for each category, when used together with
the -n or -N switch. Prepares the model for confidence
scores, when used with the <br>
-l switch. The confidence is an estimate of the typicality
of the score, assuming the null hypothesis that the given
category is correct. When used with the -v switch <br>
alone, factorizes the score as the empirical divergence plus
the shannon entropy, multiplied by complexity, in that
order. The -X switch is not supported in all possible <br>
models, and displays a percentage of &quot;0.0&quot; if it
can&rsquo;t be calculated. Note that for unknown documents,
it is quite common to have confidences close to zero.</p>

<p style="margin-top: 1em">USAGE <br>
To create two category files in the current directory from
two ASCII text files named Mark_Twain.txt and
William_Shakespeare.txt respectively, type:</p>

<p style="margin-top: 1em">% dbacl -l twain Mark_Twain.txt
<br>
% dbacl -l shake William_Shakespeare.txt</p>

<p style="margin-top: 1em">Now you can classify input text,
for example:</p>

<p style="margin-top: 1em">% echo &quot;howdy&quot; | dbacl
-v -c twain -c shake <br>
twain <br>
% echo &quot;to be or not to be&quot; | dbacl -v -c twain -c
shake <br>
shake</p>

<p style="margin-top: 1em">Note that the -v option at least
is necessary, otherwise dbacl does not print anything. The
return value is 1 in the first case, 2 in the second.</p>

<p style="margin-top: 1em">% echo &quot;to be or not to
be&quot; | dbacl -v -N -c twain -c shake <br>
twain 22.63% shake 77.37% <br>
% echo &quot;to be or not to be&quot; | dbacl -v -n -c twain
-c shake <br>
twain 7.04 * 6.0 shake 6.74 * 6.0</p>

<p style="margin-top: 1em">These invocations are
equivalent. The numbers 6.74 and 7.04 represent how close
the average token is to each category, and 6.0 is the number
of tokens observed. If you want to <br>
print a simple confidence value together with the best
category, replace -v with -U.</p>

<p style="margin-top: 1em">% echo &quot;to be or not to
be&quot; | dbacl -U -c twain -c shake <br>
shake # 34%</p>

<p style="margin-top: 1em">Note that the true probability
of category shake versus category twain is 77.37%, but the
calculation is somewhat ambiguous, and 34% is the confidence
out of 100% that the calcu&acirc; <br>
lation is qualitatively correct.</p>

<p style="margin-top: 1em">Suppose a file document.txt
contains English text lines interspersed with noise lines.
To filter out the noise lines from the English lines,
assuming you have an existing cate&acirc; <br>
gory shake say, type:</p>

<p style="margin-top: 1em">% dbacl -c shake -f shake -R
document.txt &gt; document.txt_eng <br>
% dbacl -c shake -f random -R document.txt &gt;
document.txt_rnd</p>

<p style="margin-top: 1em">Note that the quality of the
results will vary depending on how well the categories shake
and random represent each input line. It is sometimes useful
to see the posterior prob&acirc; <br>
abilities for each line without filtering:</p>

<p style="margin-top: 1em">% dbacl -c shake -f shake -RN
document.txt &gt; document.txt_probs</p>

<p style="margin-top: 1em">You can now postprocess the
posterior probabilities for each line of text with another
script, to replicate an arbitrary Bayesian decision rule of
your choice.</p>

<p style="margin-top: 1em">In the special case of exactly
two categories, the optimal Bayesian decision procedure can
be implemented for documents as follows: let p1 be the prior
probability that the input <br>
text is classified as category1. Consequently, the prior
probability of classifying as category2 is 1 - p1. Let u12
be the cost of misclassifying a category1 input text as <br>
belonging to category2 and vice versa for u21. We assume
there is no cost for classifying correctly. Then the
following command implements the optimal Bayesian
decision:</p>

<p style="margin-top: 1em">% dbacl -n -c category1 -c
category2 | awk &rsquo;{ if($2 * p1 * u12 &gt; $4 * (1 - p1)
* u21) { print $1; } else { print $3; } }&rsquo;</p>

<p style="margin-top: 1em">dbacl can also be used in
conjunction with procmail(1) to implement a simple Bayesian
email classification system. Assume that incoming mail
should be automatically delivered to <br>
one of three mail folders located in $MAILDIR and named
work, personal, and spam. Initially, these must be created
and filled with appropriate sample emails. A crontab(1) file
<br>
can be used to learn the three categories once a day,
e.g.</p>

<p style="margin-top: 1em">CATS=$HOME/.dbacl <br>
5 0 * * * dbacl -T email -l $CATS/work $MAILDIR/work <br>
10 0 * * * dbacl -T email -l $CATS/personal
$MAILDIR/personal <br>
15 0 * * * dbacl -T email -l $CATS/spam $MAILDIR/spam</p>

<p style="margin-top: 1em">To automatically deliver each
incoming email into the appropriate folder, the following
procmailrc(5) recipe fragment could be used:</p>

<p style="margin-top: 1em">CATS=$HOME/.dbacl</p>

<p style="margin-top: 1em"># run the spam classifier <br>
:0 c <br>
YAY=| dbacl -vT email -c $CATS/work -c $CATS/personal -c
$CATS/spam</p>

<p style="margin-top: 1em"># send to the appropriate
mailbox <br>
:0: <br>
* ? test -n &quot;$YAY&quot; <br>
$MAILDIR/$YAY</p>

<p style="margin-top: 1em">:0: <br>
$DEFAULT</p>

<p style="margin-top: 1em">Sometimes, dbacl will send the
email to the wrong mailbox. In that case, the misclassified
message should be removed from its wrong destination and
placed in the correct mailbox. <br>
The error will be corrected the next time your messages are
learned. If it is left in the wrong category, dbacl will
learn the wrong corpus statistics.</p>

<p style="margin-top: 1em">The default text features
(tokens) read by dbacl are purely alphabetic strings, which
minimizes memory requirements but can be unrealistic in some
cases. To construct models <br>
based on alphanumeric tokens, use the -e switch. The example
below also uses the optional -D switch, which prints a list
of actual tokens found in the document:</p>

<p style="margin-top: 1em">% dbacl -e alnum -D -l twain
Mark_Twain.txt | less</p>

<p style="margin-top: 1em">It is also possible to override
the default feature selection method used to learn the
category model by means of regular expressions. For example,
the following duplicates the <br>
default feature selection method in the C locale, while
being much slower:</p>

<p style="margin-top: 1em">% dbacl -l twain -g
&rsquo;^([[:alpha:]]+)&rsquo; -g
&rsquo;[^[:alpha:]]([[:alpha:]]+)&rsquo; Mark_Twain.txt</p>

<p style="margin-top: 1em">The category twain which is
obtained depends only on single alphabetic words in the text
file Mark_Twain.txt (and computed digram statistics for
prediction). For a second exam&acirc; <br>
ple, the following command builds a smoothed Markovian (word
bigram) model which depends on pairs of consecutive words
within each line (but pairs cannot straddle a line
break):</p>

<p style="margin-top: 1em">% dbacl -l twain2 -g
&rsquo;(^|[^[:alpha:]])([[:alpha:]]+)||2&rsquo; -g
&rsquo;(^|[^[:alpha:]])([[:alpha:]]+)[^[:alpha:]]+([[:alpha:]]+)||23&rsquo;
Mark_Twain.txt</p>

<p style="margin-top: 1em">More general, line based, n-gram
models of all orders (up to 7) can be built in a similar
way. To construct paragraph based models, you should
reformat the input corpora with <br>
awk(1) or sed(1) to obtain one paragraph per line. Line size
is limited by available memory, but note that regex
performance will degrade quickly for long lines.</p>

<p style="margin-top: 1em">PERFORMANCE <br>
The underlying assumption of statistical learning is that a
relatively small number of training documents can represent
a much larger set of input documents. Thus in the long <br>
run, learning can grind to a halt without serious impact on
classification accuracy. While not true in reality, this
assumption is surprisingly accurate for problems such as
<br>
email filtering. In practice, this means that a well chosen
corpus on the order of ten thousand documents is sufficient
for highly accurate results for years. Continual
learn&acirc; <br>
ing after such a critical mass results in diminishing
returns. Of course, when real world input document patterns
change dramatically, the predictive power of the models can
be <br>
lost. At the other end, a few hundred documents already give
acceptable results in most cases.</p>

<p style="margin-top: 1em">dbacl is heavily optimized for
the case of frequent classifications but infrequent batch
learning. This is the long run optimum described above.
Under ideal conditions, dbacl can <br>
classify a hundred emails per second on low end hardware
(500Mhz Pentium III). Learning speed is not very much
slower, but takes effectively much longer for large document
col&acirc; <br>
lections for various reasons. When using the -m switch, data
structures are aggressively mapped into memory if possible,
reducing overheads for both I/O and memory allocations.</p>

<p style="margin-top: 1em">dbacl throws away its input as
soon as possible, and has no limits on the input document
size. Both classification and learning speed are directly
proportional to the number of <br>
tokens in the input, but learning also needs a nonlinear
optimization step which takes time proportional to the
number of unique tokens discovered. At time of writing,
dbacl is <br>
one of the fastest open source mail filters given its
optimal usage scenario, but uses more memory for learning
than other filters.</p>

<p style="margin-top: 1em">MULTIPLE PROCESSES AND DATA
CORRUPTION <br>
When saving category files, dbacl first writes out a
temporary file in the same location, and renames it
afterwards. If a problem or crash occurs during learning,
the old cate&acirc; <br>
gory file is therefore left untouched. This ensures that
categories can never be corrupted, no matter how many
processes try to simultaneously learn or classify, and means
that <br>
valid categories are available for classification at any
time.</p>

<p style="margin-top: 1em">When using the -m switch, file
contents are memory mapped for speedy reading and writing.
This, together with the -o switch, is intended mainly for
testing purposes, when tens of <br>
thousands of messages must be learned and scored in a
laboratory to measure dbacl&rsquo;s accuracy. Because no
file locking is attempted for performance reasons,
corruptions are possi&acirc; <br>
ble, unless you make sure that only one dbacl process reads
or writes any file at any given time. This is the only case
(-m and -o together) when corruption is possible.</p>

<p style="margin-top: 1em">MEMORY USE <br>
When classifying a document, dbacl loads all indicated
categories into RAM, so the total memory needed is
approximately the sum of the category file sizes plus a
fixed small <br>
overhead. The input document is consumed while being read,
so its size doesn&rsquo;t matter, but very long lines can
take up space. When using the -m switch, the categories are
read <br>
using mmap(2) as available.</p>

<p style="margin-top: 1em">When learning, dbacl keeps a
large structure in memory which contains many objects which
won&rsquo;t be saved into the output category. The size of
this structure is proportional to <br>
the number of unique tokens read, but not the size of the
input documents, since they are discarded while being read.
As a rough guide, this structure is 4x-5x the size of the
<br>
final category file that is produced.</p>

<p style="margin-top: 1em">To prevent unchecked memory
growth, dbacl allocates by default a fixed smallish amount
of memory for tokens. When this space is used up, further
tokens are discarded which has <br>
the effect of skewing the learned category making it less
usable as more tokens are dropped. A warning is printed on
STDERR in such a case.</p>

<p style="margin-top: 1em">The -h switch lets you fix the
initial size of the token space in powers of 2, ie &quot;-h
17&quot; means 2^17 = 131072 possible tokens. If you type
&quot;dbacl -V&quot;, you can see the number of <br>
bytes needed for each token when either learning or
classifying. Multiply this number by the maximum number of
possible tokens to estimate the memory needed for learning.
The -H <br>
switch lets dbacl grow its tables automatically if and when
needed, up to a maximum specified. So if you type &quot;-H
21&quot;, then the initial size will be doubled repeatedly
if neces&acirc; <br>
sary, up to approximately two million unique tokens.</p>

<p style="margin-top: 1em">When learning with the -X
switch, a handful of input documents are also kept in RAM
throughout.</p>

<p style="margin-top: 1em">ENVIRONMENT <br>
DBACL_PATH <br>
When this variable is set, its value is prepended to every
category filename which doesn&rsquo;t start with a
&rsquo;/&rsquo; or a &rsquo;.&rsquo;.</p>

<p style="margin-top: 1em">SIGNALS <br>
INT If this signal is caught, dbacl simply exits without
doing any cleanup or other operations. This signal can often
be sent by pressing Ctrl-C on the keyboard. See stty(1).</p>

<p style="margin-top: 1em">HUP, QUIT, TERM <br>
If one of these signals is caught, dbacl stops reading input
and continues its operation as if no more input was
available. This is a way of quitting gracefully, but note
<br>
that in learning mode, a category file will be written based
on the incomplete input. The QUIT signal can often be sent
by pressing Ctrl- on the keyboard. See stty(1).</p>

<p style="margin-top: 1em">USR1 If this signal is caught,
dbacl reloads the current categories at the earliest
feasible opportunity. This is not normally useful at all,
but might be in special cases, <br>
such as if the -f switch is invoked together with input from
a long running pipe.</p>

<p style="margin-top: 1em">NOTES <br>
dbacl generated category files are in binary format, and may
or may not be portable to systems using a different byte
order architecture (this depends on how dbacl was compiled).
<br>
The -V switch prints out whether categories are portable, or
else you can just experiment.</p>

<p style="margin-top: 1em">dbacl does not recognize
functionally equivalent regular expressions, and in this
case duplicate features will be counted several times.</p>

<p style="margin-top: 1em">With every learned category, the
command line options that were used are saved. When
classifying, make sure that every relevant category was
learned with the same set of options <br>
(regexes are allowed to differ), otherwise behaviour is
undefined. There is no need to repeat all the switches when
classifying.</p>

<p style="margin-top: 1em">If you get many digitization
warnings, then you are trying to learn too much data at
once, or your model is too complex. dbacl is compiled to
save memory by digitizing final <br>
weights, but you can disable digitization by editing dbacl.h
and recompiling.</p>

<p style="margin-top: 1em">dbacl offers several built-in
tokenizers (see -e switch) with more to come in future
versions, as the author invents them. While the default
tokenizer may evolve, no tokenizer <br>
should ever be removed, so that you can always simulate
previous dbacl behaviour subject to bug fixes and
architectural changes.</p>

<p style="margin-top: 1em">The confidence estimates
obtained through the -X switch are underestimates, ie are
more conservative than they should be.</p>

<p style="margin-top: 1em">BUGS <br>
&quot;Ya know, some day scientists are gonna invent
something that will outsmart a rabbit.&quot; (Robot Rabbit,
1953)</p>

<p style="margin-top: 1em">SOURCE <br>
The source code for the latest version of this program is
available at the following locations:</p>

<p style="margin-top: 1em">http://www.lbreyer.com/gpl.html
<br>
http://dbacl.sourceforge.net</p>

<p style="margin-top: 1em">AUTHOR <br>
Laird A. Breyer &lt;laird@lbreyer.com&gt;</p>

<p style="margin-top: 1em">SEE ALSO <br>
awk(1), bayesol(1), crontab(1), hmine(1), hypex(1), less(1),
mailcross(1), mailfoot(1), mailinspect(1), mailtoe(1),
procmailex(5), regex(7), stty(1), sed(1)</p>

<p style="margin-top: 1em">Version 1.12 Bayesian Text
Classification Tools DBACL(1)</p>
<hr>
</body>
</html>
