<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 16:38:04 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>SKIPFISH(1) General Commands Manual SKIPFISH(1)</p>

<p style="margin-top: 1em">NAME <br>
skipfish - web application security scanner</p>

<p style="margin-top: 1em">SYNOPSIS <br>
skipfish [options] -o output-directory [ start-url |
@url-file [ start-url2 ... ]]</p>

<p style="margin-top: 1em">DESCRIPTION <br>
skipfish is an active web application security
reconnaissance tool. It prepares an interactive sitemap for
the targeted site by carrying out a recursive crawl and
dictionary- <br>
based probes. The resulting map is then annotated with the
output from a number of active (but hopefully
non-disruptive) security checks. The final report generated
by the tool <br>
is meant to serve as a foundation for professional web
application security assessments.</p>

<p style="margin-top: 1em">OPTIONS SUMMARY <br>
Authentication and access options: <br>
-A user:pass - use specified HTTP authentication credentials
<br>
-F host=IP - pretend that &Acirc;&acute;host&Acirc;&acute;
resolves to &Acirc;&acute;IP&Acirc;&acute; <br>
-C name=val - append a custom cookie to all requests <br>
-H name=val - append a custom HTTP header to all requests
<br>
-b (i|f|p) - use headers consistent with MSIE / Firefox /
iPhone <br>
-N - do not accept any new cookies</p>

<p style="margin-top: 1em">Crawl scope options: <br>
-d max_depth - maximum crawl tree depth (16) <br>
-c max_child - maximum children to index per node (512) <br>
-x max_desc - maximum descendants to index per branch (8192)
<br>
-r r_limit - max total number of requests to send
(100000000) <br>
-p crawl% - node and link crawl probability (100%) <br>
-q hex - repeat probabilistic scan with given seed <br>
-I string - only follow URLs matching
&Acirc;&acute;string&Acirc;&acute; <br>
-X string - exclude URLs matching
&Acirc;&acute;string&Acirc;&acute; <br>
-K string - do not fuzz parameters named
&Acirc;&acute;string&Acirc;&acute; <br>
-D domain - crawl cross-site links to another domain <br>
-B domain - trust, but do not crawl, another domain <br>
-Z - do not descend into 5xx locations <br>
-O - do not submit any forms <br>
-P - do not parse HTML, etc, to find new links</p>

<p style="margin-top: 1em">Reporting options: <br>
-o dir - write output to specified directory (required) <br>
-M - log warnings about mixed content / non-SSL passwords
<br>
-E - log all caching intent mismatches <br>
-U - log all external URLs and e-mails seen <br>
-Q - completely suppress duplicate nodes in reports <br>
-u - be quiet, disable realtime progress stats</p>

<p style="margin-top: 1em">Dictionary management options:
<br>
-W wordlist - use a specified read-write wordlist (required)
<br>
-S wordlist - load a supplemental read-only wordlist <br>
-L - do not auto-learn new keywords for the site <br>
-Y - do not fuzz extensions in directory brute-force <br>
-R age - purge words hit more than
&Acirc;&acute;age&Acirc;&acute; scans ago <br>
-T name=val - add new form auto-fill rule <br>
-G max_guess - maximum number of keyword guesses to keep
(256)</p>

<p style="margin-top: 1em">Performance settings: <br>
-l max_req - max requests per second (0..000000) <br>
-g max_conn - max simultaneous TCP connections, global (40)
<br>
-m host_conn - max simultaneous connections, per target IP
(10) <br>
-f max_fail - max number of consecutive HTTP errors (100)
<br>
-t req_tmout - total request response timeout (20 s) <br>
-w rw_tmout - individual network I/O timeout (10 s) <br>
-i idle_tmout - timeout on idle HTTP connections (10 s) <br>
-s s_limit - response size limit (200000 B) <br>
-e - do not keep binary responses for reporting</p>

<p style="margin-top: 1em">Other settings: <br>
-k duration - stop scanning after the given duration h:m:s
<br>
--config file - load specified configuration file</p>

<p style="margin-top: 1em">AUTHENTICATION AND ACCESS <br>
Some sites require authentication, and skipfish supports
this in different ways. First there is basic HTTP
authentication, for which you can use the -A flag. Second,
and more <br>
common, are sites that require authentication on a web
application level. For these sites, the best approach is to
capture authenticated session cookies and provide them to
skip&acirc; <br>
fish using the -C flag (multiple if needed). Last,
you&rsquo;ll need to put some effort in protecting the
session from being destroyed by excluding logout links with
-X and/or by <br>
rejecting new cookies with -N.</p>

<p style="margin-top: 1em">-F/--host &lt;ip:hostname&gt;
<br>
Using this flag, you can set the
&Acirc;&acute;Host:&Acirc;&acute; header value to define a
custom mapping between a host and an IP (bypassing the
resolver). This feature is particularly useful for <br>
not-yet-launched or legacy services that don&rsquo;t have
the necessary DNS entries.</p>

<p style="margin-top: 1em">-H/--header &lt;header:value&gt;
<br>
When it comes to customizing your HTTP requests, you can
also use the -H option to insert any additional,
non-standard headers. This flag also allows the default
headers <br>
to be overwritten.</p>

<p style="margin-top: 1em">-C/--cookie &lt;cookie:value&gt;
<br>
This flag can be used to add a cookie to the skipfish HTTP
requests; This is particularly useful to perform
authenticated scans by providing session cookies. When doing
<br>
so, keep in mind that cetain URLs (e.g. /logout) may destroy
your session; you can combat this in two ways: by using the
-N option, which causes the scanner to reject <br>
attempts to set or delete cookies; or by using the -X option
to exclude logout URLs.</p>

<p style="margin-top: 1em">-b/--user-agent &lt;i|f|p&gt;
<br>
This flag allows the user-agent to be specified where
&Acirc;&acute;i&Acirc;&acute; stands for Internet Explorer,
&Acirc;&acute;f&Acirc;&acute; for Firefox and
&Acirc;&acute;p&Acirc;&acute; for iPhone. Using this flag is
recommended in case the tar&acirc; <br>
get site shows different behavior based on the user-agent
(e.g some sites use different templates for mobiles and
desktop clients).</p>

<p style="margin-top: 1em">-N/--reject-cookies <br>
This flag causes skipfish to ignore cookies that are being
set by the site. This helps to enforce stateless tests and
also prevent that cookies set with
&Acirc;&acute;-C&Acirc;&acute; are not over&acirc; <br>
written.</p>

<p style="margin-top: 1em">-A/--auth
&lt;username:password&gt; <br>
For sites requiring basic HTTP authentication, you can use
this flag to specify your credentials.</p>

<p style="margin-top: 1em">--auth-form &lt;URL&gt; <br>
The login form to use with form authentication. By default
skipfish will use the form&rsquo;s action URL to submit the
credentials. If this is missing than the login data is <br>
send to the form URL. In case that is wrong, you can set the
form handler URL with --auth-form-target &lt;URL&gt; .</p>

<p style="margin-top: 1em">--auth-user &lt;username&gt;
<br>
The username to be used during form authentication. Skipfish
will try to detect the correct form field to use but if it
fails to do so (and gives an error), then you can <br>
specify the form field name with --auth-user-field.</p>

<p style="margin-top: 1em">--auth-pass &lt;password&gt;
<br>
The password to be used during form authentication. Similar
to auth-user, the form field name can (optionally) be set
with --auth-pass-field.</p>

<p style="margin-top: 1em">--auth-verify-url &lt;URL&gt;
<br>
This URL allows skipfish to verify whether authentication
was successful. This requires a URL where anonymous and
authenticated requests are answered with a different <br>
response.</p>

<p style="margin-top: 1em">CRAWLING SCOPE <br>
Some sites may be too big to scan in a reasonable timeframe.
If the site features well-defined tarpits - for example,
100,000 nearly identical user profiles as a part of a social
<br>
network - these specific locations can be excluded with -X
or -S. In other cases, you may need to resort to other
settings: -d limits crawl depth to a specified number of
subdi&acirc; <br>
rectories; -c limits the number of children per directory;
-x limits the total number of descendants per crawl tree
branch; and -r limits the total number of requests to send
in <br>
a scan.</p>

<p style="margin-top: 1em">-d/--max-crawl-depth
&lt;depth&gt; <br>
Limit the depth of subdirectories being crawled (see
above).</p>

<p style="margin-top: 1em">-c/--max-crawl-child
&lt;childs&gt; <br>
Limit the amount of subdirectories per directory we crawl
into (see above).</p>

<p style="margin-top: 1em">-x/--max-crawl-descendants
&lt;descendants&gt; <br>
Limit the total number of descendants per crawl tree branch
(see above).</p>

<p style="margin-top: 1em">-r/--max-request-total
&lt;request&gt; <br>
The maximum number of requests can be limited with this
flag.</p>

<p style="margin-top: 1em">-p/--crawl-probability
&lt;0-100&gt; <br>
By specifying a percentage between 1 and 100%, it is
possible to tell the crawler to follow fewer than 100% of
all links, and try fewer than 100% of all dictionary <br>
entries. This - naturally - limits the completeness of a
scan, but unlike most other settings, it does so in a
balanced, non-deterministic manner. It is extremely useful
<br>
when you are setting up time-bound, but periodic assessments
of your infrastructure.</p>

<p style="margin-top: 1em">-q/--seed &lt;seed&gt; <br>
This flag sets the initial random seed for the crawler to a
specified value. This can be used to exactly reproduce a
previous scan to compare results. Randomness is <br>
relied upon most heavily in the -p mode, but also influences
a couple of other scan management decisions.</p>

<p style="margin-top: 1em">-I/--include-string
&lt;domain/path&gt; <br>
With this flag, you can tell skipfish to only crawl and test
URLs that match a certain string. This can help to narrow
down the scope of a scan by only whitelisting cer&acirc;
<br>
tain sections of a web site (e.g. -I /shop).</p>

<p style="margin-top: 1em">-X/--exclude-string
&lt;domain/path&gt; <br>
The -X option can be used to exclude files / directories
from the scan. This is useful to avoid session termination
(i.e. by excluding /logout) or just for speeding up <br>
your scans by excluding static content directories like
/icons/, /doc/, /manuals/, and other standard, mundane
locations along these lines.</p>

<p style="margin-top: 1em">-K/--skip-parameter
&lt;parameter name&gt; <br>
This flag allows you to specify parameter names not to fuzz.
(useful for applications that put session IDs in the URL, to
minimize noise).</p>

<p style="margin-top: 1em">-D/--include-domain
&lt;domain&gt; <br>
Allows you to specify additional hosts or domains to be
in-scope for the test. By default, all hosts appearing in
the command-line URLs are added to the list - but you can
<br>
use -D to broaden these rules. The result of this will be
that the crawler will follow links and tests links that
point to these additional hosts.</p>

<p style="margin-top: 1em">-B/--trust-domain &lt;domain&gt;
<br>
In some cases, you do not want to actually crawl a
third-party domain, but you trust the owner of that domain
enough not to worry about cross-domain content inclusion
from <br>
that location. To suppress warnings, you can use the -B
option</p>

<p style="margin-top: 1em">-Z/--skip-error-pages <br>
Do not crawl into pages / directories that give an error
5XX.</p>

<p style="margin-top: 1em">-O/--no-form-submits <br>
Using this flag will cause forms to be ignored during the
scan.</p>

<p style="margin-top: 1em">-P/--no-html-parsing <br>
This flag will disable link extracting and effectively
disables crawling. Using -P is useful when you want to test
one specific URL or when you want to feed skipfish a <br>
list of URLs that were collected with an external
crawler.</p>

<p style="margin-top: 1em">TESTING SCOPE <br>
--checks <br>
EXPERIMENTAL: Displays the crawler injection tests. The
output shows the index number (useful for --checks-toggle),
the check name and whether the check is enabled.</p>

<p style="margin-top: 1em">--checks-toggle
&lt;check1,check2,..&gt; <br>
EXPERIMENTAL: Every injection test can be enabled/disabled
with using this flag. As value, you need to provide the
check numbers which can be obtained with the --checks <br>
flag. Multiple checks can be toggled via a comma separated
value (i.e. --checks-toggle 1,2 )</p>

<p style="margin-top: 1em">--no-injection-tests <br>
EXPERIMENTAL: Disables all injection tests for this scan and
limits the scan to crawling and, optionally, bruteforcing.
As with all scans, the output directory will con&acirc; <br>
tain a pivots.txt file. This file can be used to feed future
scans.</p>

<p style="margin-top: 1em">REPORTING OPTIONS <br>
-o/--output &lt;dir&gt; <br>
The report wil be written to this location. The directory is
one of the two mandatory options and must not exist upon
starting the scan.</p>

<p style="margin-top: 1em">-M/--log-mixed-content <br>
Enable the logging of mixed content. This is highly
recommended when scanning SSL-only sites to detect insecure
content inclusion via non-SSL protected links.</p>

<p style="margin-top: 1em">-E/--log-cache-mismatches <br>
This will cause additonal content caching error to be
reported.</p>

<p style="margin-top: 1em">-U/--log-external-urls <br>
Log all external URLs and email addresses that were seen
during the scan.</p>

<p style="margin-top: 1em">-Q/--log-unique-nodes <br>
Enable this to completely suppress duplicate nodes in
reports.</p>

<p style="margin-top: 1em">-u/--quiet <br>
This will cause skipfish to suppress all console output
during the scan.</p>

<p style="margin-top: 1em">-v/--verbose <br>
EXPERIMENTAL: Use this flag to enable runtime reporting of,
for example, problems that are detected. Can be used
multiple times to increase verbosity and should be used in
<br>
combination with -u unless you run skipfish with stderr
redirected to a file.</p>

<p style="margin-top: 1em">DICTIONARY MANAGEMENT <br>
Make sure you&rsquo;ve read the instructions provided in
doc/dictionaries.txt to select the right dictionary file and
configure it correctly. This step has a profound impact on
the <br>
quality of scan results later on.</p>

<p style="margin-top: 1em">-S/--wordlist &lt;file&gt; <br>
Load the specified (read-only) wordlist for use during the
scan. This flag is optional but use of a dictionary is
highly recommended when performing a blackbox scan as it
<br>
will highlight hidden files and directories.</p>

<p style="margin-top: 1em">-W/--rw-wordlist &lt;file&gt;
<br>
Specify an initially empty file for any newly learned
site-specific keywords (which will come handy in future
assessments). You can use -W- or -W /dev/null if you
don&rsquo;t <br>
want to store auto-learned keywords anywhere. Typically you
will want to use one of the packaged dictonaries (i.e.
complete.wl) and possibly add a custom dictionary.</p>

<p style="margin-top: 1em">-L/--no-keyword-learning <br>
During the scan, skipfish will try to learn and use new
keywords. This flag disables that behavior and should be
used when any form of brute-forcing is not desired.</p>

<p style="margin-top: 1em">-Y/--no-extension-brute <br>
This flag will disable extension guessing during directory
bruteforcing.</p>

<p style="margin-top: 1em">-R &lt;age&gt; <br>
Use of this flag allows old words to be purged from
wordlists. It is intended to help keeping dictionaries clean
when used in recurring scans.</p>

<p style="margin-top: 1em">-T/--form-value
&lt;name=value&gt; <br>
Skipfish also features a form auto-completion mechanism in
order to maximize scan coverage. The values should be
non-malicious, as they are not meant to implement security
<br>
checks - but rather, to get past input validation logic. You
can define additional rules, or override existing ones, with
the -T option (-T form_field_name=field_value, <br>
e.g. -T login=test123 -T password=test321 - although note
that -C and -A are a much better method of logging in).</p>

<p style="margin-top: 1em">-G &lt;max guesses&gt; <br>
During the scan, a temporary buffer of newly detected
keywords is maintained. The size of this buffer can be
changed with this flag and doing so influences
bruteforcing.</p>

<p style="margin-top: 1em">PERFORMANCE OPTIONS <br>
The default performance setting should be fine for most
servers but when the report indicates there were connection
problems, you might want to tweak some of the values here.
For <br>
unstable servers, the scan coverage is likely to improve
when using low values for rate and connection flags.</p>

<p style="margin-top: 1em">-l/--max-request-rate
&lt;rate&gt; <br>
This flag can be used to limit the amount of requests per
second. This is very useful when the target server
can&rsquo;t keep up with the high amount of requests that
are gener&acirc; <br>
ated by skipfish. Keeping the amount requests per second low
can also help preventing some rate-based DoS protection
mechanisms from kicking in and ruining the scan.</p>

<p style="margin-top: 1em">-g/--max-connections
&lt;number&gt; <br>
The max simultaneous TCP connections (global) can be set
with this flag.</p>

<p style="margin-top: 1em">-m/--max-host-connections
&lt;number&gt; <br>
The max simultaneous TCP connections, per target IP, can be
set with this flag.</p>

<p style="margin-top: 1em">-f/--max-failed-requests
&lt;number&gt; <br>
Controls the maximum number of consecutive HTTP errors you
are willing to see before aborting the scan. For large
scans, you probably want to set a higher value here.</p>

<p style="margin-top: 1em">-t/--request-timeout
&lt;timeout&gt; <br>
Set the total request timeout, to account for really slow or
really fast sites.</p>

<p style="margin-top: 1em">-w/--network-timeout
&lt;timeout&gt; <br>
Set the network I/O timeout.</p>

<p style="margin-top: 1em">-i/--idle-timeout
&lt;timeout&gt; <br>
Specify the timeout for idle HTTP connections.</p>

<p style="margin-top: 1em">-s/--response-size &lt;size&gt;
<br>
Sets the maximum length of a response to fetch and parse
(longer responses will be truncated).</p>

<p style="margin-top: 1em">-e/--discard-binary <br>
This prevents binary documents from being kept in memory for
reporting purposes, and frees up a lot of RAM.</p>

<p style="margin-top: 1em">--flush-to-disk <br>
This causes request / response data to be flushed to disk
instead of being kept in memory. As a result, the memory
usage for large scans will be significant lower.</p>

<p style="margin-top: 1em">EXAMPLES <br>
Scan type: config <br>
skipfish --config config/example.conf http://example.com</p>

<p style="margin-top: 1em">Scan type: quick <br>
skipfish -o output/dir/ http://example.com</p>

<p style="margin-top: 1em">Scan type: extensive bruteforce
<br>
skipfish [...other options..] -S dictionaries/complete.wl
http://example.com</p>

<p style="margin-top: 1em">Scan type: without bruteforcing
<br>
skipfish [...other options..] -LY http://example.com</p>

<p style="margin-top: 1em">Scan type: authenticated (basic)
<br>
skipfish [...other options..] -A username:password
http://example.com</p>

<p style="margin-top: 1em">Scan type: authenticated
(cookie) <br>
skipfish [...other options..] -C jsession=myauthcookiehere
-X /logout http://example.com</p>

<p style="margin-top: 1em">Scan type: flaky server <br>
skipfish [...other options..] -l 5 -g 2 -t 30 -i 15
http://example.com</p>

<p style="margin-top: 1em">NOTES <br>
The default values for all flags can be viewed by running
&Acirc;&acute;./skipfish -h&Acirc;&acute; .</p>

<p style="margin-top: 1em">AUTHOR <br>
skipfish was written by Michal Zalewski
&lt;lcamtuf@google.com&gt;, with contributions from Niels
Heinen &lt;heinenn@google.com&gt;, Sebastian Roschke
&lt;s.roschke@googlemail.com&gt;, and other <br>
parties.</p>

<p style="margin-top: 1em">This manual page was written
with the help of Thorsten Schifferdecker
&lt;tsd@debian.systs.org&gt;.</p>

<p style="margin-top: 1em">May 6, 2012 SKIPFISH(1)</p>
<hr>
</body>
</html>
