<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8"/>
  <style>
    table.head, table.foot { width: 100%; }
    td.head-rtitle, td.foot-os { text-align: right; }
    td.head-vol { text-align: center; }
    div.Pp { margin: 1ex 0ex; }
  </style>
  <title>SKIPFISH(1)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">SKIPFISH(1)</td>
    <td class="head-vol">General Commands Manual</td>
    <td class="head-rtitle">SKIPFISH(1)</td>
  </tr>
</table>
<div class="manual-text">
<h1 class="Sh" title="Sh" id="NAME"><a class="selflink" href="#NAME">NAME</a></h1>
skipfish - web application security scanner
<h1 class="Sh" title="Sh" id="SYNOPSIS"><a class="selflink" href="#SYNOPSIS">SYNOPSIS</a></h1>
<b>skipfish</b> [<i>options</i>]<i> -o output-directory [ start-url | @url-file
  [ start-url2 ... ]]</i>
<div>&#x00A0;</div>
<h1 class="Sh" title="Sh" id="DESCRIPTION"><a class="selflink" href="#DESCRIPTION">DESCRIPTION</a></h1>
<b>skipfish</b> is an active web application security reconnaissance tool. It
  prepares an interactive sitemap for the targeted site by carrying out a
  recursive crawl and dictionary-based probes. The resulting map is then
  annotated with the output from a number of active (but hopefully
  non-disruptive) security checks. The final report generated by the tool is
  meant to serve as a foundation for professional web application security
  assessments.
<h1 class="Sh" title="Sh" id="OPTIONS_SUMMARY"><a class="selflink" href="#OPTIONS_SUMMARY">OPTIONS
  SUMMARY</a></h1>
<div style="margin-left: 4.00ex;">
<pre>
Authentication and access options:
  -A user:pass   - use specified HTTP authentication credentials
  -F host=IP     - pretend that &#x00B4;host&#x00B4; resolves to &#x00B4;IP&#x00B4;
  -C name=val    - append a custom cookie to all requests
  -H name=val    - append a custom HTTP header to all requests
  -b (i|f|p)     - use headers consistent with MSIE / Firefox / iPhone
  -N             - do not accept any new cookies
<div class="Pp"></div>
Crawl scope options:
  -d max_depth   - maximum crawl tree depth (16)
  -c max_child   - maximum children to index per node (512)
  -x max_desc    - maximum descendants to index per branch (8192)
  -r r_limit     - max total number of requests to send (100000000)
  -p crawl%      - node and link crawl probability (100%)
  -q hex         - repeat probabilistic scan with given seed
  -I string      - only follow URLs matching &#x00B4;string&#x00B4;
  -X string      - exclude URLs matching &#x00B4;string&#x00B4;
  -K string      - do not fuzz parameters named &#x00B4;string&#x00B4;
  -D domain      - crawl cross-site links to another domain
  -B domain      - trust, but do not crawl, another domain
  -Z             - do not descend into 5xx locations
  -O             - do not submit any forms
  -P             - do not parse HTML, etc, to find new links
<div class="Pp"></div>
Reporting options:
  -o dir         - write output to specified directory (required)
  -M             - log warnings about mixed content / non-SSL passwords
  -E             - log all caching intent mismatches
  -U             - log all external URLs and e-mails seen
  -Q             - completely suppress duplicate nodes in reports
  -u             - be quiet, disable realtime progress stats
<div class="Pp"></div>
Dictionary management options:
  -W wordlist    - use a specified read-write wordlist (required)
  -S wordlist    - load a supplemental read-only wordlist
  -L             - do not auto-learn new keywords for the site
  -Y             - do not fuzz extensions in directory brute-force
  -R age         - purge words hit more than &#x00B4;age&#x00B4; scans ago
  -T name=val    - add new form auto-fill rule
  -G max_guess   - maximum number of keyword guesses to keep (256)
<div class="Pp"></div>
Performance settings:
  -l max_req     - max requests per second (0..000000)
  -g max_conn    - max simultaneous TCP connections, global (40)
  -m host_conn   - max simultaneous connections, per target IP (10)
  -f max_fail    - max number of consecutive HTTP errors (100)
  -t req_tmout   - total request response timeout (20 s)
  -w rw_tmout    - individual network I/O timeout (10 s)
  -i idle_tmout  - timeout on idle HTTP connections (10 s)
  -s s_limit     - response size limit (200000 B)
  -e             - do not keep binary responses for reporting
<div class="Pp"></div>
Other settings:
  -k duration    - stop scanning after the given duration h:m:s
  --config file  - load specified configuration file
<div class="Pp"></div>
</pre>
</div>
<h1 class="Sh" title="Sh" id="AUTHENTICATION_AND_ACCESS"><a class="selflink" href="#AUTHENTICATION_AND_ACCESS">AUTHENTICATION
  AND ACCESS</a></h1>
Some sites require authentication, and skipfish supports this in different ways.
  First there is basic HTTP authentication, for which you can use the -A flag.
  Second, and more common, are sites that require authentication on a web
  application level. For these sites, the best approach is to capture
  authenticated session cookies and provide them to skipfish using the -C flag
  (multiple if needed). Last, you'll need to put some effort in protecting the
  session from being destroyed by excluding logout links with -X and/or by
  rejecting new cookies with -N.
<div style="height: 1.00em;">&#x00A0;</div>
<dl class="Bl-tag">
  <dt class="It-tag">-F/--host &lt;ip:hostname&gt;</dt>
  <dd class="It-tag">Using this flag, you can set the
      &#x00B4;<i>Host:</i>&#x00B4; header value to define a custom mapping
      between a host and an IP (bypassing the resolver). This feature is
      particularly useful for not-yet-launched or legacy services that don't
      have the necessary DNS entries.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-H/--header &lt;header:value&gt;</dt>
  <dd class="It-tag">When it comes to customizing your HTTP requests, you can
      also use the -H option to insert any additional, non-standard headers.
      This flag also allows the default headers to be overwritten.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-C/--cookie &lt;cookie:value&gt;</dt>
  <dd class="It-tag">This flag can be used to add a cookie to the skipfish HTTP
      requests; This is particularly useful to perform authenticated scans by
      providing session cookies. When doing so, keep in mind that cetain URLs
      (e.g. /logout) may destroy your session; you can combat this in two ways:
      by using the -N option, which causes the scanner to reject attempts to set
      or delete cookies; or by using the -X option to exclude logout URLs.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-b/--user-agent &lt;i|f|p&gt;</dt>
  <dd class="It-tag">This flag allows the user-agent to be specified where
      &#x00B4; <i>i</i>&#x00B4; stands for Internet Explorer,
      &#x00B4;<i>f</i>&#x00B4; for Firefox and &#x00B4; <i>p</i>&#x00B4; for
      iPhone. Using this flag is recommended in case the target site shows
      different behavior based on the user-agent (e.g some sites use different
      templates for mobiles and desktop clients).
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-N/--reject-cookies</dt>
  <dd class="It-tag">This flag causes skipfish to ignore cookies that are being
      set by the site. This helps to enforce stateless tests and also prevent
      that cookies set with &#x00B4;-C&#x00B4; are not overwritten.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-A/--auth &lt;username:password&gt;</dt>
  <dd class="It-tag">For sites requiring basic HTTP authentication, you can use
      this flag to specify your credentials.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--auth-form &lt;URL&gt;</dt>
  <dd class="It-tag">The login form to use with form authentication. By default
      skipfish will use the form's action URL to submit the credentials. If this
      is missing than the login data is send to the form URL. In case that is
      wrong, you can set the form handler URL with --auth-form-target
      &lt;URL&gt; .
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--auth-user &lt;username&gt;</dt>
  <dd class="It-tag">The username to be used during form authentication.
      Skipfish will try to detect the correct form field to use but if it fails
      to do so (and gives an error), then you can specify the form field name
      with --auth-user-field.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--auth-pass &lt;password&gt;</dt>
  <dd class="It-tag">The password to be used during form authentication. Similar
      to auth-user, the form field name can (optionally) be set with
      --auth-pass-field.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--auth-verify-url &lt;URL&gt;</dt>
  <dd class="It-tag">This URL allows skipfish to verify whether authentication
      was successful. This requires a URL where anonymous and authenticated
      requests are answered with a different response.
    <div style="height: 1.00em;">&#x00A0;</div>
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<h1 class="Sh" title="Sh" id="CRAWLING_SCOPE"><a class="selflink" href="#CRAWLING_SCOPE">CRAWLING
  SCOPE</a></h1>
Some sites may be too big to scan in a reasonable timeframe. If the site
  features well-defined tarpits - for example, 100,000 nearly identical user
  profiles as a part of a social network - these specific locations can be
  excluded with -X or -S. In other cases, you may need to resort to other
  settings: -d limits crawl depth to a specified number of subdirectories; -c
  limits the number of children per directory; -x limits the total number of
  descendants per crawl tree branch; and -r limits the total number of requests
  to send in a scan.
<div style="height: 1.00em;">&#x00A0;</div>
<dl class="Bl-tag">
  <dt class="It-tag">-d/--max-crawl-depth &lt;depth&gt;</dt>
  <dd class="It-tag">Limit the depth of subdirectories being crawled (see
      above).</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-c/--max-crawl-child &lt;childs&gt;</dt>
  <dd class="It-tag">Limit the amount of subdirectories per directory we crawl
      into (see above).</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-x/--max-crawl-descendants &lt;descendants&gt;</dt>
  <dd class="It-tag">Limit the total number of descendants per crawl tree branch
      (see above).</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-r/--max-request-total &lt;request&gt;</dt>
  <dd class="It-tag">The maximum number of requests can be limited with this
      flag.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-p/--crawl-probability &lt;0-100&gt;</dt>
  <dd class="It-tag">By specifying a percentage between 1 and 100%, it is
      possible to tell the crawler to follow fewer than 100% of all links, and
      try fewer than 100% of all dictionary entries. This - naturally - limits
      the completeness of a scan, but unlike most other settings, it does so in
      a balanced, non-deterministic manner. It is extremely useful when you are
      setting up time-bound, but periodic assessments of your
    infrastructure.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-q/--seed &lt;seed&gt;</dt>
  <dd class="It-tag">This flag sets the initial random seed for the crawler to a
      specified value. This can be used to exactly reproduce a previous scan to
      compare results. Randomness is relied upon most heavily in the -p mode,
      but also influences a couple of other scan management decisions.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-I/--include-string &lt;domain/path&gt;</dt>
  <dd class="It-tag">With this flag, you can tell skipfish to only crawl and
      test URLs that match a certain string. This can help to narrow down the
      scope of a scan by only whitelisting certain sections of a web site (e.g.
      -I /shop).
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-X/--exclude-string &lt;domain/path&gt;</dt>
  <dd class="It-tag">The -X option can be used to exclude files / directories
      from the scan. This is useful to avoid session termination (i.e. by
      excluding /logout) or just for speeding up your scans by excluding static
      content directories like /icons/, /doc/, /manuals/, and other standard,
      mundane locations along these lines.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-K/--skip-parameter &lt;parameter name&gt;</dt>
  <dd class="It-tag">This flag allows you to specify parameter names not to
      fuzz. (useful for applications that put session IDs in the URL, to
      minimize noise).
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-D/--include-domain &lt;domain&gt;</dt>
  <dd class="It-tag">Allows you to specify additional hosts or domains to be
      in-scope for the test. By default, all hosts appearing in the command-line
      URLs are added to the list - but you can use -D to broaden these rules.
      The result of this will be that the crawler will follow links and tests
      links that point to these additional hosts.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-B/--trust-domain &lt;domain&gt;</dt>
  <dd class="It-tag">In some cases, you do not want to actually crawl a
      third-party domain, but you trust the owner of that domain enough not to
      worry about cross-domain content inclusion from that location. To suppress
      warnings, you can use the -B option
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-Z/--skip-error-pages</dt>
  <dd class="It-tag">Do not crawl into pages / directories that give an error
      5XX.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-O/--no-form-submits</dt>
  <dd class="It-tag">Using this flag will cause forms to be ignored during the
      scan.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-P/--no-html-parsing</dt>
  <dd class="It-tag">This flag will disable link extracting and effectively
      disables crawling. Using -P is useful when you want to test one specific
      URL or when you want to feed skipfish a list of URLs that were collected
      with an external crawler.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<h1 class="Sh" title="Sh" id="TESTING_SCOPE"><a class="selflink" href="#TESTING_SCOPE">TESTING
  SCOPE</a></h1>
<dl class="Bl-tag">
  <dt class="It-tag">--checks</dt>
  <dd class="It-tag">EXPERIMENTAL: Displays the crawler injection tests. The
      output shows the index number (useful for --checks-toggle), the check name
      and whether the check is enabled.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--checks-toggle &lt;check1,check2,..&gt;</dt>
  <dd class="It-tag">EXPERIMENTAL: Every injection test can be enabled/disabled
      with using this flag. As value, you need to provide the check numbers
      which can be obtained with the --checks flag. Multiple checks can be
      toggled via a comma separated value (i.e. --checks-toggle 1,2 )
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--no-injection-tests</dt>
  <dd class="It-tag">EXPERIMENTAL: Disables all injection tests for this scan
      and limits the scan to crawling and, optionally, bruteforcing. As with all
      scans, the output directory will contain a pivots.txt file. This file can
      be used to feed future scans.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<h1 class="Sh" title="Sh" id="REPORTING_OPTIONS"><a class="selflink" href="#REPORTING_OPTIONS">REPORTING
  OPTIONS</a></h1>
<dl class="Bl-tag">
  <dt class="It-tag">-o/--output &lt;dir&gt;</dt>
  <dd class="It-tag">The report wil be written to this location. The directory
      is one of the two mandatory options and must not exist upon starting the
      scan.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-M/--log-mixed-content</dt>
  <dd class="It-tag">Enable the logging of mixed content. This is highly
      recommended when scanning SSL-only sites to detect insecure content
      inclusion via non-SSL protected links.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-E/--log-cache-mismatches</dt>
  <dd class="It-tag">This will cause additonal content caching error to be
      reported.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-U/--log-external-urls</dt>
  <dd class="It-tag">Log all external URLs and email addresses that were seen
      during the scan.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-Q/--log-unique-nodes</dt>
  <dd class="It-tag">Enable this to completely suppress duplicate nodes in
      reports.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-u/--quiet</dt>
  <dd class="It-tag">This will cause skipfish to suppress all console output
      during the scan.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-v/--verbose</dt>
  <dd class="It-tag">EXPERIMENTAL: Use this flag to enable runtime reporting of,
      for example, problems that are detected. Can be used multiple times to
      increase verbosity and should be used in combination with -u unless you
      run skipfish with stderr redirected to a file.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<h1 class="Sh" title="Sh" id="DICTIONARY_MANAGEMENT"><a class="selflink" href="#DICTIONARY_MANAGEMENT">DICTIONARY
  MANAGEMENT</a></h1>
Make sure you've read the instructions provided in doc/dictionaries.txt to
  select the right dictionary file and configure it correctly. This step has a
  profound impact on the quality of scan results later on.
<div style="height: 1.00em;">&#x00A0;</div>
<dl class="Bl-tag">
  <dt class="It-tag">-S/--wordlist &lt;file&gt;</dt>
  <dd class="It-tag">Load the specified (read-only) wordlist for use during the
      scan. This flag is optional but use of a dictionary is highly recommended
      when performing a blackbox scan as it will highlight hidden files and
      directories.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-W/--rw-wordlist &lt;file&gt;</dt>
  <dd class="It-tag">Specify an initially empty file for any newly learned
      site-specific keywords (which will come handy in future assessments). You
      can use -W- or -W /dev/null if you don't want to store auto-learned
      keywords anywhere. Typically you will want to use one of the packaged
      dictonaries (i.e. complete.wl) and possibly add a custom dictionary.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-L/--no-keyword-learning</dt>
  <dd class="It-tag">During the scan, skipfish will try to learn and use new
      keywords. This flag disables that behavior and should be used when any
      form of brute-forcing is not desired.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-Y/--no-extension-brute</dt>
  <dd class="It-tag">This flag will disable extension guessing during directory
      bruteforcing.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-R &lt;age&gt;</dt>
  <dd class="It-tag">Use of this flag allows old words to be purged from
      wordlists. It is intended to help keeping dictionaries clean when used in
      recurring scans.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-T/--form-value &lt;name=value&gt;</dt>
  <dd class="It-tag">Skipfish also features a form auto-completion mechanism in
      order to maximize scan coverage. The values should be non-malicious, as
      they are not meant to implement security checks - but rather, to get past
      input validation logic. You can define additional rules, or override
      existing ones, with the -T option (-T form_field_name=field_value, e.g. -T
      login=test123 -T password=test321 - although note that -C and -A are a
      much better method of logging in).
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-G &lt;max guesses&gt;</dt>
  <dd class="It-tag">During the scan, a temporary buffer of newly detected
      keywords is maintained. The size of this buffer can be changed with this
      flag and doing so influences bruteforcing.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<h1 class="Sh" title="Sh" id="PERFORMANCE_OPTIONS"><a class="selflink" href="#PERFORMANCE_OPTIONS">PERFORMANCE
  OPTIONS</a></h1>
The default performance setting should be fine for most servers but when the
  report indicates there were connection problems, you might want to tweak some
  of the values here. For unstable servers, the scan coverage is likely to
  improve when using low values for rate and connection flags.
<div style="height: 1.00em;">&#x00A0;</div>
<dl class="Bl-tag">
  <dt class="It-tag">-l/--max-request-rate &lt;rate&gt;</dt>
  <dd class="It-tag">This flag can be used to limit the amount of requests per
      second. This is very useful when the target server can't keep up with the
      high amount of requests that are generated by skipfish. Keeping the amount
      requests per second low can also help preventing some rate-based DoS
      protection mechanisms from kicking in and ruining the scan.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-g/--max-connections &lt;number&gt;</dt>
  <dd class="It-tag">The max simultaneous TCP connections (global) can be set
      with this flag.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-m/--max-host-connections &lt;number&gt;</dt>
  <dd class="It-tag">The max simultaneous TCP connections, per target IP, can be
      set with this flag.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-f/--max-failed-requests &lt;number&gt;</dt>
  <dd class="It-tag">Controls the maximum number of consecutive HTTP errors you
      are willing to see before aborting the scan. For large scans, you probably
      want to set a higher value here.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-t/--request-timeout &lt;timeout&gt;</dt>
  <dd class="It-tag">Set the total request timeout, to account for really slow
      or really fast sites.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-w/--network-timeout &lt;timeout&gt;</dt>
  <dd class="It-tag">Set the network I/O timeout.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-i/--idle-timeout &lt;timeout&gt;</dt>
  <dd class="It-tag">Specify the timeout for idle HTTP connections.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-s/--response-size &lt;size&gt;</dt>
  <dd class="It-tag">Sets the maximum length of a response to fetch and parse
      (longer responses will be truncated).
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">-e/--discard-binary</dt>
  <dd class="It-tag">This prevents binary documents from being kept in memory
      for reporting purposes, and frees up a lot of RAM.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">--flush-to-disk</dt>
  <dd class="It-tag">This causes request / response data to be flushed to disk
      instead of being kept in memory. As a result, the memory usage for large
      scans will be significant lower.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<h1 class="Sh" title="Sh" id="EXAMPLES"><a class="selflink" href="#EXAMPLES">EXAMPLES</a></h1>
<b>Scan type: config</b>
<div>&#x00A0;</div>
skipfish --config config/example.conf http://example.com
<div>&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<div>&#x00A0;</div>
<b>Scan type: quick</b>
<div>&#x00A0;</div>
skipfish -o output/dir/ http://example.com
<div>&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<div>&#x00A0;</div>
<b>Scan type: extensive bruteforce</b>
<div>&#x00A0;</div>
skipfish [...other options..] <i>-S dictionaries/complete.wl</i>
  http://example.com
<div>&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<div>&#x00A0;</div>
<b>Scan type: without bruteforcing</b>
<div>&#x00A0;</div>
skipfish [...other options..] -LY http://example.com
<div>&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<b>Scan type: authenticated (basic)</b>
<div>&#x00A0;</div>
skipfish [...other options..] <i>-A username:password</i> http://example.com
<div>&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<b>Scan type: authenticated (cookie)</b>
<div>&#x00A0;</div>
skipfish [...other options..] -C jsession=myauthcookiehere -X /logout
  http://example.com
<div>&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<b>Scan type: flaky server</b>
<div>&#x00A0;</div>
skipfish [...other options..] -l 5 -g 2 -t 30 -i 15 http://example.com
<div>&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h1 class="Sh" title="Sh" id="NOTES"><a class="selflink" href="#NOTES">NOTES</a></h1>
The default values for all flags can be viewed by running &#x00B4;./skipfish
  -h&#x00B4; .
<div style="height: 1.00em;">&#x00A0;</div>
<h1 class="Sh" title="Sh" id="AUTHOR"><a class="selflink" href="#AUTHOR">AUTHOR</a></h1>
skipfish was written by Michal Zalewski &lt;lcamtuf@google.com&gt;, with
  contributions from Niels Heinen &lt;heinenn@google.com&gt;, Sebastian Roschke
  &lt;s.roschke@googlemail.com&gt;, and other parties.
<div class="Pp"></div>
This manual page was written with the help of Thorsten Schifferdecker
  &lt;tsd@debian.systs.org&gt;.</div>
<table class="foot">
  <tr>
    <td class="foot-date">May 6, 2012</td>
    <td class="foot-os"></td>
  </tr>
</table>
</body>
</html>
