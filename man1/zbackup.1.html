<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 16:44:49 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>zbackup(1) zbackup(1)</p>

<p style="margin-top: 1em">Introduction <br>
zbackup is a globally-deduplicating backup tool, based on
the ideas found in rsync (http://rsync.samba.org/). Feed a
large .tar into it, and it will store duplicate regions of
<br>
it only once, then compress and optionally encrypt the
result. Feed another .tar file, and it will also re-use any
data found in any previous backups. This way only new
changes <br>
are stored, and as long as the files are not very different,
the amount of storage required is very low. Any of the
backup files stored previously can be read back in full at
<br>
any time. The program is format-agnostic, so you can feed
virtually any files to it (any types of archives,
proprietary formats, even raw disk images -- but see Caveats
<br>
(#caveats)).</p>

<p style="margin-top: 1em">This is achieved by sliding a
window with a rolling hash over the input at a byte
granularity and checking whether the block in focus was ever
met already. If a rolling hash <br>
matches, an additional full cryptographic hash is calculated
to ensure the block is indeed the same. The deduplication
happens then.</p>

<p style="margin-top: 1em">Features <br>
The program has the following features:</p>

<p style="margin-top: 1em">&Acirc;&middot; Parallel LZMA or
LZO compression of the stored data</p>

<p style="margin-top: 1em">&Acirc;&middot; Built-in AES
encryption of the stored data</p>

<p style="margin-top: 1em">&Acirc;&middot; Possibility to
delete old backup data</p>

<p style="margin-top: 1em">&Acirc;&middot; Use of a 64-bit
rolling hash, keeping the amount of soft collisions to
zero</p>

<p style="margin-top: 1em">&Acirc;&middot; Repository
consists of immutable files. No existing files are ever
modified</p>

<p style="margin-top: 1em">&Acirc;&middot; Written in C++
only with only modest library dependencies</p>

<p style="margin-top: 1em">&Acirc;&middot; Safe to use in
production (see below (#safety))</p>

<p style="margin-top: 1em">&Acirc;&middot; Possibility to
exchange data between repos without recompression</p>

<p style="margin-top: 1em">Build dependencies <br>
&Acirc;&middot; cmake &gt;= 2.8.2 (though it should not be
too hard to compile the sources by hand if needed)</p>

<p style="margin-top: 1em">&Acirc;&middot; libssl-dev for
all encryption, hashing and random numbers</p>

<p style="margin-top: 1em">&Acirc;&middot; libprotobuf-dev
and protobuf-compiler for data serialization</p>

<p style="margin-top: 1em">&Acirc;&middot; liblzma-dev for
compression</p>

<p style="margin-top: 1em">&Acirc;&middot; liblzo2-dev for
compression (optional)</p>

<p style="margin-top: 1em">&Acirc;&middot; zlib1g-dev for
adler32 calculation</p>

<p style="margin-top: 1em">Quickstart <br>
To build and install:</p>

<p style="margin-top: 1em">cd zbackup <br>
cmake . <br>
make <br>
sudo make install <br>
# or just run as ./zbackup</p>

<p style="margin-top: 1em">Zbackup is also part of the
Debian
(https://packages.debian.org/search?keywords=zbackup),
Ubuntu (http://packages.ubuntu.com/search?keywords=zbackup)
and Arch Linux <br>
(https://aur.archlinux.org/packages/zbackup/) distributions
of GNU/Linux.</p>

<p style="margin-top: 1em">To use:</p>

<p style="margin-top: 1em">zbackup init --non-encrypted
/my/backup/repo <br>
tar c /my/precious/data | zbackup backup
/my/backup/repo/backups/backup-&lsquo;date
&rsquo;+%Y-%m-%d&rsquo;&lsquo; <br>
zbackup restore /my/backup/repo/backups/backup-&lsquo;date
&rsquo;+%Y-%m-%d&rsquo;&lsquo; &gt;
/my/precious/backup-restored.tar</p>

<p style="margin-top: 1em">If you have a lot of RAM to
spare, you can use it to speed-up the restore process -- to
use 512 MB more, pass --cache-size 512mb when restoring.</p>

<p style="margin-top: 1em">If encryption is wanted, create
a file with your password:</p>

<p style="margin-top: 1em"># more secure to use an editor
<br>
echo mypassword &gt; ~/.my_backup_password <br>
chmod 600 ~/.my_backup_password</p>

<p style="margin-top: 1em">Then init the repo the following
way:</p>

<p style="margin-top: 1em">zbackup init --password-file
~/.my_backup_password /my/backup/repo</p>

<p style="margin-top: 1em">And always pass the same
argument afterwards:</p>

<p style="margin-top: 1em">tar c /my/precious/data |
zbackup --password-file ~/.my_backup_password backup
/my/backup/repo/backups/backup-&lsquo;date
&rsquo;+%Y-%m-%d&rsquo;&lsquo; <br>
zbackup --password-file ~/.my_backup_password restore
/my/backup/repo/backups/backup-&lsquo;date
&rsquo;+%Y-%m-%d&rsquo;&lsquo; &gt;
/my/precious/backup-restored.tar</p>

<p style="margin-top: 1em">If you have a 32-bit system and
a lot of cores, consider lowering the number of compression
threads by passing --threads 4 or --threads 2 if the program
runs out of address space <br>
when backing up (see why below (#caveats), item 2). There
should be no problem on a 64-bit system.</p>

<p style="margin-top: 1em">Caveats <br>
&Acirc;&middot; While you can pipe any data into the
program, the data should be uncompressed and unencrypted --
otherwise no deduplication could be performed on it. zbackup
would compress <br>
and encrypt the data itself, so there&rsquo;s no need to do
that yourself. So just run tar c and pipe it into zbackup
directly. If backing up disk images employing encryption,
pipe <br>
the unencrypted version (the one you normally mount). If you
create .zip or .rar files, use no compression (-0 or -m0)
and no encryption.</p>

<p style="margin-top: 1em">&Acirc;&middot; Parallel LZMA
compression uses a lot of RAM (several hundreds of
megabytes, depending on the number of threads used), and ten
times more virtual address space. The latter is <br>
only relevant on 32-bit architectures where it&rsquo;s
limited to 2 or 3 GB. If you hit the ceiling, lower the
number of threads with --threads.</p>

<p style="margin-top: 1em">&Acirc;&middot; Since the data
is deduplicated, there&rsquo;s naturally no redundancy in
it. A loss of a single file can lead to a loss of virtually
all data. Make sure you store it on a redundant <br>
storage (RAID1, a cloud provider etc).</p>

<p style="margin-top: 1em">&Acirc;&middot; The encryption
key, if used, is stored in the info file in the root of the
repo. It is encrypted with your password. Technically thus
you can change your password without <br>
re-encrypting any data, and as long as no one possesses the
old info file and knows your old password, you would be safe
(even though the actual option to change password is <br>
not implemented yet -- someone who needs this is welcome to
create a pull request -- the possibility is all there). Also
note that it is crucial you don&rsquo;t lose your info file,
<br>
as otherwise the whole backup would be lost.</p>

<p style="margin-top: 1em">Limitations <br>
&Acirc;&middot; Right now the only modes supported are
reading from standard input and writing to standard output.
FUSE mounts and NBD servers may be added later if someone
contributes the <br>
code.</p>

<p style="margin-top: 1em">&Acirc;&middot; The program
keeps all known blocks in an in-RAM hash table, which may
create scalability problems for very large repos (see below
(#scalability)).</p>

<p style="margin-top: 1em">&Acirc;&middot; The only
encryption mode currently implemented is AES-128 in CBC mode
with PKCS#7 padding. If you believe that this is not secure
enough, patches are welcome. Before you jump <br>
to conclusions however, read this article
(http://www.schneier.com/blog/archives/2009/07/another_new_aes.html).</p>

<p style="margin-top: 1em">&Acirc;&middot; The only
compression mode supported is LZMA, which suits backups very
nicely.</p>

<p style="margin-top: 1em">&Acirc;&middot; It&rsquo;s only
possible to fully restore the backup in order to get to a
required file, without any option to quickly pick it out.
tar would not allow to do it anyway, but e.g. <br>
for zip files it could have been possible. This is possible
to implement though, e.g. by exposing the data over a FUSE
filesystem.</p>

<p style="margin-top: 1em">&Acirc;&middot; There&rsquo;s no
option to specify block and bundle sizes other than the
default ones (currently 64k and 2MB respectively), though
it&rsquo;s trivial to add command-line switches for <br>
those.</p>

<p style="margin-top: 1em">Most of those limitations can be
lifted by implementing the respective features.</p>

<p style="margin-top: 1em">Safety <br>
Is it safe to use zbackup for production data? Being free
software, the program comes with no warranty of any kind.
That said, it&rsquo;s perfectly safe for production, and
here&rsquo;s <br>
why. When performing a backup, the program never modifies or
deletes any existing files -- only new ones are created. It
specifically checks for that, and the code paths in&acirc;
<br>
volved are short and easy to inspect. Furthermore, each
backup is protected by its SHA256 sum, which is calculated
before piping the data into the deduplication logic. The
code <br>
path doing that is also short and easy to inspect. When a
backup is being restored, its SHA256 is calculated again and
compared against the stored one. The program would fail <br>
on a mismatch. Therefore, to ensure safety it is enough to
restore each backup to /dev/null immediately after creating
it. If it restores fine, it will restore fine ever after.
<br>
To add some statistics, the author of the program has been
using an older version of zbackup internally for over a
year. The SHA256 check never ever failed. Again, even if it
<br>
does, you would know immediately, so no work would be lost.
Therefore you are welcome to try the program in production,
and if you like it, stick with it.</p>

<p style="margin-top: 1em">Usage notes <br>
The repository has the following directory structure:</p>

<p style="margin-top: 1em">/repo <br>
backups/ <br>
bundles/ <br>
00/ <br>
01/ <br>
02/ <br>
... <br>
index/ <br>
info</p>

<p style="margin-top: 1em">&Acirc;&middot; The backups
directory contain your backups. Those are very small files
which are needed for restoration. They are encrypted if
encryption is enabled. The names can be arbi&acirc; <br>
trary. It is possible to arrange files in subdirectories,
too. Free renaming is also allowed.</p>

<p style="margin-top: 1em">&Acirc;&middot; The bundles
directory contains the bulk of data. Each bundle internally
contains multiple small chunks, compressed together and
encrypted. Together all those chunks account <br>
for all deduplicated data stored.</p>

<p style="margin-top: 1em">&Acirc;&middot; The index
directory contains the full index of all chunks in the
repository, together with their bundle names. A separate
index file is created for each backup session. Tech&acirc;
<br>
nically those files are redundant, all information is
contained in the bundles themselves. However, having a
separate index is nice for two reasons: 1) it&rsquo;s faster
to read as <br>
it incurs less seeks, and 2) it allows making backups while
storing bundles elsewhere. Bundles are only needed when
restoring -- otherwise it&rsquo;s sufficient to only have
index. <br>
One could then move all newly created bundles into another
machine after each backup.</p>

<p style="margin-top: 1em">&Acirc;&middot; info is a very
important file which contains all global repository
metadata, such as chunk and bundle sizes, and an encryption
key encrypted with the user password. It is <br>
paramount not to lose it, so backing it up separately
somewhere might be a good idea. On the other hand, if you
absolutely don&rsquo;t trust your remote storage provider,
you might <br>
consider not storing it with the rest of the data. It would
then be impossible to decrypt it at all, even if your
password gets known later.</p>

<p style="margin-top: 1em">The program does not have any
facilities for sending your backup over the network. You can
rsync the repo to another computer or use any kind of cloud
storage capable of storing <br>
files. Since zbackup never modifies any existing files, the
latter is especially easy -- just tell the upload tool you
use not to upload any files which already exist on the
re&acirc; <br>
mote side (e.g. with gsutil it&rsquo;s gsutil cp -R -n
/my/backup gs:/mybackup/).</p>

<p style="margin-top: 1em">To aid with creating backups,
there&rsquo;s an utility called tartool included with
zbackup. The idea is the following: one sprinkles empty
files called .backup and .no-backup across <br>
the entire filesystem. Directories where .backup files are
placed are marked for backing up. Similarly, directories
with .no-backup files are marked not to be backed up.
Addi&acirc; <br>
tionally, it is possible to place .backup-XYZ in the same
directory where XYZ is to mark XYZ for backing up, or place
.no-backup-XYZ to mark it not to be backed up. Then tartool
<br>
can be run with three arguments -- the root directory to
start from (can be /), the output includes file, and the
output excludes file. The tool traverses over the given
direc&acirc; <br>
tory noting the .backup* and .no-backup* files and creating
include and exclude lists for the tar utility. The tar
utility could then be run as tar c --files-from includes
--ex&acirc; <br>
clude-from excludes to store all chosen data.</p>

<p style="margin-top: 1em">Scalability <br>
This section tries do address the question on the maximum
amount of data which can be held in a backup repository.
What is meant here is the deduplicated data. The number of
<br>
bytes in all source files ever fed into the repository
doesn&rsquo;t matter, but the total size of the resulting
repository does. <br>
Internally all input data is split into small blocks called
chunks (up to 64k each by default). Chunks are collected
into bundles (up to 2MB each by default), and those bundles
<br>
are then compressed and encrypted.</p>

<p style="margin-top: 1em">There are then two problems with
the total number of chunks in the repository:</p>

<p style="margin-top: 1em">&Acirc;&middot; Hashes of all
existing chunks are needed to be kept in RAM while the
backup is ongoing. Since the sliding window performs
checking with a single-byte granularity, lookups <br>
would otherwise be too slow. The amount of data needed to be
stored is technically only 24 bytes for each chunk, where
the size of the chunk is up to 64k. In an example re&acirc;
<br>
al-life 18GB repo, only 18MB are taken by in its hash index.
Multiply this roughly by two to have an estimate of RAM
needed to store this index as an in-RAM hash table.
How&acirc; <br>
ever, as this size is proportional to the total size of the
repo, for 2TB repo you could already require 2GB of RAM.
Most repos are much smaller though, and as long as the <br>
deduplication works properly, in many cases you can store
terabytes of highly-redundant backup files in a 20GB repo
easily.</p>

<p style="margin-top: 1em">&Acirc;&middot; We use a 64-bit
rolling hash, which allows to have an O(1) lookup cost at
each byte we process. Due to birthday paradox
(https://en.wikipedia.org/wiki/Birthday_paradox), we <br>
would start having collisions when we approach 2^32 hashes.
If each chunk we have is 32k on average, we would get there
when our repo grows to 128TB. We would still be able <br>
to continue, but as the number of collisions would grow, we
would have to resort to calculating the full hash of a block
at each byte more and more often, which would result in <br>
a considerable slowdown.</p>

<p style="margin-top: 1em">All in all, as long as the
amount of RAM permits, one can go up to several terabytes in
deduplicated data, and start having some slowdown after
having hundreds of terabytes, <br>
RAM-permitting.</p>

<p style="margin-top: 1em">Design choices <br>
&Acirc;&middot; We use a 64-bit modified Rabin-Karp rolling
hash (see rolling_hash.hh for details), while most other
programs use a 32-bit one. As noted previously, one problem
with the hash <br>
size is its birthday bound, which with the 32-bit hash is
met after having only 2^16 hashes. The choice of a 64-bit
hash allows us to scale much better while having virtually
<br>
the same calculation cost on a typical 64-bit machine.</p>

<p style="margin-top: 1em">&Acirc;&middot; rsync uses MD5
as its strong hash. While MD5 is known to be fast, it is
also known to be broken, allowing a malicious user to craft
colliding inputs. zbackup uses SHA1 in&acirc; <br>
stead. The cost of SHA1 calculations on modern machines is
actually less than that of MD5 (run openssl speed md5 sha1
on yours), so it&rsquo;s a win-win situation. We only keep
the <br>
first 128 bits of the SHA1 output, and therefore together
with the rolling hash we have a 192-bit hash for each chunk.
It&rsquo;s a multiple of 8 bytes which is a nice properly on
<br>
64-bit machines, and it is long enough not to worry about
possible collisions.</p>

<p style="margin-top: 1em">&Acirc;&middot; AES-128 in CBC
mode with PKCS#7 padding is used for encryption. This seems
to be a reasonbly safe classic solution. Each encrypted file
has a random IV as its first 16 bytes.</p>

<p style="margin-top: 1em">&Acirc;&middot; We use
Google&rsquo;s protocol buffers
(https://developers.google.com/protocol-buffers/) to
represent data structures in binary form. They are very
efficient and relatively simple <br>
to use.</p>

<p style="margin-top: 1em">Compression <br>
zbackup uses LZMA to compress stored data. It compresses
very well, but it will slow down your backup <br>
(unless you have a very fast CPU).</p>

<p style="margin-top: 1em">LZO is much faster, but the
files will be bigger. If you don&rsquo;t <br>
want your backup process to be cpu-bound, you should
consider using LZO. However, there are some caveats:</p>

<p style="margin-top: 1em">1. LZO is so fast that other
parts of zbackup consume significant portions of the CPU. In
fact, it is only <br>
using one core on my machine because compression is the only
thing that can run in parallel.</p>

<p style="margin-top: 1em">2. I&rsquo;ve hacked the LZO
support in a day. You shouldn&rsquo;t trust it. Please make
sure that restore works before <br>
you assume that your data is safe. That may still be faster
than a backup with LZMA ;-)</p>

<p style="margin-top: 1em">3. LZMA is still the default, so
make sure that you use the --compression lzo argument when
you init the <br>
repo or whenever you do a backup.</p>

<p style="margin-top: 1em">You can mix LZMA and LZO in a
repository. Each bundle file has a field that says how it
was compressed, so <br>
zbackup will use the right method to decompress it. You
could use an old zbackup respository with only LZMA <br>
bundles and start using LZO. However, please think twice
before you do that because old versions of zbackup <br>
won&rsquo;t be able to read those bundles.</p>

<p style="margin-top: 1em">Improvements <br>
There&rsquo;s a lot to be improved in the program. It was
released with the minimum amount of functionality to be
useful. It is also stable. This should hopefully stimulate
people <br>
to join the development and add all those other fancy
features. Here&rsquo;s a list of ideas:</p>

<p style="margin-top: 1em">&Acirc;&middot; Additional
options, such as configurable chunk and bundle sizes
etc.</p>

<p style="margin-top: 1em">&Acirc;&middot; A command to
change password.</p>

<p style="margin-top: 1em">&Acirc;&middot; Improved garbage
collection. The program should support ability to specify
maximum index file size / maximum index file count (for
better compatibility with cloud storages as <br>
well) or something like retention policy.</p>

<p style="margin-top: 1em">&Acirc;&middot; A command to
fsck the repo by doing something close to what garbage
collection does, but also checking all hashes and so on.</p>

<p style="margin-top: 1em">&Acirc;&middot; Parallel
decompression. Right now decompression is single-threaded,
but it is possible to look ahead in the stream and perform
prefetching.</p>

<p style="margin-top: 1em">&Acirc;&middot; Support for
mounting the repo over FUSE. Random access to data would
then be possible.</p>

<p style="margin-top: 1em">&Acirc;&middot; Support for
exposing a backed up file over a userspace NBD server. It
would then be possible to mount raw disk images without
extracting them.</p>

<p style="margin-top: 1em">&Acirc;&middot; Support for
other encryption types (preferably for everything openssl
supports with its evp).</p>

<p style="margin-top: 1em">&Acirc;&middot; Support for
other compression methods.</p>

<p style="margin-top: 1em">&Acirc;&middot; You name it!</p>

<p style="margin-top: 1em">Communication <br>
&Acirc;&middot; The program&rsquo;s website is at
&lt;http://zbackup.org/&gt;.</p>

<p style="margin-top: 1em">&Acirc;&middot; Development
happens at &lt;https://github.com/zbackup/zbackup&gt;.</p>

<p style="margin-top: 1em">&Acirc;&middot; Discussion forum
is at
&lt;https://groups.google.com/forum/#!forum/zbackup&gt;.
Please ask for help there!</p>

<p style="margin-top: 1em">The author is reachable over
email at &lt;ikm@zbackup.org&gt;. Please be constructive and
don&rsquo;t ask for help using the program, though. In most
cases it&rsquo;s best to stick to the forum, <br>
unless you have something to discuss with the author in
private.</p>

<p style="margin-top: 1em">Similar projects <br>
zbackup is certainly not the first project to embrace the
idea of using a rolling hash for deduplication. Here&rsquo;s
a list of other projects the author found on the web:</p>

<p style="margin-top: 1em">&Acirc;&middot; bup
(https://github.com/bup/bup), based on storing data in git
packs. No possibility of removing old data. This program was
the initial inspiration for zbackup.</p>

<p style="margin-top: 1em">&Acirc;&middot; ddar
(http://www.synctus.com/ddar/), seems to be a little bit
outdated. Contains a nice list of alternatives with
comparisons.</p>

<p style="margin-top: 1em">&Acirc;&middot; rdiff-backup
(http://www.nongnu.org/rdiff-backup/), based on the original
rsync algorithm. Does not do global deduplication, only
working over the files with the same file <br>
name.</p>

<p style="margin-top: 1em">&Acirc;&middot; duplicity
(http://duplicity.nongnu.org/), which looks similar to
rdiff-backup with regards to mode of operation.</p>

<p style="margin-top: 1em">&Acirc;&middot; Some filesystems
(most notably ZFS (http://en.wikipedia.org/wiki/ZFS) and
Btrfs (http://en.wikipedia.org/wiki/Btrfs)) provide
deduplication features. They do so only at block <br>
level though, without a sliding window, so they can not
accomodate to arbitrary byte insertion/deletion in the
middle of data.</p>

<p style="margin-top: 1em">&Acirc;&middot; Attic
(https://attic-backup.org/), which looks very similar to
zbackup.</p>

<p style="margin-top: 1em">Credits <br>
Copyright (c) 2012-2014 Konstantin Isakov
(&lt;ikm@zbackup.org&gt;) and ZBackup contributors, see
CONTRIBUTORS. Licensed under GNU GPLv2 or later + OpenSSL,
see LICENSE.</p>

<p style="margin-top: 1em">This program is free software;
you can redistribute it and/or modify it under the terms of
the GNU General Public License as published by the Free
Software Foundation; either <br>
version 2 of the License, or (at your option) any later
version.</p>

<p style="margin-top: 1em">This program is distributed in
the hope that it will be useful, but WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PUR&acirc; <br>
POSE. See the GNU General Public License for more
details.</p>

<p style="margin-top: 1em">You should have received a copy
of the GNU General Public License along with this program;
if not, write to the Free Software Foundation, Inc., 51
Franklin Street, Fifth Floor, <br>
Boston, MA 02110-1301 USA.</p>

<p style="margin-top: 1em">February 12, 2017 zbackup(1)</p>
<hr>
</body>
</html>
