<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8"/>
  <style>
    table.head, table.foot { width: 100%; }
    td.head-rtitle, td.foot-os { text-align: right; }
    td.head-vol { text-align: center; }
    div.Pp { margin: 1ex 0ex; }
  </style>
  <title>zbackup(1)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">zbackup(1)</td>
    <td class="head-vol"></td>
    <td class="head-rtitle">zbackup(1)</td>
  </tr>
</table>
<div class="manual-text">
<h1 class="Sh" title="Sh" id="Introduction"><a class="selflink" href="#Introduction">Introduction</a></h1>
<b>zbackup</b> is a globally-deduplicating backup tool, based on the ideas found
  in rsync (http://rsync.samba.org/). Feed a large .tar into it, and it will
  store duplicate regions of it only once, then compress and optionally encrypt
  the result. Feed another .tar file, and it will also re-use any data found in
  any previous backups. This way only new changes are stored, and as long as the
  files are not very different, the amount of storage required is very low. Any
  of the backup files stored previously can be read back in full at any time.
  The program is format-agnostic, so you can feed virtually any files to it (any
  types of archives, proprietary formats, even raw disk images -- but see
  Caveats (#caveats)).
<div class="Pp"></div>
This is achieved by sliding a window with a rolling hash over the input at a
  byte granularity and checking whether the block in focus was ever met already.
  If a rolling hash matches, an additional full cryptographic hash is calculated
  to ensure the block is indeed the same. The deduplication happens then.
<h1 class="Sh" title="Sh" id="Features"><a class="selflink" href="#Features">Features</a></h1>
The program has the following features:
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Parallel LZMA or LZO compression of the stored data</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Built-in AES encryption of the stored data</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Possibility to delete old backup data</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Use of a 64-bit rolling hash, keeping the amount of soft
      collisions to zero</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Repository consists of immutable files. No existing files
      are ever modified</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Written in C++ only with only modest library
    dependencies</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Safe to use in production (see below (#safety))</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Possibility to exchange data between repos without
      recompression</dd>
</dl>
<h1 class="Sh" title="Sh" id="Build_dependencies"><a class="selflink" href="#Build_dependencies">Build
  dependencies</a></h1>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">cmake &gt;= 2.8.2 (though it should not be too hard to
      compile the sources by hand if needed)</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">libssl-dev for all encryption, hashing and random
    numbers</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">libprotobuf-dev and protobuf-compiler for data
      serialization</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">liblzma-dev for compression</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">liblzo2-dev for compression (optional)</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">zlib1g-dev for adler32 calculation</dd>
</dl>
<h1 class="Sh" title="Sh" id="Quickstart"><a class="selflink" href="#Quickstart">Quickstart</a></h1>
To build and install:
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">
    <pre>
cd&#x00A0;zbackup
cmake&#x00A0;.
make
sudo&#x00A0;make&#x00A0;install
#&#x00A0;or&#x00A0;just&#x00A0;run&#x00A0;as&#x00A0;./zbackup
    </pre>
  </dd>
</dl>
<div class="Pp"></div>
Zbackup is also part of the Debian
  (https://packages.debian.org/search?keywords=zbackup), Ubuntu
  (http://packages.ubuntu.com/search?keywords=zbackup) and Arch Linux
  (https://aur.archlinux.org/packages/zbackup/) distributions of GNU/Linux.
<div class="Pp"></div>
To use:
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">
    <pre>
zbackup&#x00A0;init&#x00A0;--non-encrypted&#x00A0;/my/backup/repo
tar&#x00A0;c&#x00A0;/my/precious/data&#x00A0;|&#x00A0;zbackup&#x00A0;backup&#x00A0;/my/backup/repo/backups/backup-`date&#x00A0;'+%Y-%m-%d'`
zbackup&#x00A0;restore&#x00A0;/my/backup/repo/backups/backup-`date&#x00A0;'+%Y-%m-%d'`&#x00A0;&gt;&#x00A0;/my/precious/backup-restored.tar
    </pre>
  </dd>
</dl>
<div class="Pp"></div>
If you have a lot of RAM to spare, you can use it to speed-up the restore
  process -- to use 512 MB more, pass --cache-size&#x00A0;512mb when restoring.
<div class="Pp"></div>
If encryption is wanted, create a file with your password:
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">
    <pre>
#&#x00A0;more&#x00A0;secure&#x00A0;to&#x00A0;use&#x00A0;an&#x00A0;editor
echo&#x00A0;mypassword&#x00A0;&gt;&#x00A0;~/.my_backup_password
chmod&#x00A0;600&#x00A0;~/.my_backup_password
    </pre>
  </dd>
</dl>
<div class="Pp"></div>
Then init the repo the following way:
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">
    <pre>
zbackup&#x00A0;init&#x00A0;--password-file&#x00A0;~/.my_backup_password&#x00A0;/my/backup/repo
    </pre>
  </dd>
</dl>
<div class="Pp"></div>
And always pass the same argument afterwards:
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">
    <pre>
tar&#x00A0;c&#x00A0;/my/precious/data&#x00A0;|&#x00A0;zbackup&#x00A0;--password-file&#x00A0;~/.my_backup_password&#x00A0;backup&#x00A0;/my/backup/repo/backups/backup-`date&#x00A0;'+%Y-%m-%d'`
zbackup&#x00A0;--password-file&#x00A0;~/.my_backup_password&#x00A0;restore&#x00A0;/my/backup/repo/backups/backup-`date&#x00A0;'+%Y-%m-%d'`&#x00A0;&gt;&#x00A0;/my/precious/backup-restored.tar
    </pre>
  </dd>
</dl>
<div class="Pp"></div>
If you have a 32-bit system and a lot of cores, consider lowering the number of
  compression threads by passing --threads&#x00A0;4 or --threads&#x00A0;2 if the
  program runs out of address space when backing up (see why below (#caveats),
  item 2). There should be no problem on a 64-bit system.
<h1 class="Sh" title="Sh" id="Caveats"><a class="selflink" href="#Caveats">Caveats</a></h1>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">While you can pipe any data into the program, the data
      should be uncompressed and unencrypted -- otherwise no deduplication could
      be performed on it. zbackup would compress and encrypt the data itself, so
      there's no need to do that yourself. So just run tar&#x00A0;c and pipe it
      into zbackup directly. If backing up disk images employing encryption,
      pipe the unencrypted version (the one you normally mount). If you create
      .zip or .rar files, use no compression (-0 or -m0) and no encryption.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Parallel LZMA compression uses a lot of RAM (several
      hundreds of megabytes, depending on the number of threads used), and ten
      times more virtual address space. The latter is only relevant on 32-bit
      architectures where it's limited to 2 or 3 GB. If you hit the ceiling,
      lower the number of threads with --threads.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Since the data is deduplicated, there's naturally no
      redundancy in it. A loss of a single file can lead to a loss of virtually
      all data. Make sure you store it on a redundant storage (RAID1, a cloud
      provider etc).</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">The encryption key, if used, is stored in the info file in
      the root of the repo. It is encrypted with your password. Technically thus
      you can change your password without re-encrypting any data, and as long
      as no one possesses the old info file and knows your old password, you
      would be safe (even though the actual option to change password is not
      implemented yet -- someone who needs this is welcome to create a pull
      request -- the possibility is all there). Also note that it is crucial you
      don't lose your info file, as otherwise the whole backup would be
    lost.</dd>
</dl>
<h1 class="Sh" title="Sh" id="Limitations"><a class="selflink" href="#Limitations">Limitations</a></h1>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Right now the only modes supported are reading from
      standard input and writing to standard output. FUSE mounts and NBD servers
      may be added later if someone contributes the code.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">The program keeps all known blocks in an in-RAM hash table,
      which may create scalability problems for very large repos (see below
      (#scalability)).</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">The only encryption mode currently implemented is AES-128
      in CBC mode with PKCS#7 padding. If you believe that this is not secure
      enough, patches are welcome. Before you jump to conclusions however, read
      this article
      (http://www.schneier.com/blog/archives/2009/07/another_new_aes.html).</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">The only compression mode supported is LZMA, which suits
      backups very nicely.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">It's only possible to fully restore the backup in order to
      get to a required file, without any option to quickly pick it out. tar
      would not allow to do it anyway, but e.g. for zip files it could have been
      possible. This is possible to implement though, e.g. by exposing the data
      over a FUSE filesystem.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">There's no option to specify block and bundle sizes other
      than the default ones (currently 64k and 2MB respectively), though it's
      trivial to add command-line switches for those.</dd>
</dl>
<div class="Pp"></div>
Most of those limitations can be lifted by implementing the respective features.
<h1 class="Sh" title="Sh" id="Safety"><a class="selflink" href="#Safety">Safety</a></h1>
Is it safe to use zbackup for production data? Being free software, the program
  comes with no warranty of any kind. That said, it's perfectly safe for
  production, and here's why. When performing a backup, the program never
  modifies or deletes any existing files -- only new ones are created. It
  specifically checks for that, and the code paths involved are short and easy
  to inspect. Furthermore, each backup is protected by its SHA256 sum, which is
  calculated before piping the data into the deduplication logic. The code path
  doing that is also short and easy to inspect. When a backup is being restored,
  its SHA256 is calculated again and compared against the stored one. The
  program would fail on a mismatch. Therefore, to ensure safety it is enough to
  restore each backup to /dev/null immediately after creating it. If it restores
  fine, it will restore fine ever after.
<div class="Pp"></div>
To add some statistics, the author of the program has been using an older
  version of zbackup internally for over a year. The SHA256 check never ever
  failed. Again, even if it does, you would know immediately, so no work would
  be lost. Therefore you are welcome to try the program in production, and if
  you like it, stick with it.
<h1 class="Sh" title="Sh" id="Usage_notes"><a class="selflink" href="#Usage_notes">Usage
  notes</a></h1>
The repository has the following directory structure:
<dl class="Bl-tag">
  <dt class="It-tag"></dt>
  <dd class="It-tag">
    <pre>
/repo
&#x00A0;&#x00A0;&#x00A0;&#x00A0;backups/
&#x00A0;&#x00A0;&#x00A0;&#x00A0;bundles/
&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;00/
&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;01/
&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;02/
&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;&#x00A0;...
&#x00A0;&#x00A0;&#x00A0;&#x00A0;index/
&#x00A0;&#x00A0;&#x00A0;&#x00A0;info
    </pre>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">The backups directory contain your backups. Those are very
      small files which are needed for restoration. They are encrypted if
      encryption is enabled. The names can be arbitrary. It is possible to
      arrange files in subdirectories, too. Free renaming is also allowed.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">The bundles directory contains the bulk of data. Each
      bundle internally contains multiple small chunks, compressed together and
      encrypted. Together all those chunks account for all deduplicated data
      stored.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">The index directory contains the full index of all chunks
      in the repository, together with their bundle names. A separate index file
      is created for each backup session. Technically those files are redundant,
      all information is contained in the bundles themselves. However, having a
      separate index is nice for two reasons: 1) it's faster to read as it
      incurs less seeks, and 2) it allows making backups while storing bundles
      elsewhere. Bundles are only needed when restoring -- otherwise it's
      sufficient to only have index. One could then move all newly created
      bundles into another machine after each backup.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">info is a very important file which contains all global
      repository metadata, such as chunk and bundle sizes, and an encryption key
      encrypted with the user password. It is paramount not to lose it, so
      backing it up separately somewhere might be a good idea. On the other
      hand, if you absolutely don't trust your remote storage provider, you
      might consider not storing it with the rest of the data. It would then be
      impossible to decrypt it at all, even if your password gets known
    later.</dd>
</dl>
<div class="Pp"></div>
The program does not have any facilities for sending your backup over the
  network. You can rsync the repo to another computer or use any kind of cloud
  storage capable of storing files. Since zbackup never modifies any existing
  files, the latter is especially easy -- just tell the upload tool you use not
  to upload any files which already exist on the remote side (e.g. with gsutil
  it's
  gsutil&#x00A0;cp&#x00A0;-R&#x00A0;-n&#x00A0;/my/backup&#x00A0;gs:/mybackup/).
<div class="Pp"></div>
To aid with creating backups, there's an utility called tartool included with
  zbackup. The idea is the following: one sprinkles empty files called .backup
  and .no-backup across the entire filesystem. Directories where .backup files
  are placed are marked for backing up. Similarly, directories with .no-backup
  files are marked not to be backed up. Additionally, it is possible to place
  .backup-XYZ in the same directory where XYZ is to mark XYZ for backing up, or
  place .no-backup-XYZ to mark it not to be backed up. Then tartool can be run
  with three arguments -- the root directory to start from (can be /), the
  output includes file, and the output excludes file. The tool traverses over
  the given directory noting the .backup* and .no-backup* files and creating
  include and exclude lists for the tar utility. The tar utility could then be
  run as
  tar&#x00A0;c&#x00A0;--files-from&#x00A0;includes&#x00A0;--exclude-from&#x00A0;excludes
  to store all chosen data.
<h1 class="Sh" title="Sh" id="Scalability"><a class="selflink" href="#Scalability">Scalability</a></h1>
This section tries do address the question on the maximum amount of data which
  can be held in a backup repository. What is meant here is the deduplicated
  data. The number of bytes in all source files ever fed into the repository
  doesn't matter, but the total size of the resulting repository does.
<div class="Pp"></div>
Internally all input data is split into small blocks called chunks (up to 64k
  each by default). Chunks are collected into bundles (up to 2MB each by
  default), and those bundles are then compressed and encrypted.
<div class="Pp"></div>
There are then two problems with the total number of chunks in the repository:
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Hashes of all existing chunks are needed to be kept in RAM
      while the backup is ongoing. Since the sliding window performs checking
      with a single-byte granularity, lookups would otherwise be too slow. The
      amount of data needed to be stored is technically only 24 bytes for each
      chunk, where the size of the chunk is up to 64k. In an example real-life
      18GB repo, only 18MB are taken by in its hash index. Multiply this roughly
      by two to have an estimate of RAM needed to store this index as an in-RAM
      hash table. However, as this size is proportional to the total size of the
      repo, for 2TB repo you could already require 2GB of RAM. Most repos are
      much smaller though, and as long as the deduplication works properly, in
      many cases you can store terabytes of highly-redundant backup files in a
      20GB repo easily.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">We use a 64-bit rolling hash, which allows to have an O(1)
      lookup cost at each byte we process. Due to birthday paradox
      (https://en.wikipedia.org/wiki/Birthday_paradox), we would start having
      collisions when we approach 2^32 hashes. If each chunk we have is 32k on
      average, we would get there when our repo grows to 128TB. We would still
      be able to continue, but as the number of collisions would grow, we would
      have to resort to calculating the full hash of a block at each byte more
      and more often, which would result in a considerable slowdown.</dd>
</dl>
<div class="Pp"></div>
All in all, as long as the amount of RAM permits, one can go up to several
  terabytes in deduplicated data, and start having some slowdown after having
  hundreds of terabytes, RAM-permitting.
<h1 class="Sh" title="Sh" id="Design_choices"><a class="selflink" href="#Design_choices">Design
  choices</a></h1>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">We use a 64-bit modified Rabin-Karp rolling hash (see
      rolling_hash.hh for details), while most other programs use a 32-bit one.
      As noted previously, one problem with the hash size is its birthday bound,
      which with the 32-bit hash is met after having only 2^16 hashes. The
      choice of a 64-bit hash allows us to scale much better while having
      virtually the same calculation cost on a typical 64-bit machine.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">rsync uses MD5 as its strong hash. While MD5 is known to be
      fast, it is also known to be broken, allowing a malicious user to craft
      colliding inputs. zbackup uses SHA1 instead. The cost of SHA1 calculations
      on modern machines is actually less than that of MD5 (run
      openssl&#x00A0;speed&#x00A0;md5&#x00A0;sha1 on yours), so it's a win-win
      situation. We only keep the first 128 bits of the SHA1 output, and
      therefore together with the rolling hash we have a 192-bit hash for each
      chunk. It's a multiple of 8 bytes which is a nice properly on 64-bit
      machines, and it is long enough not to worry about possible
    collisions.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">AES-128 in CBC mode with PKCS#7 padding is used for
      encryption. This seems to be a reasonbly safe classic solution. Each
      encrypted file has a random IV as its first 16 bytes.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">We use Google's protocol buffers
      (https://developers.google.com/protocol-buffers/) to represent data
      structures in binary form. They are very efficient and relatively simple
      to use.</dd>
</dl>
<h1 class="Sh" title="Sh" id="Compression"><a class="selflink" href="#Compression">Compression</a></h1>
zbackup uses LZMA to compress stored data. It compresses very well, but it will
  slow down your backup
<div class="Pp"></div>
(unless you have a very fast CPU).
<div class="Pp"></div>
LZO is much faster, but the files will be bigger. If you don't
<div class="Pp"></div>
want your backup process to be cpu-bound, you should consider using LZO.
  However, there are some caveats:
<dl class="Bl-tag">
  <dt class="It-tag">1.</dt>
  <dd class="It-tag">LZO is so fast that other parts of zbackup consume
      significant portions of the CPU. In fact, it is only</dd>
</dl>
<div class="Pp"></div>
using one core on my machine because compression is the only thing that can run
  in parallel.
<dl class="Bl-tag">
  <dt class="It-tag">2.</dt>
  <dd class="It-tag">I've hacked the LZO support in a day. You shouldn't trust
      it. Please make sure that restore works before</dd>
</dl>
<div class="Pp"></div>
you assume that your data is safe. That may still be faster than a backup with
  LZMA ;-)
<dl class="Bl-tag">
  <dt class="It-tag">3.</dt>
  <dd class="It-tag">LZMA is still the default, so make sure that you use the
      --compression&#x00A0;lzo argument when you init the</dd>
</dl>
<div class="Pp"></div>
repo or whenever you do a backup.
<div class="Pp"></div>
You can mix LZMA and LZO in a repository. Each bundle file has a field that says
  how it was compressed, so
<div class="Pp"></div>
zbackup will use the right method to decompress it. You could use an old zbackup
  respository with only LZMA
<div class="Pp"></div>
bundles and start using LZO. However, please think twice before you do that
  because old versions of zbackup
<div class="Pp"></div>
won't be able to read those bundles.
<h1 class="Sh" title="Sh" id="Improvements"><a class="selflink" href="#Improvements">Improvements</a></h1>
There's a lot to be improved in the program. It was released with the minimum
  amount of functionality to be useful. It is also stable. This should hopefully
  stimulate people to join the development and add all those other fancy
  features. Here's a list of ideas:
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Additional options, such as configurable chunk and bundle
      sizes etc.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">A command to change password.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Improved garbage collection. The program should support
      ability to specify maximum index file size / maximum index file count (for
      better compatibility with cloud storages as well) or something like
      retention policy.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">A command to fsck the repo by doing something close to what
      garbage collection does, but also checking all hashes and so on.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Parallel decompression. Right now decompression is
      single-threaded, but it is possible to look ahead in the stream and
      perform prefetching.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Support for mounting the repo over FUSE. Random access to
      data would then be possible.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Support for exposing a backed up file over a userspace NBD
      server. It would then be possible to mount raw disk images without
      extracting them.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Support for other encryption types (preferably for
      everything openssl supports with its evp).</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Support for other compression methods.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">You name it!</dd>
</dl>
<h1 class="Sh" title="Sh" id="Communication"><a class="selflink" href="#Communication">Communication</a></h1>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">The program's website is at
    &lt;http://zbackup.org/&gt;.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Development happens at
      &lt;https://github.com/zbackup/zbackup&gt;.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Discussion forum is at
      &lt;https://groups.google.com/forum/#!forum/zbackup&gt;. Please ask for
      help there!</dd>
</dl>
<div class="Pp"></div>
The author is reachable over email at &lt;ikm@zbackup.org&gt;. Please be
  constructive and don't ask for help using the program, though. In most cases
  it's best to stick to the forum, unless you have something to discuss with the
  author in private.
<h1 class="Sh" title="Sh" id="Similar_projects"><a class="selflink" href="#Similar_projects">Similar
  projects</a></h1>
zbackup is certainly not the first project to embrace the idea of using a
  rolling hash for deduplication. Here's a list of other projects the author
  found on the web:
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">bup (https://github.com/bup/bup), based on storing data in
      git packs. No possibility of removing old data. This program was the
      initial inspiration for zbackup.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">ddar (http://www.synctus.com/ddar/), seems to be a little
      bit outdated. Contains a nice list of alternatives with comparisons.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">rdiff-backup (http://www.nongnu.org/rdiff-backup/), based
      on the original rsync algorithm. Does not do global deduplication, only
      working over the files with the same file name.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">duplicity (http://duplicity.nongnu.org/), which looks
      similar to rdiff-backup with regards to mode of operation.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Some filesystems (most notably ZFS
      (http://en.wikipedia.org/wiki/ZFS) and Btrfs
      (http://en.wikipedia.org/wiki/Btrfs)) provide deduplication features. They
      do so only at block level though, without a sliding window, so they can
      not accomodate to arbitrary byte insertion/deletion in the middle of
    data.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag">&#x2022;</dt>
  <dd class="It-tag">Attic (https://attic-backup.org/), which looks very similar
      to zbackup.</dd>
</dl>
<h1 class="Sh" title="Sh" id="Credits"><a class="selflink" href="#Credits">Credits</a></h1>
Copyright (c) 2012-2014 Konstantin Isakov (&lt;ikm@zbackup.org&gt;) and ZBackup
  contributors, see CONTRIBUTORS. Licensed under GNU GPLv2 or later + OpenSSL,
  see LICENSE.
<div class="Pp"></div>
This program is free software; you can redistribute it and/or modify it under
  the terms of the GNU General Public License as published by the Free Software
  Foundation; either version 2 of the License, or (at your option) any later
  version.
<div class="Pp"></div>
This program is distributed in the hope that it will be useful, but WITHOUT ANY
  WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR
  A PARTICULAR PURPOSE. See the GNU General Public License for more details.
<div class="Pp"></div>
You should have received a copy of the GNU General Public License along with
  this program; if not, write to the Free Software Foundation, Inc., 51 Franklin
  Street, Fifth Floor, Boston, MA 02110-1301 USA.</div>
<table class="foot">
  <tr>
    <td class="foot-date">February 12, 2017</td>
    <td class="foot-os"></td>
  </tr>
</table>
</body>
</html>
