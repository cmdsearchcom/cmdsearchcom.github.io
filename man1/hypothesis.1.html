<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 16:17:38 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>HYPOTHESIS(1) Hypothesis HYPOTHESIS(1)</p>

<p style="margin-top: 1em">NAME <br>
hypothesis - Hypothesis Documentation</p>

<p style="margin-top: 1em">Hypothesis is a Python library
for creating unit tests which are simpler to write and more
powerful when run, finding edge cases in your code you
wouldn&rsquo;t have thought to look <br>
for. It is stable, powerful and easy to add to any existing
test suite.</p>

<p style="margin-top: 1em">It works by letting you write
tests that assert that something should be true for every
case, not just the ones you happen to think of.</p>

<p style="margin-top: 1em">Think of a normal unit test as
being something like the following:</p>

<p style="margin-top: 1em">1. Set up some data.</p>

<p style="margin-top: 1em">2. Perform some operations on
the data.</p>

<p style="margin-top: 1em">3. Assert something about the
result.</p>

<p style="margin-top: 1em">Hypothesis lets you write tests
which instead look like this:</p>

<p style="margin-top: 1em">1. For all data matching some
specification.</p>

<p style="margin-top: 1em">2. Perform some operations on
the data.</p>

<p style="margin-top: 1em">3. Assert something about the
result.</p>

<p style="margin-top: 1em">This is often called property
based testing, and was popularised by the Haskell library
Quickcheck.</p>

<p style="margin-top: 1em">It works by generating random
data matching your specification and checking that your
guarantee still holds in that case. If it finds an example
where it doesn&rsquo;t, it takes that <br>
example and cuts it down to size, simplifying it until it
finds a much smaller example that still causes the problem.
It then saves that example for later, so that once it has
<br>
found a problem with your code it will not forget it in the
future.</p>

<p style="margin-top: 1em">Writing tests of this form
usually consists of deciding on guarantees that your code
should make - properties that should always hold true,
regardless of what the world throws at <br>
you. Examples of such guarantees might be:</p>

<p style="margin-top: 1em">&Acirc;&middot; Your code
shouldn&rsquo;t throw an exception, or should only throw a
particular type of exception (this works particularly well
if you have a lot of internal assertions).</p>

<p style="margin-top: 1em">&Acirc;&middot; If you delete an
object, it is no longer visible.</p>

<p style="margin-top: 1em">&Acirc;&middot; If you serialize
and then deserialize a value, then you get the same value
back.</p>

<p style="margin-top: 1em">Now you know the basics of what
Hypothesis does, the rest of this documentation will take
you through how and why. It&rsquo;s divided into a number of
sections, which you can see in <br>
the sidebar (or the menu at the top if you&rsquo;re on
mobile), but you probably want to begin with the Quick start
guide, which will give you a worked example of how to use
Hypothesis <br>
and a detailed outline of the things you need to know to
begin testing your code with it, or check out some of the
introductory articles.</p>

<p style="margin-top: 1em">QUICK START GUIDE <br>
This document should talk you through everything you need to
get started with Hypothesis.</p>

<p style="margin-top: 1em">An example <br>
Suppose we&rsquo;ve written a run length encoding system and
we want to test it out.</p>

<p style="margin-top: 1em">We have the following code which
I took straight from the Rosetta Code wiki (OK, I removed
some commented out code and fixed the formatting, but there
are no functional modifica&acirc; <br>
tions):</p>

<p style="margin-top: 1em">def encode(input_string): <br>
count = 1 <br>
prev = &rsquo;&rsquo; <br>
lst = [] <br>
for character in input_string: <br>
if character != prev: <br>
if prev: <br>
entry = (prev, count) <br>
lst.append(entry) <br>
count = 1 <br>
prev = character <br>
else: <br>
count += 1 <br>
else: <br>
entry = (character, count) <br>
lst.append(entry) <br>
return lst</p>

<p style="margin-top: 1em">def decode(lst): <br>
q = &rsquo;&rsquo; <br>
for character, count in lst: <br>
q += character * count <br>
return q</p>

<p style="margin-top: 1em">We want to write a test for this
that will check some invariant of these functions.</p>

<p style="margin-top: 1em">The invariant one tends to try
when you&rsquo;ve got this sort of encoding / decoding is
that if you encode something and then decode it then you get
the same value back.</p>

<p style="margin-top: 1em">Lets see how you&rsquo;d do that
with Hypothesis:</p>

<p style="margin-top: 1em">from hypothesis import given
<br>
from hypothesis.strategies import text</p>

<p style="margin-top: 1em">@given(text()) <br>
def test_decode_inverts_encode(s): <br>
assert decode(encode(s)) == s</p>

<p style="margin-top: 1em">(For this example we&rsquo;ll
just let pytest discover and run the test. We&rsquo;ll cover
other ways you could have run it later).</p>

<p style="margin-top: 1em">The text function returns what
Hypothesis calls a search strategy. An object with methods
that describe how to generate and simplify certain kinds of
values. The @given decorator <br>
then takes our test function and turns it into a
parametrized one which, when called, will run the test
function over a wide range of matching data from that
strategy.</p>

<p style="margin-top: 1em">Anyway, this test immediately
finds a bug in the code:</p>

<p style="margin-top: 1em">Falsifying example:
test_decode_inverts_encode(s=&rsquo;&rsquo;)</p>

<p style="margin-top: 1em">UnboundLocalError: local
variable &rsquo;character&rsquo; referenced before
assignment</p>

<p style="margin-top: 1em">Hypothesis correctly points out
that this code is simply wrong if called on an empty
string.</p>

<p style="margin-top: 1em">If we fix that by just adding
the following code to the beginning of the function then
Hypothesis tells us the code is correct (by doing nothing as
you&rsquo;d expect a passing test <br>
to).</p>

<p style="margin-top: 1em">if not input_string: <br>
return []</p>

<p style="margin-top: 1em">If we wanted to make sure this
example was always checked we could add it in
explicitly:</p>

<p style="margin-top: 1em">from hypothesis import given,
example <br>
from hypothesis.strategies import text</p>

<p style="margin-top: 1em">@given(text()) <br>
@example(&rsquo;&rsquo;) <br>
def test_decode_inverts_encode(s): <br>
assert decode(encode(s)) == s</p>

<p style="margin-top: 1em">You don&rsquo;t have to do this,
but it can be useful both for clarity purposes and for
reliably hitting hard to find examples. Also in local
development Hypothesis will just remember <br>
and reuse the examples anyway, but there&rsquo;s not
currently a very good workflow for sharing those in your
CI.</p>

<p style="margin-top: 1em">It&rsquo;s also worth noting
that both example and given support keyword arguments as
well as positional. The following would have worked just as
well:</p>

<p style="margin-top: 1em">@given(s=text()) <br>
@example(s=&rsquo;&rsquo;) <br>
def test_decode_inverts_encode(s): <br>
assert decode(encode(s)) == s</p>

<p style="margin-top: 1em">Suppose we had a more
interesting bug and forgot to reset the count each time. Say
we missed a line in our encode method:</p>

<p style="margin-top: 1em">def encode(input_string): <br>
count = 1 <br>
prev = &rsquo;&rsquo; <br>
lst = [] <br>
for character in input_string: <br>
if character != prev: <br>
if prev: <br>
entry = (prev, count) <br>
lst.append(entry) <br>
# count = 1 # Missing reset operation <br>
prev = character <br>
else: <br>
count += 1 <br>
else: <br>
entry = (character, count) <br>
lst.append(entry) <br>
return lst</p>

<p style="margin-top: 1em">Hypothesis quickly informs us of
the following example:</p>

<p style="margin-top: 1em">Falsifying example:
test_decode_inverts_encode(s=&rsquo;001&rsquo;)</p>

<p style="margin-top: 1em">Note that the example provided
is really quite simple. Hypothesis doesn&rsquo;t just find
any counter-example to your tests, it knows how to simplify
the examples it finds to produce <br>
small easy to understand ones. In this case, two identical
values are enough to set the count to a number different
from one, followed by another distinct value which should
have <br>
reset the count but in this case didn&rsquo;t.</p>

<p style="margin-top: 1em">The examples Hypothesis provides
are valid Python code you can run. Any arguments that you
explicitly provide when calling the function are not
generated by Hypothesis, and if <br>
you explicitly provide all the arguments Hypothesis will
just call the underlying function the once rather than
running it multiple times.</p>

<p style="margin-top: 1em">Installing <br>
Hypothesis is available on pypi as &quot;hypothesis&quot;.
You can install it with:</p>

<p style="margin-top: 1em">pip install hypothesis</p>

<p style="margin-top: 1em">If you want to install directly
from the source code (e.g. because you want to make changes
and install the changed version) you can do this with:</p>

<p style="margin-top: 1em">pip install -e .</p>

<p style="margin-top: 1em">You should probably run the
tests first to make sure nothing is broken. You can do this
with:</p>

<p style="margin-top: 1em">python setup.py test</p>

<p style="margin-top: 1em">Note that if they&rsquo;re not
already installed this will try to install the test
dependencies.</p>

<p style="margin-top: 1em">You may wish to do all of this
in a virtualenv. For example:</p>

<p style="margin-top: 1em">virtualenv venv <br>
source venv/bin/activate <br>
pip install hypothesis</p>

<p style="margin-top: 1em">Will create an isolated
environment for you to try hypothesis out in without
affecting your system installed packages.</p>

<p style="margin-top: 1em">Running tests <br>
In our example above we just let pytest discover and run our
tests, but we could also have run it explicitly
ourselves:</p>

<p style="margin-top: 1em">if __name__ ==
&rsquo;__main__&rsquo;: <br>
test_decode_inverts_encode()</p>

<p style="margin-top: 1em">We could also have done this as
a unittest TestCase:</p>

<p style="margin-top: 1em">import unittest</p>

<p style="margin-top: 1em">class
TestEncoding(unittest.TestCase): <br>
@given(text()) <br>
def test_decode_inverts_encode(self, s): <br>
self.assertEqual(decode(encode(s)), s)</p>

<p style="margin-top: 1em">if __name__ ==
&rsquo;__main__&rsquo;: <br>
unittest.main()</p>

<p style="margin-top: 1em">A detail: This works because
Hypothesis ignores any arguments it hasn&rsquo;t been told
to provide (positional arguments start from the right), so
the self argument to the test is sim&acirc; <br>
ply ignored and works as normal. This also means that
Hypothesis will play nicely with other ways of
parameterizing tests. e.g it works fine if you use pytest
fixtures for some <br>
arguments and Hypothesis for others.</p>

<p style="margin-top: 1em">Writing tests <br>
A test in Hypothesis consists of two parts: A function that
looks like a normal test in your test framework of choice
but with some additional arguments, and a @given decorator
<br>
that specifies how to provide those arguments.</p>

<p style="margin-top: 1em">Here are some other examples of
how you could use that:</p>

<p style="margin-top: 1em">from hypothesis import given
<br>
import hypothesis.strategies as st</p>

<p style="margin-top: 1em">@given(st.integers(),
st.integers()) <br>
def test_ints_are_commutative(x, y): <br>
assert x + y == y + x</p>

<p style="margin-top: 1em">@given(x=st.integers(),
y=st.integers()) <br>
def test_ints_cancel(x, y): <br>
assert (x + y) - y == x</p>

<p style="margin-top: 1em">@given(st.lists(st.integers()))
<br>
def test_reversing_twice_gives_same_list(xs): <br>
# This will generate lists of arbitrary length (usually
between 0 and <br>
# 100 elements) whose elements are integers. <br>
ys = list(xs) <br>
ys.reverse() <br>
ys.reverse() <br>
assert xs == ys</p>

<p style="margin-top: 1em">@given(st.tuples(st.booleans(),
st.text())) <br>
def test_look_tuples_work_too(t): <br>
# A tuple is generated as the one you provided, with the
corresponding <br>
# types in those positions. <br>
assert len(t) == 2 <br>
assert isinstance(t[0], bool) <br>
assert isinstance(t[1], str)</p>

<p style="margin-top: 1em">Note that as we saw in the above
example you can pass arguments to @given either as
positional or as keywords.</p>

<p style="margin-top: 1em">Where to start <br>
You should now know enough of the basics to write some tests
for your code using Hypothesis. The best way to learn is by
doing, so go have a try.</p>

<p style="margin-top: 1em">If you&rsquo;re stuck for ideas
for how to use this sort of test for your code, here are
some good starting points:</p>

<p style="margin-top: 1em">1. Try just calling functions
with appropriate random data and see if they crash. You may
be surprised how often this works. e.g. note that the first
bug we found in the encoding <br>
example didn&rsquo;t even get as far as our assertion: It
crashed because it couldn&rsquo;t handle the data we gave
it, not because it did the wrong thing.</p>

<p style="margin-top: 1em">2. Look for duplication in your
tests. Are there any cases where you&rsquo;re testing the
same thing with multiple different examples? Can you
generalise that to a single test using <br>
Hypothesis?</p>

<p style="margin-top: 1em">3. This piece is designed for an
F# implementation, but is still very good advice which you
may find helps give you good ideas for using Hypothesis.</p>

<p style="margin-top: 1em">If you have any trouble getting
started, don&rsquo;t feel shy about asking for help.</p>

<p style="margin-top: 1em">DETAILS AND ADVANCED FEATURES
<br>
This is an account of slightly less common Hypothesis
features that you don&rsquo;t need to get started but will
nevertheless make your life easier.</p>

<p style="margin-top: 1em">Additional test output <br>
Normally the output of a failing test will look something
like:</p>

<p style="margin-top: 1em">Falsifying example:
test_a_thing(x=1, y=&quot;foo&quot;)</p>

<p style="margin-top: 1em">With the repr of each keyword
argument being printed.</p>

<p style="margin-top: 1em">Sometimes this isn&rsquo;t
enough, either because you have values with a repr that
isn&rsquo;t very descriptive or because you need to see the
output of some intermediate steps of your test. <br>
That&rsquo;s where the note function comes in:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; from hypothesis
import given, note, strategies as st <br>
&gt;&gt;&gt; @given(st.lists(st.integers()), st.randoms())
<br>
... def test_shuffle_is_noop(ls, r): <br>
... ls2 = list(ls) <br>
... r.shuffle(ls2) <br>
... note(&quot;Shuffle: %r&quot; % (ls2)) <br>
... assert ls == ls2 <br>
... <br>
&gt;&gt;&gt; try: <br>
... test_shuffle_is_noop() <br>
... except AssertionError: <br>
... print(&rsquo;ls != ls2&rsquo;) <br>
Falsifying example: test_shuffle_is_noop(ls=[0, 0, 1],
r=RandomWithSeed(0)) <br>
Shuffle: [0, 1, 0] <br>
ls != ls2</p>

<p style="margin-top: 1em">The note is printed in the final
run of the test in order to include any additional
information you might need in your test.</p>

<p style="margin-top: 1em">Test Statistics <br>
If you are using py.test you can see a number of statistics
about the executed tests by passing the command line
argument --hypothesis-show-statistics. This will include
some <br>
general statistics about the test:</p>

<p style="margin-top: 1em">For example if you ran the
following with --hypothesis-show-statistics:</p>

<p style="margin-top: 1em">from hypothesis import given,
strategies as st</p>

<p style="margin-top: 1em">@given(st.integers()) <br>
def test_integers(i): <br>
pass</p>

<p style="margin-top: 1em">You would see:</p>

<p style="margin-top: 1em">test_integers:</p>

<p style="margin-top: 1em">- 200 passing examples, 0
failing examples, 0 invalid examples <br>
- Typical runtimes: &lt; 1ms <br>
- Stopped because settings.max_examples=200</p>

<p style="margin-top: 1em">The final &quot;Stopped
because&quot; line is particularly important to note: It
tells you the setting value that determined when the test
should stop trying new examples. This can be use&acirc; <br>
ful for understanding the behaviour of your tests. Ideally
you&rsquo;d always want this to be max_examples.</p>

<p style="margin-top: 1em">In some cases (such as filtered
and recursive strategies) you will see events mentioned
which describe some aspect of the data generation:</p>

<p style="margin-top: 1em">from hypothesis import given,
strategies as st</p>


<p style="margin-top: 1em">@given(st.integers().filter(lambda
x: x % 2 == 0)) <br>
def test_even_integers(i): <br>
pass</p>

<p style="margin-top: 1em">You would see something
like:</p>

<p style="margin-top: 1em">test_even_integers:</p>

<p style="margin-top: 1em">- 200 passing examples, 0
failing examples, 16 invalid examples <br>
- Typical runtimes: &lt; 1ms <br>
- Stopped because settings.max_examples=200 <br>
- Events: <br>
* 30.56%, Retried draw from integers().filter(lambda x: x %
2 == 0) to satisfy filter <br>
* 7.41%, Aborted test because unable to satisfy
integers().filter(lambda x: x % 2 == 0)</p>

<p style="margin-top: 1em">hypothesis.event(value) <br>
Record an event that occurred this test. Statistics on
number of test runs with each event will be reported at the
end if you run Hypothesis in statistics reporting mode.</p>

<p style="margin-top: 1em">Events should be strings or
convertable to them.</p>

<p style="margin-top: 1em">You can also mark custom events
in a test using the &rsquo;event&rsquo; function:</p>

<p style="margin-top: 1em">from hypothesis import given,
event, strategies as st</p>


<p style="margin-top: 1em">@given(st.integers().filter(lambda
x: x % 2 == 0)) <br>
def test_even_integers(i): <br>
event(&quot;i mod 3 = %d&quot; % (i % 3,))</p>

<p style="margin-top: 1em">You will then see output
like:</p>

<p style="margin-top: 1em">test_even_integers:</p>

<p style="margin-top: 1em">- 200 passing examples, 0
failing examples, 28 invalid examples <br>
- Typical runtimes: &lt; 1ms <br>
- Stopped because settings.max_examples=200 <br>
- Events: <br>
* 47.81%, Retried draw from integers().filter(lambda x: x %
2 == 0) to satisfy filter <br>
* 31.14%, i mod 3 = 2 <br>
* 28.95%, i mod 3 = 1 <br>
* 27.63%, i mod 3 = 0 <br>
* 12.28%, Aborted test because unable to satisfy
integers().filter(lambda x: x % 2 == 0)</p>

<p style="margin-top: 1em">Arguments to event() can be any
hashable type, but two events will be considered the same if
they are the same when converted to a string with str().</p>

<p style="margin-top: 1em">Making assumptions <br>
Sometimes Hypothesis doesn&rsquo;t give you exactly the
right sort of data you want - it&rsquo;s mostly of the right
shape, but some examples won&rsquo;t work and you
don&rsquo;t want to care about <br>
them. You can just ignore these by aborting the test early,
but this runs the risk of accidentally testing a lot less
than you think you are. Also it would be nice to spend less
<br>
time on bad examples - if you&rsquo;re running 200 examples
per test (the default) and it turns out 150 of those
examples don&rsquo;t match your needs, that&rsquo;s a lot of
wasted time.</p>

<p style="margin-top: 1em">hypothesis.assume(condition)
<br>
assume() is like an assert that marks the example as bad,
rather than failing the test.</p>

<p style="margin-top: 1em">This allows you to specify
properties that you assume will be true, and let Hypothesis
try to avoid similar examples in future.</p>

<p style="margin-top: 1em">For example suppose had the
following test:</p>

<p style="margin-top: 1em">@given(floats()) <br>
def test_negation_is_self_inverse(x): <br>
assert x == -(-x)</p>

<p style="margin-top: 1em">Running this gives us:</p>

<p style="margin-top: 1em">Falsifying example:
test_negation_is_self_inverse(x=float(&rsquo;nan&rsquo;))
<br>
AssertionError</p>

<p style="margin-top: 1em">This is annoying. We know about
NaN and don&rsquo;t really care about it, but as soon as
Hypothesis finds a NaN example it will get distracted by
that and tell us about it. Also the <br>
test will fail and we want it to pass.</p>

<p style="margin-top: 1em">So lets block off this
particular example:</p>

<p style="margin-top: 1em">from math import isnan</p>

<p style="margin-top: 1em">@given(floats()) <br>
def test_negation_is_self_inverse_for_non_nan(x): <br>
assume(not isnan(x)) <br>
assert x == -(-x)</p>

<p style="margin-top: 1em">And this passes without a
problem.</p>

<p style="margin-top: 1em">In order to avoid the easy trap
where you assume a lot more than you intended, Hypothesis
will fail a test when it can&rsquo;t find enough examples
passing the assumption.</p>

<p style="margin-top: 1em">If we&rsquo;d written:</p>

<p style="margin-top: 1em">@given(floats()) <br>
def test_negation_is_self_inverse_for_non_nan(x): <br>
assume(False) <br>
assert x == -(-x)</p>

<p style="margin-top: 1em">Then on running we&rsquo;d have
got the exception:</p>

<p style="margin-top: 1em">Unsatisfiable: Unable to satisfy
assumptions of hypothesis
test_negation_is_self_inverse_for_non_nan. Only 0 examples
considered satisfied assumptions</p>

<p style="margin-top: 1em">How good is assume? <br>
Hypothesis has an adaptive exploration strategy to try to
avoid things which falsify assumptions, which should
generally result in it still being able to find examples in
hard to <br>
find situations.</p>

<p style="margin-top: 1em">Suppose we had the
following:</p>

<p style="margin-top: 1em">@given(lists(integers())) <br>
def test_sum_is_positive(xs): <br>
assert sum(xs) &gt; 0</p>

<p style="margin-top: 1em">Unsurprisingly this fails and
gives the falsifying example [].</p>

<p style="margin-top: 1em">Adding assume(xs) to this
removes the trivial empty example and gives us [0].</p>

<p style="margin-top: 1em">Adding assume(all(x &gt; 0 for x
in xs)) and it passes: A sum of a list of positive integers
is positive.</p>

<p style="margin-top: 1em">The reason that this should be
surprising is not that it doesn&rsquo;t find a
counter-example, but that it finds enough examples at
all.</p>

<p style="margin-top: 1em">In order to make sure something
interesting is happening, suppose we wanted to try this for
long lists. e.g. suppose we added an assume(len(xs) &gt; 10)
to it. This should basi&acirc; <br>
cally never find an example: A naive strategy would find
fewer than one in a thousand examples, because if each
element of the list is negative with probability half,
you&rsquo;d have <br>
to have ten of these go the right way by chance. In the
default configuration Hypothesis gives up long before
it&rsquo;s tried 1000 examples (by default it tries
200).</p>

<p style="margin-top: 1em">Here&rsquo;s what happens if we
try to run this:</p>

<p style="margin-top: 1em">@given(lists(integers())) <br>
def test_sum_is_positive(xs): <br>
assume(len(xs) &gt; 10) <br>
assume(all(x &gt; 0 for x in xs)) <br>
print(xs) <br>
assert sum(xs) &gt; 0</p>

<p style="margin-top: 1em">In: test_sum_is_positive() <br>
[17, 12, 7, 13, 11, 3, 6, 9, 8, 11, 47, 27, 1, 31, 1] <br>
[6, 2, 29, 30, 25, 34, 19, 15, 50, 16, 10, 3, 16] <br>
[25, 17, 9, 19, 15, 2, 2, 4, 22, 10, 10, 27, 3, 1, 14, 17,
13, 8, 16, 9, 2... <br>
[17, 65, 78, 1, 8, 29, 2, 79, 28, 18, 39] <br>
[13, 26, 8, 3, 4, 76, 6, 14, 20, 27, 21, 32, 14, 42, 9, 24,
33, 9, 5, 15, ... <br>
[2, 1, 2, 2, 3, 10, 12, 11, 21, 11, 1, 16]</p>

<p style="margin-top: 1em">As you can see, Hypothesis
doesn&rsquo;t find many examples here, but it finds some -
enough to keep it happy.</p>

<p style="margin-top: 1em">In general if you can shape your
strategies better to your tests you should - for example
integers(1, 1000) is a lot better than assume(1 &lt;= x
&lt;= 1000), but assume will take you <br>
a long way if you can&rsquo;t.</p>

<p style="margin-top: 1em">Defining strategies <br>
The type of object that is used to explore the examples
given to your test function is called a SearchStrategy.
These are created using the functions exposed in the
hypothe&acirc; <br>
sis.strategies module.</p>

<p style="margin-top: 1em">Many of these strategies expose
a variety of arguments you can use to customize generation.
For example for integers you can specify min and max values
of integers you want. If <br>
you want to see exactly what a strategy produces you can ask
for an example:</p>

<p style="margin-top: 1em">&gt;&gt;&gt;
integers(min_value=0, max_value=10).example() <br>
5</p>

<p style="margin-top: 1em">Many strategies are build out of
other strategies. For example, if you want to define a tuple
you need to say what goes in each element:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; from
hypothesis.strategies import tuples <br>
&gt;&gt;&gt; tuples(integers(), integers()).example() <br>
(50, 15)</p>

<p style="margin-top: 1em">Further details are available in
a separate document.</p>

<p style="margin-top: 1em">The gory details of given
parameters <br>
hypothesis.given(*given_arguments, **given_kwargs) <br>
A decorator for turning a test function that accepts
arguments into a randomized test.</p>

<p style="margin-top: 1em">This is the main entry point to
Hypothesis.</p>

<p style="margin-top: 1em">The @given decorator may be used
to specify what arguments of a function should be
parametrized over. You can use either positional or keyword
arguments or a mixture of the two.</p>

<p style="margin-top: 1em">For example all of the following
are valid uses:</p>

<p style="margin-top: 1em">@given(integers(), integers())
<br>
def a(x, y): <br>
pass</p>

<p style="margin-top: 1em">@given(integers()) <br>
def b(x, y): <br>
pass</p>

<p style="margin-top: 1em">@given(y=integers()) <br>
def c(x, y): <br>
pass</p>

<p style="margin-top: 1em">@given(x=integers()) <br>
def d(x, y): <br>
pass</p>

<p style="margin-top: 1em">@given(x=integers(),
y=integers()) <br>
def e(x, **kwargs): <br>
pass</p>

<p style="margin-top: 1em">@given(x=integers(),
y=integers()) <br>
def f(x, *args, **kwargs): <br>
pass</p>

<p style="margin-top: 1em">class SomeTest(TestCase): <br>
@given(integers()) <br>
def test_a_thing(self, x): <br>
pass</p>

<p style="margin-top: 1em">The following are not:</p>

<p style="margin-top: 1em">@given(integers(), integers(),
integers()) <br>
def g(x, y): <br>
pass</p>

<p style="margin-top: 1em">@given(integers()) <br>
def h(x, *args): <br>
pass</p>

<p style="margin-top: 1em">@given(integers(), x=integers())
<br>
def i(x, y): <br>
pass</p>

<p style="margin-top: 1em">@given() <br>
def j(x, y): <br>
pass</p>

<p style="margin-top: 1em">The rules for determining what
are valid uses of given are as follows:</p>

<p style="margin-top: 1em">1. You may pass any keyword
argument to given.</p>

<p style="margin-top: 1em">2. Positional arguments to given
are equivalent to the rightmost named arguments for the test
function.</p>

<p style="margin-top: 1em">3. positional arguments may not
be used if the underlying test function has varargs or
arbitrary keywords.</p>

<p style="margin-top: 1em">4. Functions tested with given
may not have any defaults.</p>

<p style="margin-top: 1em">The reason for the
&quot;rightmost named arguments&quot; behaviour is so that
using @given with instance methods works: self will be
passed to the function as normal and not be <br>
parametrized over.</p>

<p style="margin-top: 1em">The function returned by given
has all the arguments that the original test did , minus the
ones that are being filled in by given.</p>

<p style="margin-top: 1em">Custom function execution <br>
Hypothesis provides you with a hook that lets you control
how it runs examples.</p>

<p style="margin-top: 1em">This lets you do things like set
up and tear down around each example, run examples in a
subprocess, transform coroutine tests into normal tests,
etc.</p>

<p style="margin-top: 1em">The way this works is by
introducing the concept of an executor. An executor is
essentially a function that takes a block of code and run
it. The default executor is:</p>

<p style="margin-top: 1em">def default_executor(function):
<br>
return function()</p>

<p style="margin-top: 1em">You define executors by defining
a method execute_example on a class. Any test methods on
that class with @given used on them will use
self.execute_example as an executor with <br>
which to run tests. For example, the following executor runs
all its code twice:</p>

<p style="margin-top: 1em">from unittest import
TestCase</p>

<p style="margin-top: 1em">class
TestTryReallyHard(TestCase): <br>
@given(integers()) <br>
def test_something(self, i): <br>
perform_some_unreliable_operation(i)</p>

<p style="margin-top: 1em">def execute_example(self, f):
<br>
f() <br>
return f()</p>

<p style="margin-top: 1em">Note: The functions you use in
map, etc. will run inside the executor. i.e. they will not
be called until you invoke the function passed to
setup_example.</p>

<p style="margin-top: 1em">An executor must be able to
handle being passed a function which returns None, otherwise
it won&rsquo;t be able to run normal test cases. So for
example the following executor is <br>
invalid:</p>

<p style="margin-top: 1em">from unittest import
TestCase</p>

<p style="margin-top: 1em">class TestRunTwice(TestCase):
<br>
def execute_example(self, f): <br>
return f()()</p>

<p style="margin-top: 1em">and should be rewritten as:</p>

<p style="margin-top: 1em">from unittest import TestCase
<br>
import inspect</p>

<p style="margin-top: 1em">class TestRunTwice(TestCase):
<br>
def execute_example(self, f): <br>
result = f() <br>
if inspect.isfunction(result): <br>
result = result() <br>
return result</p>

<p style="margin-top: 1em">Using Hypothesis to find values
<br>
You can use Hypothesis&rsquo;s data exploration features to
find values satisfying some predicate. This is generally
useful for exploring custom strategies defined with
@composite, or <br>
experimenting with conditions for filtering data.</p>

<p style="margin-top: 1em">hypothesis.find(specifier,
condition, settings=None, random=None, database_key=None)
<br>
Returns the minimal example from the given strategy
specifier that matches the predicate function condition.</p>

<p style="margin-top: 1em">&gt;&gt;&gt; from hypothesis
import find <br>
&gt;&gt;&gt; from hypothesis.strategies import sets, lists,
integers <br>
&gt;&gt;&gt; find(lists(integers()), lambda x: sum(x) &gt;=
10) <br>
[10] <br>
&gt;&gt;&gt; find(lists(integers()), lambda x: sum(x) &gt;=
10 and len(x) &gt;= 3) <br>
[0, 0, 10] <br>
&gt;&gt;&gt; find(sets(integers()), lambda x: sum(x) &gt;=
10 and len(x) &gt;= 3) <br>
{0, 1, 9}</p>

<p style="margin-top: 1em">The first argument to find()
describes data in the usual way for an argument to given,
and supports all the same data types. The second is a
predicate it must satisfy.</p>

<p style="margin-top: 1em">Of course not all conditions are
satisfiable. If you ask Hypothesis for an example to a
condition that is always false it will raise an error:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; find(integers(),
lambda x: False) <br>
Traceback (most recent call last): <br>
... <br>
hypothesis.errors.NoSuchExample: No examples of condition
lambda x: &lt;unknown&gt;</p>

<p style="margin-top: 1em">(The lambda x: unknown is
because Hypothesis can&rsquo;t retrieve the source code of
lambdas from the interactive python console. It gives a
better error message most of the time which <br>
contains the actual condition)</p>

<p style="margin-top: 1em">Providing explicit examples <br>
You can explicitly ask Hypothesis to try a particular
example, using</p>

<p style="margin-top: 1em">hypothesis.example(*args,
**kwargs) <br>
A decorator to that ensures a specific example is always
tested.</p>

<p style="margin-top: 1em">Hypothesis will run all examples
you&rsquo;ve asked for first. If any of them fail it will
not go on to look for more examples.</p>

<p style="margin-top: 1em">It doesn&rsquo;t matter whether
you put the example decorator before or after given. Any
permutation of the decorators in the above will do the same
thing.</p>

<p style="margin-top: 1em">Note that examples can be
positional or keyword based. If they&rsquo;re positional
then they will be filled in from the right when calling, so
either of the following styles will work <br>
as expected:</p>

<p style="margin-top: 1em">@given(text()) <br>
@example(&quot;Hello world&quot;) <br>
@example(x=&quot;Some very long string&quot;) <br>
def test_some_code(x): <br>
assert True</p>

<p style="margin-top: 1em">from unittest import
TestCase</p>

<p style="margin-top: 1em">class TestThings(TestCase): <br>
@given(text()) <br>
@example(&quot;Hello world&quot;) <br>
@example(x=&quot;Some very long string&quot;) <br>
def test_some_code(self, x): <br>
assert True</p>

<p style="margin-top: 1em">It is not permitted for a single
example to be a mix of positional and keyword arguments.
Either are fine, and you can use one in one example and the
other in another example if <br>
for some reason you really want to, but a single example
must be consistent.</p>

<p style="margin-top: 1em">SETTINGS <br>
Hypothesis tries to have good defaults for its behaviour,
but sometimes that&rsquo;s not enough and you need to tweak
it.</p>

<p style="margin-top: 1em">The mechanism for doing this is
the settings object. You can set up a @given based test to
use this using a settings decorator:</p>

<p style="margin-top: 1em">@given invocation as
follows:</p>

<p style="margin-top: 1em">from hypothesis import given,
settings</p>

<p style="margin-top: 1em">@given(integers()) <br>
@settings(max_examples=500) <br>
def test_this_thoroughly(x): <br>
pass</p>

<p style="margin-top: 1em">This uses a settings object
which causes the test to receive a much larger set of
examples than normal.</p>

<p style="margin-top: 1em">This may be applied either
before or after the given and the results are the same. The
following is exactly equivalent:</p>

<p style="margin-top: 1em">from hypothesis import given,
settings</p>

<p style="margin-top: 1em">@settings(max_examples=500) <br>
@given(integers()) <br>
def test_this_thoroughly(x): <br>
pass</p>

<p style="margin-top: 1em">Available settings <br>
class hypothesis.settings(parent=None, **kwargs) <br>
A settings object controls a variety of parameters that are
used in falsification. These may control both the
falsification strategy and the details of the data that is
<br>
generated.</p>

<p style="margin-top: 1em">Default values are picked up
from the settings.default object and changes made there will
be picked up in newly created settings.</p>

<p style="margin-top: 1em">database_file <br>
database: An instance of hypothesis.database.ExampleDatabase
that will be used to save examples to and load previous
examples from. May be None in which case no <br>
storage will be used. default value: (dynamically
calculated)</p>

<p style="margin-top: 1em">database <br>
An ExampleDatabase instance to use for storage of examples.
May be None.</p>

<p style="margin-top: 1em">If this was explicitly set at
settings instantiation then that value will be used (even if
it was None). If not and the database_file setting is not
None this will <br>
be lazily loaded as an ExampleDatabase using that file the
first time this property is accessed on a particular
thread.</p>

<p style="margin-top: 1em">buffer_size <br>
The size of the underlying data used to generate examples.
If you need to generate really large examples you may want
to increase this, but it will make your tests <br>
slower. default value: 8192</p>

<p style="margin-top: 1em">max_examples <br>
Once this many satisfying examples have been considered
without finding any counter-example, falsification will
terminate. default value: 200</p>

<p style="margin-top: 1em">max_iterations <br>
Once this many iterations of the example loop have run,
including ones which failed to satisfy assumptions and ones
which produced duplicates, falsification will <br>
terminate. default value: 1000</p>

<p style="margin-top: 1em">max_shrinks <br>
Once this many successful shrinks have been performed,
Hypothesis will assume something has gone a bit wrong and
give up rather than continuing to try to shrink the <br>
example. default value: 500</p>

<p style="margin-top: 1em">min_satisfying_examples <br>
Raise Unsatisfiable for any tests which do not produce at
least this many values that pass all assume() calls and
which have not exhaustively covered the search <br>
space. default value: 5</p>

<p style="margin-top: 1em">perform_health_check <br>
If set to True, Hypothesis will run a preliminary health
check before attempting to actually execute your test.
default value: True</p>

<p style="margin-top: 1em">stateful_step_count <br>
Number of steps to run a stateful program for before giving
up on it breaking. default value: 50</p>

<p style="margin-top: 1em">strict If set to True, anything
that would cause Hypothesis to issue a warning will instead
raise an error. Note that new warnings may be added at any
time, so running <br>
with strict set to True means that new Hypothesis releases
may validly break your code.</p>

<p style="margin-top: 1em">You can enable this setting
temporarily by setting the HYPOTHESIS_STRICT_MODE
environment variable to the string &rsquo;true&rsquo;.
default value: False</p>

<p style="margin-top: 1em">suppress_health_check <br>
A list of health checks to disable default value: []</p>

<p style="margin-top: 1em">timeout <br>
Once this many seconds have passed, falsify will terminate
even if it has not found many examples. This is a soft
rather than a hard limit - Hypothesis won&rsquo;t e.g. <br>
interrupt execution of the called function to stop it. If
this value is &lt;= 0 then no timeout will be applied.
default value: 60</p>

<p style="margin-top: 1em">Seeing intermediate result <br>
To see what&rsquo;s going on while Hypothesis runs your
tests, you can turn up the verbosity setting. This works
with both find() and @given.</p>

<p style="margin-top: 1em">&gt;&gt;&gt; from hypothesis
import find, settings, Verbosity <br>
&gt;&gt;&gt; from hypothesis.strategies import lists,
booleans <br>
&gt;&gt;&gt; find(lists(integers()), any,
settings=settings(verbosity=Verbosity.verbose)) <br>
Found satisfying example [-208] <br>
Shrunk example to [-208] <br>
Shrunk example to [208] <br>
Shrunk example to [1] <br>
[1]</p>

<p style="margin-top: 1em">The four levels are quiet,
normal, verbose and debug. normal is the default, while in
quiet Hypothesis will not print anything out, even the final
falsifying example. debug is <br>
basically verbose but a bit more so. You probably
don&rsquo;t want it.</p>

<p style="margin-top: 1em">You can also override the
default by setting the environment variable
HYPOTHESIS_VERBOSITY_LEVEL to the name of the level you
want. So e.g. setting HYPOTHESIS_VER&acirc; <br>
BOSITY_LEVEL=verbose will run all your tests printing
intermediate results and errors.</p>

<p style="margin-top: 1em">Building settings objects <br>
settings can be created by calling settings with any of the
available settings values. Any absent ones will be set to
defaults:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; from hypothesis
import settings <br>
&gt;&gt;&gt; settings() <br>
settings(buffer_size=8192, database_file=&rsquo;...&rsquo;,
derandomize=False, <br>
max_examples=200, max_iterations=1000, max_mutations=10,
<br>
max_shrinks=500, min_satisfying_examples=5,
perform_health_check=True, <br>
phases=..., report_statistics=..., stateful_step_count=50,
strict=..., <br>
suppress_health_check=[], timeout=60,
verbosity=Verbosity.normal) <br>
&gt;&gt;&gt; settings().max_examples <br>
200 <br>
&gt;&gt;&gt; settings(max_examples=10).max_examples <br>
10</p>

<p style="margin-top: 1em">You can also copy settings off
other settings:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; s =
settings(max_examples=10) <br>
&gt;&gt;&gt; t = settings(s, max_iterations=20) <br>
&gt;&gt;&gt; s.max_examples <br>
10 <br>
&gt;&gt;&gt; t.max_iterations <br>
20 <br>
&gt;&gt;&gt; s.max_iterations <br>
1000 <br>
&gt;&gt;&gt; s.max_shrinks <br>
500 <br>
&gt;&gt;&gt; t.max_shrinks <br>
500</p>

<p style="margin-top: 1em">Default settings <br>
At any given point in your program there is a current
default settings, available as settings.default. As well as
being a settings object in its own right, all newly created
set&acirc; <br>
tings objects which are not explicitly based off another
settings are based off the default, so will inherit any
values that are not explicitly set from it.</p>

<p style="margin-top: 1em">You can change the defaults by
using profiles (see next section), but you can also override
them locally by using a settings object as a context
manager</p>

<p style="margin-top: 1em">&gt;&gt;&gt; with
settings(max_examples=150): <br>
... print(settings.default.max_examples) <br>
... print(settings().max_examples) <br>
150 <br>
150 <br>
&gt;&gt;&gt; settings().max_examples <br>
200</p>

<p style="margin-top: 1em">Note that after the block exits
the default is returned to normal.</p>

<p style="margin-top: 1em">You can use this by nesting test
definitions inside the context:</p>

<p style="margin-top: 1em">from hypothesis import given,
settings</p>

<p style="margin-top: 1em">with settings(max_examples=500):
<br>
@given(integers()) <br>
def test_this_thoroughly(x): <br>
pass</p>

<p style="margin-top: 1em">All settings objects created or
tests defined inside the block will inherit their defaults
from the settings object used as the context. You can still
override them with custom <br>
defined settings of course.</p>

<p style="margin-top: 1em">Warning: If you use define test
functions which don&rsquo;t use @given inside a context
block, these will not use the enclosing settings. This is
because the context manager only <br>
affects the definition, not the execution of the
function.</p>

<p style="margin-top: 1em">settings Profiles <br>
Depending on your environment you may want different default
settings. For example: during development you may want to
lower the number of examples to speed up the tests.
How&acirc; <br>
ever, in a CI environment you may want more examples so you
are more likely to find bugs.</p>

<p style="margin-top: 1em">Hypothesis allows you to define
different settings profiles. These profiles can be loaded at
any time.</p>

<p style="margin-top: 1em">Loading a profile changes the
default settings but will not change the behavior of tests
that explicitly change the settings.</p>

<p style="margin-top: 1em">&gt;&gt;&gt; from hypothesis
import settings <br>
&gt;&gt;&gt; settings.register_profile(&quot;ci&quot;,
settings(max_examples=1000)) <br>
&gt;&gt;&gt; settings().max_examples <br>
200 <br>
&gt;&gt;&gt; settings.load_profile(&quot;ci&quot;) <br>
&gt;&gt;&gt; settings().max_examples <br>
1000</p>

<p style="margin-top: 1em">Instead of loading the profile
and overriding the defaults you can retrieve profiles for
specific tests.</p>

<p style="margin-top: 1em">&gt;&gt;&gt; with
settings.get_profile(&quot;ci&quot;): <br>
... print(settings().max_examples) <br>
... <br>
1000</p>

<p style="margin-top: 1em">Optionally, you may define the
environment variable to load a profile for you. This is the
suggested pattern for running your tests on CI. The code
below should run in a con&acirc; <br>
ftest.py or any setup/initialization section of your test
suite. If this variable is not defined the Hypothesis
defined defaults will be loaded.</p>

<p style="margin-top: 1em">&gt;&gt;&gt; import os <br>
&gt;&gt;&gt; from hypothesis import settings <br>
&gt;&gt;&gt; settings.register_profile(&quot;ci&quot;,
settings(max_examples=1000)) <br>
&gt;&gt;&gt; settings.register_profile(&quot;dev&quot;,
settings(max_examples=10)) <br>
&gt;&gt;&gt; settings.register_profile(&quot;debug&quot;,
settings(max_examples=10, verbosity=Verbosity.verbose)) <br>
&gt;&gt;&gt;
settings.load_profile(os.getenv(u&rsquo;HYPOTHESIS_PROFILE&rsquo;,
&rsquo;default&rsquo;))</p>

<p style="margin-top: 1em">If you are using the hypothesis
pytest plugin and your profiles are registered by your
conftest you can load one with the command line option
--hypothesis-profile.</p>

<p style="margin-top: 1em">$ py.test tests
--hypothesis-profile &lt;profile-name&gt;</p>

<p style="margin-top: 1em">WHAT YOU CAN GENERATE AND HOW
<br>
The general philosophy of Hypothesis data generation is that
everything should be possible to generate and most things
should be easy. Most things in the standard library is more
<br>
aspirational than achieved, the state of the art is already
pretty good.</p>

<p style="margin-top: 1em">This document is a guide to what
strategies are available for generating data and how to
build them. Strategies have a variety of other important
internal features, such as how <br>
they simplify, but the data they can generate is the only
public part of their API.</p>

<p style="margin-top: 1em">Functions for building
strategies are all available in the hypothesis.strategies
module. The salient functions from it are as follows:</p>

<p style="margin-top: 1em">hypothesis.strategies.nothing()
<br>
This strategy never successfully draws a value and will
always reject on an attempt to draw.</p>


<p style="margin-top: 1em">hypothesis.strategies.just(value)
<br>
Return a strategy which only generates value.</p>

<p style="margin-top: 1em">Note: value is not copied. Be
wary of using mutable values.</p>

<p style="margin-top: 1em">hypothesis.strategies.none()
<br>
Return a strategy which only generates None.</p>


<p style="margin-top: 1em">hypothesis.strategies.one_of(*args)
<br>
Return a strategy which generates values from any of the
argument strategies.</p>

<p style="margin-top: 1em">This may be called with one
iterable argument instead of multiple strategy arguments. In
which case one_of(x) and one_of(*x) are equivalent.</p>


<p style="margin-top: 1em">hypothesis.strategies.integers(min_value=None,
max_value=None) <br>
Returns a strategy which generates integers (in Python 2
these may be ints or longs).</p>

<p style="margin-top: 1em">If min_value is not None then
all values will be &gt;= min_value. If max_value is not None
then all values will be &lt;= max_value</p>


<p style="margin-top: 1em">hypothesis.strategies.booleans()
<br>
Returns a strategy which generates instances of bool.</p>


<p style="margin-top: 1em">hypothesis.strategies.floats(min_value=None,
max_value=None, allow_nan=None, allow_infinity=None) <br>
Returns a strategy which generates floats.</p>

<p style="margin-top: 1em">&Acirc;&middot; If min_value is
not None, all values will be &gt;= min_value.</p>

<p style="margin-top: 1em">&Acirc;&middot; If max_value is
not None, all values will be &lt;= max_value.</p>

<p style="margin-top: 1em">&Acirc;&middot; If min_value or
max_value is not None, it is an error to enable
allow_nan.</p>

<p style="margin-top: 1em">&Acirc;&middot; If both
min_value and max_value are not None, it is an error to
enable allow_infinity.</p>

<p style="margin-top: 1em">Where not explicitly ruled out
by the bounds, all of infinity, -infinity and NaN are
possible values generated by this strategy.</p>


<p style="margin-top: 1em">hypothesis.strategies.complex_numbers()
<br>
Returns a strategy that generates complex numbers.</p>


<p style="margin-top: 1em">hypothesis.strategies.tuples(*args)
<br>
Return a strategy which generates a tuple of the same length
as args by generating the value at index i from args[i].</p>

<p style="margin-top: 1em">e.g. tuples(integers(),
integers()) would generate a tuple of length two with both
values an integer.</p>


<p style="margin-top: 1em">hypothesis.strategies.sampled_from(elements)
<br>
Returns a strategy which generates any value present in the
iterable elements.</p>

<p style="margin-top: 1em">Note that as with just, values
will not be copied and thus you should be careful of using
mutable data.</p>


<p style="margin-top: 1em">hypothesis.strategies.lists(elements=None,
min_size=None, average_size=None, max_size=None,
unique_by=None, unique=False) <br>
Returns a list containing values drawn from elements length
in the interval [min_size, max_size] (no bounds in that
direction if these are None). If max_size is 0 then <br>
elements may be None and only the empty list will be
drawn.</p>

<p style="margin-top: 1em">average_size may be used as a
size hint to roughly control the size of list but it may not
be the actual average of sizes you get, due to a variety of
factors.</p>

<p style="margin-top: 1em">If unique is True (or something
that evaluates to True), we compare direct object equality,
as if unique_by was lambda x: x. This comparison only works
for hashable types.</p>

<p style="margin-top: 1em">if unique_by is not None it must
be a function returning a hashable type when given a value
drawn from elements. The resulting list will satisfy the
condition that for i <br>
!= j, unique_by(result[i]) != unique_by(result[j]).</p>


<p style="margin-top: 1em">hypothesis.strategies.sets(elements=None,
min_size=None, average_size=None, max_size=None) <br>
This has the same behaviour as lists, but returns sets
instead.</p>

<p style="margin-top: 1em">Note that Hypothesis cannot tell
if values are drawn from elements are hashable until running
the test, so you can define a strategy for sets of an
unhashable type but it <br>
will fail at test time.</p>


<p style="margin-top: 1em">hypothesis.strategies.frozensets(elements=None,
min_size=None, average_size=None, max_size=None) <br>
This is identical to the sets function but instead returns
frozensets.</p>


<p style="margin-top: 1em">hypothesis.strategies.iterables(elements=None,
min_size=None, average_size=None, max_size=None,
unique_by=None, unique=False) <br>
This has the same behaviour as lists, but returns iterables
instead.</p>

<p style="margin-top: 1em">Some iterables cannot be indexed
(e.g. sets) and some do not have a fixed length (e.g.
generators). This strategy produces iterators, which cannot
be indexed and do not <br>
have a fixed length. This ensures that you do not
accidentally depend on sequence behaviour.</p>


<p style="margin-top: 1em">hypothesis.strategies.fixed_dictionaries(mapping)
<br>
Generate a dictionary of the same type as mapping with a
fixed set of keys mapping to strategies. mapping must be a
dict subclass.</p>

<p style="margin-top: 1em">Generated values have all keys
present in mapping, with the corresponding values drawn from
mapping[key]. If mapping is an instance of OrderedDict the
keys will also be in <br>
the same order, otherwise the order is arbitrary.</p>


<p style="margin-top: 1em">hypothesis.strategies.dictionaries(keys,
values, dict_class=&lt;type &rsquo;dict&rsquo;&gt;,
min_size=None, average_size=None, max_size=None) <br>
Generates dictionaries of type dict_class with keys drawn
from the keys argument and values drawn from the values
argument.</p>

<p style="margin-top: 1em">The size parameters have the
same interpretation as for lists.</p>


<p style="margin-top: 1em">hypothesis.strategies.streaming(elements)
<br>
Generates an infinite stream of values where each value is
drawn from elements.</p>

<p style="margin-top: 1em">The result is iterable (the
iterator will never terminate) and indexable.</p>


<p style="margin-top: 1em">hypothesis.strategies.characters(whitelist_categories=None,
blacklist_categories=None, blacklist_characters=None,
min_codepoint=None, max_codepoint=None) <br>
Generates unicode text type (unicode on python 2, str on
python 3) characters following specified filtering
rules.</p>

<p style="margin-top: 1em">This strategy accepts lists of
Unicode categories, characters of which should
(whitelist_categories) or should not (blacklist_categories)
be produced.</p>

<p style="margin-top: 1em">Also there could be applied
limitation by minimal and maximal produced code point of the
characters.</p>

<p style="margin-top: 1em">If you know what exactly
characters you don&rsquo;t want to be produced, pass them
with blacklist_characters argument.</p>


<p style="margin-top: 1em">hypothesis.strategies.text(alphabet=None,
min_size=None, average_size=None, max_size=None) <br>
Generates values of a unicode text type (unicode on python
2, str on python 3) with values drawn from alphabet, which
should be an iterable of length one strings or a <br>
strategy generating such. If it is None it will default to
generating the full unicode range. If it is an empty
collection this will only generate empty strings.</p>

<p style="margin-top: 1em">min_size, max_size and
average_size have the usual interpretations.</p>


<p style="margin-top: 1em">hypothesis.strategies.binary(min_size=None,
average_size=None, max_size=None) <br>
Generates the appropriate binary type (str in python 2,
bytes in python 3).</p>

<p style="margin-top: 1em">min_size, average_size and
max_size have the usual interpretations.</p>

<p style="margin-top: 1em">hypothesis.strategies.randoms()
<br>
Generates instances of Random (actually a Hypothesis
specific RandomWithSeed class which displays what it was
initially seeded with)</p>


<p style="margin-top: 1em">hypothesis.strategies.random_module()
<br>
If your code depends on the global random module then you
need to use this.</p>

<p style="margin-top: 1em">It will explicitly seed the
random module at the start of your test so that tests are
reproducible. The value it passes you is an opaque object
whose only useful feature <br>
is that its repr displays the random seed. It is not itself
a random number generator. If you want a random number
generator you should use the randoms() strategy which <br>
will give you one.</p>


<p style="margin-top: 1em">hypothesis.strategies.builds(target,
*args, **kwargs) <br>
Generates values by drawing from args and kwargs and passing
them to target in the appropriate argument position.</p>

<p style="margin-top: 1em">e.g. builds(target, integers(),
flag=booleans()) would draw an integer i and a boolean b and
call target(i, flag=b).</p>


<p style="margin-top: 1em">hypothesis.strategies.fractions(min_value=None,
max_value=None, max_denominator=None) <br>
Returns a strategy which generates Fractions.</p>

<p style="margin-top: 1em">If min_value is not None then
all generated values are no less than min_value.</p>

<p style="margin-top: 1em">If max_value is not None then
all generated values are no greater than max_value.</p>

<p style="margin-top: 1em">If max_denominator is not None
then the absolute value of the denominator of any generated
values is no greater than max_denominator. Note that
max_denominator must be at <br>
least 1.</p>


<p style="margin-top: 1em">hypothesis.strategies.decimals(min_value=None,
max_value=None, allow_nan=None, allow_infinity=None,
places=None) <br>
Generates instances of decimals.Decimal, which may be:</p>

<p style="margin-top: 1em">&Acirc;&middot; A finite
rational number, between min_value and max_value.</p>

<p style="margin-top: 1em">&Acirc;&middot; Not a Number, if
allow_nan is True. None means &quot;allow NaN, unless
min__value and max_value are not None&quot;.</p>

<p style="margin-top: 1em">&Acirc;&middot; Positive or
negative infinity, if max_value and min_value respectively
are None, and allow_infinity is not False. None means
&quot;allow infinity, unless excluded by the min <br>
and max values&quot;.</p>

<p style="margin-top: 1em">Note that where floats have one
NaN value, Decimals have four: signed, and either quiet or
signalling. See the decimal module docs for more information
on special values.</p>

<p style="margin-top: 1em">If places is not None, all
finite values drawn from the strategy will have that number
of digits after the decimal place.</p>


<p style="margin-top: 1em">hypothesis.strategies.recursive(base,
extend, max_leaves=100) <br>
base: A strategy to start from.</p>

<p style="margin-top: 1em">extend: A function which takes a
strategy and returns a new strategy.</p>

<p style="margin-top: 1em">max_leaves: The maximum number
of elements to be drawn from base on a given run.</p>

<p style="margin-top: 1em">This returns a strategy S such
that S = extend(base | S). That is, values maybe drawn from
base, or from any strategy reachable by mixing applications
of | and extend.</p>

<p style="margin-top: 1em">An example may clarify:
recursive(booleans(), lists) would return a strategy that
may return arbitrarily nested and mixed lists of booleans.
So e.g. False, [True], <br>
[False, []], [[[[True]]]], are all valid values to be drawn
from that strategy.</p>


<p style="margin-top: 1em">hypothesis.strategies.permutations(values)
<br>
Return a strategy which returns permutations of the
collection &quot;values&quot;.</p>


<p style="margin-top: 1em">hypothesis.strategies.datetimes(min_datetime=datetime.datetime(1,
1, 1, 0, 0), max_datetime=datetime.datetime(9999, 12, 31,
23, 59, 59, 999999), timezones=none()) <br>
A strategy for generating datetimes, which may be
timezone-aware.</p>

<p style="margin-top: 1em">This strategy works by drawing a
naive datetime between min_datetime and max_datetime, which
must both be naive (have no timezone).</p>

<p style="margin-top: 1em">timezones must be a strategy
that generates tzinfo objects (or None, which is valid for
naive datetimes). A value drawn from this strategy will be
added to a naive date&acirc; <br>
time, and the resulting tz-aware datetime returned.</p>

<p style="margin-top: 1em">NOTE: <br>
tz-aware datetimes from this strategy may be ambiguous or
non-existent due to daylight savings, leap seconds, timezone
and calendar adjustments, etc. This is inten&acirc; <br>
tional, as malformed timestamps are a common source of
bugs.</p>

<p style="margin-top: 1em">hypothesis.extra.timezones()
requires the pytz package, but provides all timezones in the
Olsen database. If you also want to allow naive datetimes,
combine strategies <br>
like none() | timezones().</p>

<p style="margin-top: 1em">Alternatively, you can create a
list of the timezones you wish to allow (e.g. from the
standard library, datetutil, or pytz) and use
sampled_from(). Ensure that simple <br>
values such as None or UTC are at the beginning of the list
for proper minimisation.</p>


<p style="margin-top: 1em">hypothesis.strategies.dates(min_date=datetime.date(1,
1, 1), max_date=datetime.date(9999, 12, 31)) <br>
A strategy for dates between min_date and max_date.</p>


<p style="margin-top: 1em">hypothesis.strategies.times(min_time=datetime.time(0,
0), max_time=datetime.time(23, 59, 59, 999999),
timezones=none()) <br>
A strategy for times between min_time and max_time.</p>

<p style="margin-top: 1em">The timezones argument is
handled as for datetimes().</p>


<p style="margin-top: 1em">hypothesis.strategies.timedeltas(min_delta=datetime.timedelta(-999999999),
max_delta=datetime.timedelta(999999999, 86399, 999999)) <br>
A strategy for timedeltas between min_delta and
max_delta.</p>


<p style="margin-top: 1em">hypothesis.strategies.composite(f)
<br>
Defines a strategy that is built out of potentially
arbitrarily many other strategies.</p>

<p style="margin-top: 1em">This is intended to be used as a
decorator. See the full documentation for more details about
how to use this function.</p>


<p style="margin-top: 1em">hypothesis.strategies.shared(base,
key=None) <br>
Returns a strategy that draws a single shared value per run,
drawn from base. Any two shared instances with the same key
will share the same value, otherwise the identity <br>
of this strategy will be used. That is:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; s = integers() # or
any other strategy <br>
&gt;&gt;&gt; x = shared(s) <br>
&gt;&gt;&gt; y = shared(s)</p>

<p style="margin-top: 1em">In the above x and y may draw
different (or potentially the same) values. In the following
they will always draw the same:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; x = shared(s,
key=&quot;hi&quot;) <br>
&gt;&gt;&gt; y = shared(s, key=&quot;hi&quot;)</p>

<p style="margin-top: 1em">hypothesis.strategies.choices()
<br>
Strategy that generates a function that behaves like
random.choice.</p>

<p style="margin-top: 1em">Will note choices made for
reproducibility.</p>

<p style="margin-top: 1em">hypothesis.strategies.uuids()
<br>
Returns a strategy that generates UUIDs.</p>

<p style="margin-top: 1em">All returned values from this
will be unique, so e.g. if you do lists(uuids()) the
resulting list will never contain duplicates.</p>


<p style="margin-top: 1em">hypothesis.strategies.runner(default=not_set)
<br>
A strategy for getting &quot;the current test runner&quot;,
whatever that may be. The exact meaning depends on the entry
point, but it will usually be the associated
&rsquo;self&rsquo; value <br>
for it.</p>

<p style="margin-top: 1em">If there is no current test
runner and a default is provided, return that default. If no
default is provided, raises InvalidArgument.</p>

<p style="margin-top: 1em">hypothesis.strategies.data()
<br>
This isn&rsquo;t really a normal strategy, but instead gives
you an object which can be used to draw data interactively
from other strategies.</p>

<p style="margin-top: 1em">It can only be used within
@given, not find(). This is because the lifetime of the
object cannot outlast the test body.</p>

<p style="margin-top: 1em">See the rest of the
documentation for more complete information.</p>

<p style="margin-top: 1em">Choices <br>
Sometimes you need an input to be from a known set of items.
Hypothesis gives you two ways to do this. First up,
choice():</p>

<p style="margin-top: 1em">from hypothesis import given,
strategies as st</p>


<p style="margin-top: 1em">@given(user=st.text(min_size=1),
service=st.text(min_size=1), choice=st.choices()) <br>
def test_tickets(user, service, choice): <br>
t=choice((&rsquo;ST&rsquo;, &rsquo;LT&rsquo;,
&rsquo;TG&rsquo;, &rsquo;CT&rsquo;)) <br>
# asserts go here.</p>

<p style="margin-top: 1em">This means t will randomly be
one of the items in the list (&rsquo;ST&rsquo;,
&rsquo;LT&rsquo;, &rsquo;TG&rsquo;, &rsquo;CT&rsquo;), as if
you were calling python:random.choice() on the list.</p>

<p style="margin-top: 1em">A different, and probably better
way to do this, is to use sampled_from():</p>

<p style="margin-top: 1em">from hypothesis import given,
strategies as st</p>

<p style="margin-top: 1em">@given( <br>
user=st.text(min_size=1), service=st.text(min_size=1), <br>
t=st.sampled_from((&rsquo;ST&rsquo;, &rsquo;LT&rsquo;,
&rsquo;TG&rsquo;, &rsquo;CT&rsquo;))) <br>
def test_tickets(user, service, t): <br>
# asserts and test code go here.</p>

<p style="margin-top: 1em">Values from sampled_from() will
not be copied and thus you should be careful of using
mutable data. This is great for the above use case, but may
not always work out.</p>

<p style="margin-top: 1em">Infinite streams <br>
Sometimes you need examples of a particular type to keep
your test going but you&rsquo;re not sure how many
you&rsquo;ll need in advance. For this, we have streaming
types.</p>

<p style="margin-top: 1em">&gt;&gt;&gt; from
hypothesis.types import Stream <br>
&gt;&gt;&gt; x = Stream(iter(integers().example, None)) <br>
&gt;&gt;&gt; # Equivalent to
&lsquo;streaming(integers()).example()&lsquo;, which is not
supported <br>
&gt;&gt;&gt; x <br>
Stream(...) <br>
&gt;&gt;&gt; x[2] <br>
131 <br>
&gt;&gt;&gt; x <br>
Stream(-225, 50, 131, ...) <br>
&gt;&gt;&gt; x[10] <br>
127 <br>
&gt;&gt;&gt; x <br>
Stream(-225, 50, 131,
30781241791694610923869406150329382725, 89, 62248, 107,
35771, -113, 79, 127, ...)</p>

<p style="margin-top: 1em">Think of a Stream as an infinite
list where we&rsquo;ve only evaluated as much as we need to.
As per above, you can index into it and the stream will be
evaluated up to that index and <br>
no further.</p>

<p style="margin-top: 1em">You can iterate over it too
(warning: iter on a stream given to you by Hypothesis in
this way will never terminate):</p>

<p style="margin-top: 1em">&gt;&gt;&gt; it = iter(x) <br>
&gt;&gt;&gt; next(it) <br>
-225 <br>
&gt;&gt;&gt; next(it) <br>
50 <br>
&gt;&gt;&gt; next(it) <br>
131</p>

<p style="margin-top: 1em">Slicing will also work, and will
give you back Streams. If you set an upper bound then iter
on those streams will terminate:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; list(x[:5]) <br>
[-225, 50, 131, 30781241791694610923869406150329382725, 89]
<br>
&gt;&gt;&gt; y = x[1::2] <br>
&gt;&gt;&gt; y <br>
Stream(...) <br>
&gt;&gt;&gt; y[0] <br>
50 <br>
&gt;&gt;&gt; y[1] <br>
30781241791694610923869406150329382725 <br>
&gt;&gt;&gt; y <br>
Stream(50, 30781241791694610923869406150329382725, ...)</p>

<p style="margin-top: 1em">You can also apply a function to
transform a stream:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; t = x[20:] <br>
&gt;&gt;&gt; tm = t.map(lambda n: n * 2) <br>
&gt;&gt;&gt; tm[0] <br>
-344 <br>
&gt;&gt;&gt; t[0] <br>
-172 <br>
&gt;&gt;&gt; tm <br>
Stream(-344, ...) <br>
&gt;&gt;&gt; t <br>
Stream(-172, ...)</p>

<p style="margin-top: 1em">map creates a new stream where
each element of the stream is the function applied to the
corresponding element of the original stream. Evaluating the
new stream will force evalu&acirc; <br>
ating the original stream up to that index.</p>

<p style="margin-top: 1em">(Warning: This isn&rsquo;t the
map builtin. In Python 3 the builtin map should do more or
less the right thing, but in Python 2 it will never
terminate and will just eat up all your <br>
memory as it tries to build an infinitely long list)</p>

<p style="margin-top: 1em">These are the only operations a
Stream supports. There are a few more internal ones, but you
shouldn&rsquo;t rely on them.</p>

<p style="margin-top: 1em">Adapting strategies <br>
Often it is the case that a strategy doesn&rsquo;t produce
exactly what you want it to and you need to adapt it.
Sometimes you can do this in the test, but this hurts reuse
because you <br>
then have to repeat the adaption in every test.</p>

<p style="margin-top: 1em">Hypothesis gives you ways to
build strategies from other strategies given functions for
transforming the data.</p>

<p style="margin-top: 1em">Mapping <br>
Map is probably the easiest and most useful of these to use.
If you have a strategy s and a function f, then an example
s.map(f).example() is f(s.example()), i.e. we draw an <br>
example from s and then apply f to it.</p>

<p style="margin-top: 1em">e.g.:</p>

<p style="margin-top: 1em">&gt;&gt;&gt;
lists(integers()).map(sorted).example() <br>
[-224, -222, 16, 159, 120699286316048]</p>

<p style="margin-top: 1em">Note that many things that you
might use mapping for can also be done with
hypothesis.strategies.builds().</p>

<p style="margin-top: 1em">Filtering <br>
filter lets you reject some examples. s.filter(f).example()
is some example of s such that f(example) is truthy.</p>

<p style="margin-top: 1em">&gt;&gt;&gt;
integers().filter(lambda x: x &gt; 11).example() <br>
1609027033942695427531 <br>
&gt;&gt;&gt; integers().filter(lambda x: x &gt;
11).example() <br>
251</p>

<p style="margin-top: 1em">It&rsquo;s important to note
that filter isn&rsquo;t magic and if your condition is too
hard to satisfy then this can fail:</p>

<p style="margin-top: 1em">&gt;&gt;&gt;
integers().filter(lambda x: False).example() <br>
Traceback (most recent call last): <br>
... <br>
hypothesis.errors.NoExamples: Could not find any valid
examples in 20 tries</p>

<p style="margin-top: 1em">In general you should try to use
filter only to avoid corner cases that you don&rsquo;t want
rather than attempting to cut out a large chunk of the
search space.</p>

<p style="margin-top: 1em">A technique that often works
well here is to use map to first transform the data and then
use filter to remove things that didn&rsquo;t work out. So
for example if you wanted pairs of <br>
integers (x,y) such that x &lt; y you could do the
following:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; tuples(integers(),
integers()).map( <br>
... lambda x: tuple(sorted(x))).filter(lambda x: x[0] !=
x[1]).example() <br>
(180, 241)</p>

<p style="margin-top: 1em">Chaining strategies together
<br>
Finally there is flatmap. Flatmap draws an example, then
turns that example into a strategy, then draws an example
from that strategy.</p>

<p style="margin-top: 1em">It may not be obvious why you
want this at first, but it turns out to be quite useful
because it lets you generate different types of data with
relationships to eachother.</p>

<p style="margin-top: 1em">For example suppose we wanted to
generate a list of lists of the same length:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; rectangle_lists =
integers(min_value=0, max_value=10).flatmap( <br>
... lambda n: lists(lists(integers(), min_size=n,
max_size=n))) <br>
&gt;&gt;&gt; find(rectangle_lists, lambda x: True) <br>
[] <br>
&gt;&gt;&gt; find(rectangle_lists, lambda x: len(x) &gt;=
10) <br>
[[], [], [], [], [], [], [], [], [], []] <br>
&gt;&gt;&gt; find(rectangle_lists, lambda t: len(t) &gt;= 3
and len(t[0]) &gt;= 3) <br>
[[0, 0, 0], [0, 0, 0], [0, 0, 0]] <br>
&gt;&gt;&gt; find(rectangle_lists, lambda t: sum(len(s) for
s in t) &gt;= 10) <br>
[[0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]</p>

<p style="margin-top: 1em">In this example we first choose
a length for our tuples, then we build a strategy which
generates lists containing lists precisely of that length.
The finds show what simple <br>
examples for this look like.</p>

<p style="margin-top: 1em">Most of the time you probably
don&rsquo;t want flatmap, but unlike filter and map which
are just conveniences for things you could just do in your
tests, flatmap allows genuinely new <br>
data generation that you wouldn&rsquo;t otherwise be able to
easily do.</p>

<p style="margin-top: 1em">(If you know Haskell: Yes, this
is more or less a monadic bind. If you don&rsquo;t know
Haskell, ignore everything in these parentheses. You do not
need to understand anything about <br>
monads to use this, or anything else in Hypothesis).</p>

<p style="margin-top: 1em">Recursive data <br>
Sometimes the data you want to generate has a recursive
definition. e.g. if you wanted to generate JSON data, valid
JSON is:</p>

<p style="margin-top: 1em">1. Any float, any boolean, any
unicode string.</p>

<p style="margin-top: 1em">2. Any list of valid JSON
data</p>

<p style="margin-top: 1em">3. Any dictionary mapping
unicode strings to valid JSON data.</p>

<p style="margin-top: 1em">The problem is that you cannot
call a strategy recursively and expect it to not just blow
up and eat all your memory. The other problem here is that
not all unicode strings dis&acirc; <br>
play consistently on different machines, so we&rsquo;ll
restrict them in our doctest.</p>

<p style="margin-top: 1em">The way Hypothesis handles this
is with the recursive() function which you pass in a base
case and a function that given a strategy for your data type
returns a new strategy for <br>
it. So for example:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; from string import
printable; from pprint import pprint <br>
&gt;&gt;&gt; json = recursive(none() | booleans() | floats()
| text(printable), <br>
... lambda children: lists(children) |
dictionaries(text(printable), children)) <br>
&gt;&gt;&gt; pprint(json.example()) <br>
{&rsquo;&rsquo;:
&rsquo;Me$&rsquo;,5qPZ%etF:vL&rsquo;9gC&quot;: False, <br>
&quot; <br>
&rsquo;$KsT(( J/(wQ&rsquo;: [], <br>
&rsquo;0)G&amp;31&rsquo;: False, <br>
&rsquo;7&rsquo;: [], <br>
&rsquo;C.i]A-I&rsquo;: {&rsquo;:?Xh&gt;[;&rsquo;:
None,!b&rsquo;: -6.801160220000663e+18, <br>
&rsquo;YHT <br>
... <br>
&gt;&gt;&gt; pprint(json.example()) <br>
[{&quot;7_8&rsquo;qyb&quot;: None, <br>
&rsquo;:&rsquo;: -0.3641507440748771, <br>
&rsquo;TI_^0L{Tc&rsquo;: -0.0, <br>
&rsquo;ZiOqQ&rsquo;:
&rsquo;RKT*a]IjI/Zx2HB4ODiSUN)LsZ&rsquo;, <br>
&rsquo;n;E^^6|9=@g@@BmAi&rsquo;: &rsquo;7j5\&rsquo;}, <br>
True] <br>
&gt;&gt;&gt; pprint(json.example()) <br>
[]</p>

<p style="margin-top: 1em">That is, we start with our leaf
data and then we augment it by allowing lists and
dictionaries of anything we can generate as JSON data.</p>

<p style="margin-top: 1em">The size control of this works
by limiting the maximum number of values that can be drawn
from the base strategy. So for example if we wanted to only
generate really small JSON <br>
we could do this as:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; small_lists =
recursive(booleans(), lists, max_leaves=5) <br>
&gt;&gt;&gt; small_lists.example() <br>
True <br>
&gt;&gt;&gt; small_lists.example() <br>
[True, False] <br>
&gt;&gt;&gt; small_lists.example() <br>
True</p>

<p style="margin-top: 1em">Composite strategies <br>
The @composite decorator lets you combine other strategies
in more or less arbitrary ways. It&rsquo;s probably the main
thing you&rsquo;ll want to use for complicated custom
strategies.</p>

<p style="margin-top: 1em">The composite decorator works by
giving you a function as the first argument that you can use
to draw examples from other strategies. For example, the
following gives you a list <br>
and an index into it:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; @composite <br>
... def list_and_index(draw, elements=integers()): <br>
... xs = draw(lists(elements, min_size=1)) <br>
... i = draw(integers(min_value=0, max_value=len(xs) - 1))
<br>
... return (xs, i)</p>

<p style="margin-top: 1em">&rsquo;draw(s)&rsquo; is a
function that should be thought of as returning s.example(),
except that the result is reproducible and will minimize
correctly. The decorated function has the <br>
initial argument removed from the list, but will accept all
the others in the expected order. Defaults are
preserved.</p>

<p style="margin-top: 1em">&gt;&gt;&gt; list_and_index()
<br>
list_and_index() <br>
&gt;&gt;&gt; list_and_index().example() <br>
([215, 112], 0)</p>

<p style="margin-top: 1em">&gt;&gt;&gt;
list_and_index(booleans()) <br>
list_and_index(elements=booleans()) <br>
&gt;&gt;&gt; list_and_index(booleans()).example() <br>
([False, False], 1)</p>

<p style="margin-top: 1em">Note that the repr will work
exactly like it does for all the built-in strategies: It
will be a function that you can call to get the strategy in
question, with values provided <br>
only if they do not match the defaults.</p>

<p style="margin-top: 1em">You can use assume inside
composite functions:</p>

<p style="margin-top: 1em">@composite <br>
def distinct_strings_with_common_characters(draw): <br>
x = draw(text(), min_size=1) <br>
y = draw(text(alphabet=x)) <br>
assume(x != y) <br>
return (x, y)</p>

<p style="margin-top: 1em">This works as assume normally
would, filtering out any examples for which the passed in
argument is falsey.</p>

<p style="margin-top: 1em">Drawing interactively in tests
<br>
There is also the data() strategy, which gives you a means
of using strategies interactively. Rather than having to
specify everything up front in @given you can draw from <br>
strategies in the body of your test:</p>

<p style="margin-top: 1em">@given(data()) <br>
def test_draw_sequentially(data): <br>
x = data.draw(integers()) <br>
y = data.draw(integers(min_value=x)) <br>
assert x &lt; y</p>

<p style="margin-top: 1em">If the test fails, each draw
will be printed with the falsifying example. e.g. the above
is wrong (it has a boundary condition error), so will
print:</p>

<p style="margin-top: 1em">Falsifying example:
test_draw_sequentially(data=data(...)) <br>
Draw 1: 0 <br>
Draw 2: 0</p>

<p style="margin-top: 1em">As you can see, data drawn this
way is simplified as usual.</p>

<p style="margin-top: 1em">Test functions using the data()
strategy do not support explicit @example(...)s. In this
case, the best option is usually to construct your data with
@composite or the explicit <br>
example, and unpack this within the body of the test.</p>

<p style="margin-top: 1em">Optionally, you can provide a
label to identify values generated by each call to
data.draw(). These labels can be used to identify values in
the output of a falsifying example.</p>

<p style="margin-top: 1em">For instance:</p>

<p style="margin-top: 1em">@given(data()) <br>
def test_draw_sequentially(data): <br>
x = data.draw(integers(), label=&rsquo;First number&rsquo;)
<br>
y = data.draw(integers(min_value=x), label=&rsquo;Second
number&rsquo;) <br>
assert x &lt; y</p>

<p style="margin-top: 1em">will produce the output:</p>

<p style="margin-top: 1em">Falsifying example:
test_draw_sequentially(data=data(...)) <br>
Draw 1 (First number): 0 <br>
Draw 2 (Second number): 0</p>

<p style="margin-top: 1em">ADDITIONAL PACKAGES <br>
Hypothesis itself does not have any dependencies, but there
are some packages that need additional things installed in
order to work.</p>

<p style="margin-top: 1em">You can install these
dependencies using the setuptools extra feature as e.g. pip
install hypothesis[django]. This will check installation of
compatible versions.</p>

<p style="margin-top: 1em">You can also just install
hypothesis into a project using them, ignore the version
constraints, and hope for the best.</p>

<p style="margin-top: 1em">In general &quot;Which version
is Hypothesis compatible with?&quot; is a hard question to
answer and even harder to regularly test. Hypothesis is
always tested against the latest compati&acirc; <br>
ble version and each package will note the expected
compatibility range. If you run into a bug with any of these
please specify the dependency version.</p>

<p style="margin-top: 1em">hypothesis[pytz] <br>
This module provides pytz timezones.</p>

<p style="margin-top: 1em">You can use this strategy to
make hypothesis.strategies.datetimes() and
hypothesis.strategies.times() produce timezone-aware
values.</p>


<p style="margin-top: 1em">hypothesis.extra.pytz.timezones()
<br>
Any timezone in the Olsen database, as a pytz tzinfo
object.</p>

<p style="margin-top: 1em">This strategy minimises to UTC,
or the smallest possible fixed offset, and is designed for
use with hypothesis.strategies.datetimes().</p>

<p style="margin-top: 1em">hypothesis[datetime] <br>
This module provides deprecated time and date related
strategies.</p>

<p style="margin-top: 1em">It depends on the pytz package,
which is stable enough that almost any version should be
compatible - most updates are for the timezone database.</p>


<p style="margin-top: 1em">hypothesis.extra.datetime.datetimes(allow_naive=None,
timezones=None, min_year=None, max_year=None) <br>
Return a strategy for generating datetimes.</p>

<p style="margin-top: 1em">Deprecated since version 3.9.0:
use hypothesis.strategies.datetimes() instead.</p>

<p style="margin-top: 1em">allow_naive=True will cause the
values to sometimes be naive. timezones is the set of
permissible timezones. If set to an empty collection all
datetimes will be naive. If <br>
set to None all timezones available via pytz will be
used.</p>

<p style="margin-top: 1em">All generated datetimes will be
between min_year and max_year, inclusive.</p>


<p style="margin-top: 1em">hypothesis.extra.datetime.dates(min_year=None,
max_year=None) <br>
Return a strategy for generating dates.</p>

<p style="margin-top: 1em">Deprecated since version 3.9.0:
use hypothesis.strategies.dates() instead.</p>

<p style="margin-top: 1em">All generated dates will be
between min_year and max_year, inclusive.</p>


<p style="margin-top: 1em">hypothesis.extra.datetime.times(allow_naive=None,
timezones=None) <br>
Return a strategy for generating times.</p>

<p style="margin-top: 1em">Deprecated since version 3.9.0:
use hypothesis.strategies.times() instead.</p>

<p style="margin-top: 1em">The allow_naive and timezones
arguments act the same as the datetimes strategy above.</p>

<p style="margin-top: 1em">hypothesis[fakefactory] <br>
Fake-factory is another Python library for data generation.
hypothesis.extra.fakefactory is a package which lets you use
fake-factory generators to parametrize tests.</p>

<p style="margin-top: 1em">The fake-factory API is
extremely unstable, even between patch releases, and
Hypothesis&rsquo;s support for it is unlikely to work with
anything except the exact version it has been <br>
tested against.</p>

<p style="margin-top: 1em">hypothesis.extra.fakefactory
defines a function fake_factory which returns a strategy for
producing text data from any FakeFactory provider.</p>

<p style="margin-top: 1em">So for example the following
will parametrize a test by an email address:</p>

<p style="margin-top: 1em">&gt;&gt;&gt;
fake_factory(&rsquo;email&rsquo;).example() <br>
&rsquo;tnader@prosacco.info&rsquo;</p>

<p style="margin-top: 1em">&gt;&gt;&gt;
fake_factory(&rsquo;name&rsquo;).example() <br>
&rsquo;Zbyn&Auml;k &Auml;ern&Atilde;&frac12; CSc.&rsquo;</p>

<p style="margin-top: 1em">You can explicitly specify the
locale (otherwise it uses any of the available locales),
either as a single locale or as several:</p>

<p style="margin-top: 1em">&gt;&gt;&gt;
fake_factory(&rsquo;name&rsquo;,
locale=&rsquo;en_GB&rsquo;).example() <br>
&rsquo;Antione Gerlach&rsquo; <br>
&gt;&gt;&gt; fake_factory(&rsquo;name&rsquo;,
locales=[&rsquo;en_GB&rsquo;,
&rsquo;cs_CZ&rsquo;]).example() <br>
&rsquo;Milo&Aring;&iexcl;
&Aring;&nbsp;&Aring;&yen;astn&Atilde;&frac12;&rsquo; <br>
&gt;&gt;&gt; fake_factory(&rsquo;name&rsquo;,
locales=[&rsquo;en_GB&rsquo;,
&rsquo;cs_CZ&rsquo;]).example() <br>
&rsquo;Harm Sanford&rsquo;</p>

<p style="margin-top: 1em">If you want to your own
FakeFactory providers you can do that too, passing them in
as a providers argument:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; from
faker.providers import BaseProvider <br>
&gt;&gt;&gt; class KittenProvider(BaseProvider): <br>
... def meows(self): <br>
... return &rsquo;meow %d&rsquo; %
(self.random_number(digits=10),) <br>
... <br>
&gt;&gt;&gt; fake_factory(&rsquo;meows&rsquo;,
providers=[KittenProvider]).example() <br>
&rsquo;meow 9139348419&rsquo;</p>

<p style="margin-top: 1em">Generally you probably
shouldn&rsquo;t do this unless you&rsquo;re reusing a
provider you already have - Hypothesis&rsquo;s facilities
for strategy generation are much more powerful and easier to
<br>
use. This is only here to provide easy reuse of things you
already have.</p>

<p style="margin-top: 1em">hypothesis[django] <br>
hypothesis.extra.django adds support for testing your Django
models with Hypothesis.</p>

<p style="margin-top: 1em">It is tested extensively against
all versions of Django in mainstream or extended support,
including LTS releases. It may be compatible with earlier
versions too, but there&rsquo;s no <br>
support from us either and you really should update to get
security patches.</p>

<p style="margin-top: 1em">It&rsquo;s large enough that it
is documented elsewhere.</p>

<p style="margin-top: 1em">hypothesis[numpy] <br>
hypothesis.extra.numpy adds support for testing your Numpy
code with Hypothesis.</p>

<p style="margin-top: 1em">This includes generating arrays,
array shapes, and both scalar or compound dtypes.</p>

<p style="margin-top: 1em">Like the Django extra, Numpy has
it&rsquo;s own page.</p>

<p style="margin-top: 1em">HYPOTHESIS FOR DJANGO USERS <br>
Hypothesis offers a number of features specific for Django
testing, available in the hypothesis[django] extra. This is
tested against each supported series with mainstream or <br>
extended support - if you&rsquo;re still getting security
patches, you can test with Hypothesis.</p>

<p style="margin-top: 1em">Using it is quite
straightforward: All you need to do is subclass
hypothesis.extra.django.TestCase or
hypothesis.extra.django.TransactionTestCase and you can use
@given as nor&acirc; <br>
mal, and the transactions will be per example rather than
per test function as they would be if you used @given with a
normal django test suite (this is important because your
<br>
test function will be called multiple times and you
don&rsquo;t want them to interfere with each other). Test
cases on these classes that do not use @given will be run as
normal.</p>

<p style="margin-top: 1em">I strongly recommend not using
TransactionTestCase unless you really have to. Because
Hypothesis runs this in a loop the performance problems it
normally has are significantly <br>
exacerbated and your tests will be really slow.</p>

<p style="margin-top: 1em">In addition to the above,
Hypothesis has some limited support for automatically
deriving strategies for your model types, which you can then
customize further.</p>

<p style="margin-top: 1em">WARNING: <br>
Hypothesis creates saved models. This will run inside your
testing transaction when using the test runner, but if you
use the dev console this will leave debris in your
data&acirc; <br>
base.</p>

<p style="margin-top: 1em">For example, using the trivial
django project I have for testing:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; from
hypothesis.extra.django.models import models <br>
&gt;&gt;&gt; from toystore.models import Customer <br>
&gt;&gt;&gt; c = models(Customer).example() <br>
&gt;&gt;&gt; c <br>
&lt;Customer: Customer object&gt; <br>
&gt;&gt;&gt; c.email <br>
&rsquo;jaime.urbina@gmail.com&rsquo; <br>
&gt;&gt;&gt; c.name <br>

&rsquo;U00109d3dU000e07beU000165f8U0003fabfU000c12cdU000f1910U00059f12U000519b0U0003fabfU000f1910U000423fbU000423fbU00059f12U000e07beU000c12cdU000e07beU000519b0U000165f8U0003fabfU0007bc31&rsquo;
<br>
&gt;&gt;&gt; c.age <br>
-873375803</p>

<p style="margin-top: 1em">Hypothesis has just created this
with whatever the relevant type of data is.</p>

<p style="margin-top: 1em">Obviously the customer&rsquo;s
age is implausible, so lets fix that:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; from
hypothesis.strategies import integers <br>
&gt;&gt;&gt; c = models(Customer, age=integers(min_value=0,
max_value=120)).example() <br>
&gt;&gt;&gt; c <br>
&lt;Customer: Customer object&gt; <br>
&gt;&gt;&gt; c.age <br>
5</p>

<p style="margin-top: 1em">You can use this to override any
fields you like. Sometimes this will be mandatory: If you
have a non-nullable field of a type Hypothesis doesn&rsquo;t
know how to create (e.g. a for&acirc; <br>
eign key) then the models function will error unless you
explicitly pass a strategy to use there.</p>

<p style="margin-top: 1em">Foreign keys are not
automatically derived. If they&rsquo;re nullable they will
default to always being null, otherwise you always have to
specify them. e.g. suppose we had a Shop type <br>
with a foreign key to company, we would define a strategy
for it as:</p>

<p style="margin-top: 1em">shop_strategy = models(Shop,
company=models(Company))</p>

<p style="margin-top: 1em">Tips and tricks <br>
Custom field types <br>
If you have a custom Django field type you can register it
with Hypothesis&rsquo;s model deriving functionality by
registering a default strategy for it:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; from
toystore.models import CustomishField, Customish <br>
&gt;&gt;&gt; models(Customish).example() <br>
hypothesis.errors.InvalidArgument: Missing arguments for
mandatory field <br>
customish for model Customish <br>
&gt;&gt;&gt; from hypothesis.extra.django.models import
add_default_field_mapping <br>
&gt;&gt;&gt; from hypothesis.strategies import just <br>
&gt;&gt;&gt; add_default_field_mapping(CustomishField,
just(&quot;hi&quot;)) <br>
&gt;&gt;&gt; x = models(Customish).example() <br>
&gt;&gt;&gt; x.customish <br>
&rsquo;hi&rsquo;</p>

<p style="margin-top: 1em">Note that this mapping is on
exact type. Subtypes will not inherit it.</p>

<p style="margin-top: 1em">Generating child models <br>
For the moment there&rsquo;s no explicit support in
hypothesis-django for generating dependent models. i.e. a
Company model will generate no Shops. However if you want to
generate some <br>
dependent models as well, you can emulate this by using the
flatmap function as follows:</p>

<p style="margin-top: 1em">from hypothesis.strategies
import lists, just</p>

<p style="margin-top: 1em">def
generate_with_shops(company): <br>
return lists(models(Shop, company=just(company))).map(lambda
_: company)</p>

<p style="margin-top: 1em">company_with_shops_strategy =
models(Company).flatmap(generate_with_shops)</p>

<p style="margin-top: 1em">Lets unpack what this is
doing:</p>

<p style="margin-top: 1em">The way flatmap works is that we
draw a value from the original strategy, then apply a
function to it which gives us a new strategy. We then draw a
value from that strategy. So <br>
in this case we&rsquo;re first drawing a company, and then
we&rsquo;re drawing a list of shops belonging to that
company: The just strategy is a strategy such that drawing
it always produces <br>
the individual value, so models(Shop, company=just(company))
is a strategy that generates a Shop belonging to the
original company.</p>

<p style="margin-top: 1em">So the following code would give
us a list of shops all belonging to the same company:</p>

<p style="margin-top: 1em">models(Company).flatmap(lambda
c: lists(models(Shop, company=just(c))))</p>

<p style="margin-top: 1em">The only difference from this
and the above is that we want the company, not the shops.
This is where the inner map comes in. We build the list of
shops and then throw it away, <br>
instead returning the company we started for. This works
because the models that Hypothesis generates are saved in
the database, so we&rsquo;re essentially running the inner
strategy <br>
purely for the side effect of creating those children in the
database.</p>

<p style="margin-top: 1em">Using default field values <br>
Hypothesis ignores field defaults and always tries to
generate values, even if it doesn&rsquo;t know how to. You
can tell it to use the default value for a field instead of
generating <br>
one by passing fieldname=default_value to models():</p>

<p style="margin-top: 1em">&gt;&gt;&gt; from
toystore.models import DefaultCustomish <br>
&gt;&gt;&gt; models(DefaultCustomish).example() <br>
hypothesis.errors.InvalidArgument: Missing arguments for
mandatory field <br>
customish for model DefaultCustomish <br>
&gt;&gt;&gt; from hypothesis.extra.django.models import
default_value <br>
&gt;&gt;&gt; x = models(DefaultCustomish,
customish=default_value).example() <br>
&gt;&gt;&gt; x.customish <br>
&rsquo;b&rsquo;</p>

<p style="margin-top: 1em">SCIENTIFIC HYPOTHESIS (FOR
NUMPY) <br>
Hypothesis offers a number of strategies for NumPy testing,
available in the hypothesis[numpy] extra. It lives in the
hypothesis.extra.numpy package.</p>

<p style="margin-top: 1em">The centerpiece is the arrays
strategy, which generates arrays with any dtype, shape, and
contents you can specify or give a strategy for. To make
this as useful as possible, <br>
strategies are provided to generate array shapes and
generate all kinds of fixed-size or compound dtypes.</p>


<p style="margin-top: 1em">hypothesis.extra.numpy.arrays(dtype,
shape, elements=None) <br>
dtype may be any valid input to np.dtype (this includes
np.dtype objects), or a strategy that generates such values.
shape may be an integer &gt;= 0, a tuple of length &gt;= of
<br>
such integers, or a strategy that generates such values.</p>

<p style="margin-top: 1em">Arrays of specified dtype and
shape are generated for example like this:</p>

<p style="margin-top: 1em">&gt;&gt;&gt; import numpy as np
<br>
&gt;&gt;&gt; arrays(np.int8, (2, 3)).example() <br>
array([[-8, 6, 3], <br>
[-6, 4, 6]], dtype=int8)</p>

<p style="margin-top: 1em">If elements is None, Hypothesis
infers a strategy based on the dtype, which may give any
legal value (including eg NaN for floats). If you have more
specific require&acirc; <br>
ments, you can supply your own elements strategy - see What
you can generate and how.</p>

<p style="margin-top: 1em">&gt;&gt;&gt; import numpy as np
<br>
&gt;&gt;&gt; from hypothesis.strategies import floats <br>
&gt;&gt;&gt; arrays(np.float, 3, elements=floats(0,
1)).example() <br>
array([ 0.88974794, 0.77387938, 0.1977879 ])</p>

<p style="margin-top: 1em">WARNING: <br>
Hypothesis works really well with NumPy, but is designed for
small data. The default entropy is 8192 bytes - it is
impossible to draw an example where exam&acirc; <br>
ple_array.nbytes is greater than
settings.default.buffer_size. See the settings documentation
if you need to increase this value, but be aware that
Hypothesis may take <br>
much longer to produce a minimal failure case.</p>


<p style="margin-top: 1em">hypothesis.extra.numpy.array_shapes(min_dims=1,
max_dims=3, min_side=1, max_side=10) <br>
Return a strategy for array shapes (tuples of int &gt;=
1).</p>


<p style="margin-top: 1em">hypothesis.extra.numpy.scalar_dtypes()
<br>
Return a strategy that can return any non-flexible scalar
dtype.</p>


<p style="margin-top: 1em">hypothesis.extra.numpy.unsigned_integer_dtypes(endianness=&rsquo;?&rsquo;,
sizes=(8, 16, 32, 64)) <br>
Return a strategy for unsigned integer dtypes.</p>

<p style="margin-top: 1em">endianness may be &lt; for
little-endian, &gt; for big-endian, = for native byte order,
or ? to allow either byte order. This argument only applies
to dtypes of more than one <br>
byte.</p>

<p style="margin-top: 1em">sizes must be a collection of
integer sizes in bits. The default (8, 16, 32, 64) covers
the full range of sizes.</p>


<p style="margin-top: 1em">hypothesis.extra.numpy.integer_dtypes(endianness=&rsquo;?&rsquo;,
sizes=(8, 16, 32, 64)) <br>
Return a strategy for signed integer dtypes.</p>

<p style="margin-top: 1em">endianness and sizes are treated
as for unsigned_integer_dtypes.</p>


<p style="margin-top: 1em">hypothesis.extra.numpy.floating_dtypes(endianness=&rsquo;?&rsquo;,
sizes=(16, 32, 64)) <br>
Return a strategy for floating-point dtypes.</p>

<p style="margin-top: 1em">sizes is the size in bits of
floating-point number. Some machines support 96- or 128-bit
floats, but these are not generated by default.</p>

<p style="margin-top: 1em">Larger floats (96 and 128 bit
real parts) are not supported on all platforms and therefore
disabled by default. To generate these dtypes, include these
values in the <br>
sizes argument.</p>


<p style="margin-top: 1em">hypothesis.extra.numpy.complex_number_dtypes(endianness=&rsquo;?&rsquo;,
sizes=(64, 128)) <br>
Return a strategy complex-number dtypes.</p>

<p style="margin-top: 1em">sizes is the total size in bits
of a complex number, which consists of two floats. Complex
halfs (a 16-bit real part) are not supported by numpy and
will not be generated <br>
by this strategy.</p>


<p style="margin-top: 1em">hypothesis.extra.numpy.datetime64_dtypes(max_period=&rsquo;Y&rsquo;,
min_period=&rsquo;ns&rsquo;, endianness=&rsquo;?&rsquo;)
<br>
Return a strategy for datetime64 dtypes, with various
precisions from year to attosecond.</p>


<p style="margin-top: 1em">hypothesis.extra.numpy.timedelta64_dtypes(max_period=&rsquo;Y&rsquo;,
min_period=&rsquo;ns&rsquo;, endianness=&rsquo;?&rsquo;)
<br>
Return a strategy for timedelta64 dtypes, with various
precisions from year to attosecond.</p>


<p style="margin-top: 1em">hypothesis.extra.numpy.byte_string_dtypes(endianness=&rsquo;?&rsquo;,
min_len=0, max_len=16) <br>
Return a strategy for generating bytestring dtypes, of
various lengths and byteorder.</p>


<p style="margin-top: 1em">hypothesis.extra.numpy.unicode_string_dtypes(endianness=&rsquo;?&rsquo;,
min_len=0, max_len=16) <br>
Return a strategy for generating unicode string dtypes, of
various lengths and byteorder.</p>


<p style="margin-top: 1em">hypothesis.extra.numpy.array_dtypes(subtype_strategy=scalar_dtypes(),
min_size=1, max_size=5, allow_subarrays=False) <br>
Return a strategy for generating array (compound) dtypes,
with members drawn from the given subtype strategy.</p>


<p style="margin-top: 1em">hypothesis.extra.numpy.nested_dtypes(subtype_strategy=scalar_dtypes(),
max_leaves=10, max_itemsize=None) <br>
Return the most-general dtype strategy.</p>

<p style="margin-top: 1em">Elements drawn from this
strategy may be simple (from the subtype_strategy), or
several such values drawn from array_dtypes with
allow_subarrays=True. Subdtypes in an <br>
array dtype may be nested to any depth, subject to the
max_leaves argument.</p>

<p style="margin-top: 1em">HEALTH CHECKS <br>
Hypothesis tries to detect common mistakes and things that
will cause difficulty at run time in the form of a number of
&rsquo;health checks&rsquo;.</p>

<p style="margin-top: 1em">These include detecting and
warning about:</p>

<p style="margin-top: 1em">&Acirc;&middot; Strategies with
very slow data generation</p>

<p style="margin-top: 1em">&Acirc;&middot; Strategies which
filter out too much</p>

<p style="margin-top: 1em">&Acirc;&middot; Recursive
strategies which branch too much</p>

<p style="margin-top: 1em">&Acirc;&middot; Use of the
global random module</p>

<p style="margin-top: 1em">If any of these scenarios are
detected, Hypothesis will emit a warning about them.</p>

<p style="margin-top: 1em">The general goal of these health
checks is to warn you about things that you are doing that
might appear to work but will either cause Hypothesis to not
work correctly or to per&acirc; <br>
form badly.</p>

<p style="margin-top: 1em">To selectively disable health
checks, use the suppress_health_check settings. The argument
for this parameter is a list with elements drawn from any of
the class-level <br>
attributes of the HealthCheck class.</p>

<p style="margin-top: 1em">To disable all health checks,
set the perform_health_check settings parameter to
False.</p>

<p style="margin-top: 1em">THE HYPOTHESIS EXAMPLE DATABASE
<br>
When Hypothesis finds a bug it stores enough information in
its database to reproduce it. This enables you to have a
classic testing workflow of find a bug, fix a bug, and be
<br>
confident that this is actually doing the right thing
because Hypothesis will start by retrying the examples that
broke things last time.</p>

<p style="margin-top: 1em">Limitations <br>
The database is best thought of as a cache that you never
need to invalidate: Information may be lost when you upgrade
a Hypothesis version or change your test, so you
shouldn&rsquo;t <br>
rely on it for correctness - if there&rsquo;s an example you
want to ensure occurs each time then there&rsquo;s a feature
for including them in your source code - but it helps the
develop&acirc; <br>
ment workflow considerably by making sure that the examples
you&rsquo;ve just found are reproduced.</p>

<p style="margin-top: 1em">File locations <br>
The default storage format is as a fairly opaque directory
structure. Each test corresponds to a directory, and each
example to a file within that directory. The standard
loca&acirc; <br>
tion for it is .hypothesis/examples in your current working
directory. You can override this, either by setting either
the database_file property on a settings object (you
proba&acirc; <br>
bly want to specify it on settings.default) or by setting
the HYPOTHESIS_DATABASE_FILE environment variable.</p>

<p style="margin-top: 1em">There is also a legacy sqlite3
based format. This is mostly still supported for
compatibility reasons, and support will be dropped in some
future version of Hypothesis. If you <br>
use a database file name ending in .db, .sqlite or .sqlite3
that format will be used instead.</p>

<p style="margin-top: 1em">Upgrading Hypothesis and
changing your tests <br>
The design of the Hypothesis database is such that you can
put arbitrary data in the database and not get wrong
behaviour. When you upgrade Hypothesis, old data might be
invali&acirc; <br>
dated, but this should happen transparently. It should never
be the case that e.g. changing the strategy that generates
an argument sometimes gives you data from the old
strat&acirc; <br>
egy.</p>

<p style="margin-top: 1em">Sharing your example database
<br>
NOTE: <br>
If specific examples are important for correctness you
should use the @example decorator, as the example database
may discard entries due to changes in your code or
dependen&acirc; <br>
cies. For most users, we therefore recommend using the
example database locally and possibly persisting it between
CI builds, but not tracking it under version control.</p>

<p style="margin-top: 1em">The examples database can be
shared simply by checking the directory into version
control, for example with the following .gitignore:</p>

<p style="margin-top: 1em"># Ignore files cached by
Hypothesis... <br>
.hypothesis/ <br>
# except for the examples directory <br>
!.hypothesis/examples/</p>

<p style="margin-top: 1em">Like everything under
.hypothesis/, the examples directory will be transparently
created on demand. Unlike the other subdirectories,
examples/ is designed to handle merges, <br>
deletes, etc if you just add the directory into git,
mercurial, or any similar version control system.</p>

<p style="margin-top: 1em">STATEFUL TESTING <br>
Hypothesis offers support for a stateful style of test,
where instead of trying to produce a single data value that
causes a specific test to fail, it tries to generate a
program <br>
that errors. In many ways, this sort of testing is to
classical property based testing as property based testing
is to normal example based testing.</p>

<p style="margin-top: 1em">The idea doesn&rsquo;t originate
with Hypothesis, though Hypothesis&rsquo;s implementation
and approach is mostly not based on an existing
implementation and should be considered some mix of <br>
novel and independent reinventions.</p>

<p style="margin-top: 1em">This style of testing is useful
both for programs which involve some sort of mutable state
and for complex APIs where there&rsquo;s no state per se but
the actions you perform involve <br>
e.g. taking data from one function and feeding it into
another.</p>

<p style="margin-top: 1em">The idea is that you teach
Hypothesis how to interact with your program: Be it a
server, a python API, whatever. All you need is to be able
to answer the question &quot;Given what <br>
I&rsquo;ve done so far, what could I do now?&quot;. After
that, Hypothesis takes over and tries to find sequences of
actions which cause a test failure.</p>

<p style="margin-top: 1em">Right now the stateful testing
is a bit new and experimental and should be considered as a
semi-public API: It may break between minor versions but
won&rsquo;t break between patch <br>
releases, and there are still some rough edges in the API
that will need to be filed off.</p>

<p style="margin-top: 1em">This shouldn&rsquo;t discourage
you from using it. Although it&rsquo;s not as robust as the
rest of Hypothesis, it&rsquo;s still pretty robust and more
importantly is extremely powerful. I found a <br>
number of really subtle bugs in Hypothesis by turning the
stateful testing onto a subset of the Hypothesis API, and
you likely will find the same.</p>

<p style="margin-top: 1em">Enough preamble, lets see how to
use it.</p>

<p style="margin-top: 1em">The first thing to note is that
there are two levels of API: The low level but more flexible
API and the higher level rule based API which is both easier
to use and also produces <br>
a much better display of data due to its greater structure.
We&rsquo;ll start with the more structured one.</p>

<p style="margin-top: 1em">Rule based state machines <br>
Rule based state machines are the ones you&rsquo;re most
likely to want to use. They&rsquo;re significantly more user
friendly and should be good enough for most things
you&rsquo;d want to do.</p>

<p style="margin-top: 1em">A rule based state machine is a
collection of functions (possibly with side effects) which
may depend on both values that Hypothesis can generate and
also on values that have <br>
resulted from previous function calls.</p>

<p style="margin-top: 1em">You define a rule based state
machine as follows:</p>

<p style="margin-top: 1em">import unittest <br>
from collections import namedtuple</p>

<p style="margin-top: 1em">from hypothesis import
strategies as st <br>
from hypothesis.stateful import RuleBasedStateMachine,
Bundle, rule</p>

<p style="margin-top: 1em">Leaf =
namedtuple(&rsquo;Leaf&rsquo;, (&rsquo;label&rsquo;,)) <br>
Split = namedtuple(&rsquo;Split&rsquo;, (&rsquo;left&rsquo;,
&rsquo;right&rsquo;))</p>

<p style="margin-top: 1em">class
BalancedTrees(RuleBasedStateMachine): <br>
trees = Bundle(&rsquo;BinaryTree&rsquo;)</p>

<p style="margin-top: 1em">@rule(target=trees,
x=st.integers()) <br>
def leaf(self, x): <br>
return Leaf(x)</p>

<p style="margin-top: 1em">@rule(target=trees, left=trees,
right=trees) <br>
def split(self, left, right): <br>
return Split(left, right)</p>

<p style="margin-top: 1em">@rule(tree=trees) <br>
def check_balanced(self, tree): <br>
if isinstance(tree, Leaf): <br>
return <br>
else: <br>
assert abs(self.size(tree.left) - self.size(tree.right))
&lt;= 1 <br>
self.check_balanced(tree.left) <br>
self.check_balanced(tree.right)</p>

<p style="margin-top: 1em">def size(self, tree): <br>
if isinstance(tree, Leaf): <br>
return 1 <br>
else: <br>
return 1 + self.size(tree.left) + self.size(tree.right)</p>

<p style="margin-top: 1em">In this we declare a Bundle,
which is a named collection of previously generated values.
We define two rules which put data onto this bundle - one
which just generates leaves <br>
with integer labels, the other of which takes two previously
generated values and returns a new one.</p>

<p style="margin-top: 1em">We can then integrate this into
our test suite by getting a unittest TestCase from it:</p>

<p style="margin-top: 1em">TestTrees =
BalancedTrees.TestCase</p>

<p style="margin-top: 1em">if __name__ ==
&rsquo;__main__&rsquo;: <br>
unittest.main()</p>

<p style="margin-top: 1em">(these will also be picked up by
py.test if you prefer to use that). Running this we get:</p>

<p style="margin-top: 1em">Step #1: v1 = leaf(x=0) <br>
Step #2: v2 = split(left=v1, right=v1) <br>
Step #3: v3 = split(left=v2, right=v1) <br>
Step #4: check_balanced(tree=v3) <br>
F <br>

======================================================================
<br>
FAIL: runTest (hypothesis.stateful.BalancedTrees.TestCase)
<br>

----------------------------------------------------------------------
<br>
Traceback (most recent call last): <br>
(...) <br>
assert abs(self.size(tree.left) - self.size(tree.right))
&lt;= 1 <br>
AssertionError</p>

<p style="margin-top: 1em">Note how it&rsquo;s printed out
a very short program that will demonstrate the problem.</p>

<p style="margin-top: 1em">...the problem of course being
that we&rsquo;ve not actually written any code to balance
this tree at all, so of course it&rsquo;s not balanced.</p>

<p style="margin-top: 1em">So lets balance some trees.</p>

<p style="margin-top: 1em">from collections import
namedtuple</p>

<p style="margin-top: 1em">from hypothesis import
strategies as st <br>
from hypothesis.stateful import RuleBasedStateMachine,
Bundle, rule</p>

<p style="margin-top: 1em">Leaf =
namedtuple(&rsquo;Leaf&rsquo;, (&rsquo;label&rsquo;,)) <br>
Split = namedtuple(&rsquo;Split&rsquo;, (&rsquo;left&rsquo;,
&rsquo;right&rsquo;))</p>

<p style="margin-top: 1em">class
BalancedTrees(RuleBasedStateMachine): <br>
trees = Bundle(&rsquo;BinaryTree&rsquo;) <br>
balanced_trees = Bundle(&rsquo;balanced
BinaryTree&rsquo;)</p>

<p style="margin-top: 1em">@rule(target=trees,
x=st.integers()) <br>
def leaf(self, x): <br>
return Leaf(x)</p>

<p style="margin-top: 1em">@rule(target=trees, left=trees,
right=trees) <br>
def split(self, left, right): <br>
return Split(left, right)</p>

<p style="margin-top: 1em">@rule(tree=balanced_trees) <br>
def check_balanced(self, tree): <br>
if isinstance(tree, Leaf): <br>
return <br>
else: <br>
assert abs(self.size(tree.left) - self.size(tree.right))
&lt;= 1, repr(tree) <br>
self.check_balanced(tree.left) <br>
self.check_balanced(tree.right)</p>

<p style="margin-top: 1em">@rule(target=balanced_trees,
tree=trees) <br>
def balance_tree(self, tree): <br>
return self.split_leaves(self.flatten(tree))</p>

<p style="margin-top: 1em">def size(self, tree): <br>
if isinstance(tree, Leaf): <br>
return 1 <br>
else: <br>
return self.size(tree.left) + self.size(tree.right)</p>

<p style="margin-top: 1em">def flatten(self, tree): <br>
if isinstance(tree, Leaf): <br>
return (tree.label,) <br>
else: <br>
return self.flatten(tree.left) +
self.flatten(tree.right)</p>

<p style="margin-top: 1em">def split_leaves(self, leaves):
<br>
assert leaves <br>
if len(leaves) == 1: <br>
return Leaf(leaves[0]) <br>
else: <br>
mid = len(leaves) // 2 <br>
return Split( <br>
self.split_leaves(leaves[:mid]), <br>
self.split_leaves(leaves[mid:]), <br>
)</p>

<p style="margin-top: 1em">We&rsquo;ve now written a really
noddy tree balancing implementation. This takes trees and
puts them into a new bundle of data, and we only assert that
things in the balanced_trees <br>
bundle are actually balanced.</p>

<p style="margin-top: 1em">If you run this it will sit
there silently for a while (you can turn on verbose output
to get slightly more information about what&rsquo;s
happening. debug will give you all the inter&acirc; <br>
mediate programs being run) and then run, telling you your
test has passed! Our balancing algorithm worked.</p>

<p style="margin-top: 1em">Now lets break it to make sure
the test is still valid:</p>

<p style="margin-top: 1em">Changing the split to mid =
max(len(leaves) // 3, 1) this should no longer balance,
which gives us the following counter-example:</p>

<p style="margin-top: 1em">v1 = leaf(x=0) <br>
v2 = split(left=v1, right=v1) <br>
v3 = balance_tree(tree=v1) <br>
v4 = split(left=v2, right=v2) <br>
v5 = balance_tree(tree=v4) <br>
check_balanced(tree=v5)</p>

<p style="margin-top: 1em">Note that the example could be
shrunk further by deleting v3. Due to some technical
limitations, Hypothesis was unable to find that particular
shrink. In general it&rsquo;s rare for <br>
examples produced to be long, but they won&rsquo;t always be
minimal.</p>

<p style="margin-top: 1em">You can control the detailed
behaviour with a settings object on the TestCase (this is a
normal hypothesis settings object using the defaults at the
time the TestCase class was <br>
first referenced). For example if you wanted to run fewer
examples with larger programs you could change the settings
to:</p>

<p style="margin-top: 1em">TestTrees.settings =
settings(max_examples=100, stateful_step_count=100)</p>

<p style="margin-top: 1em">Which doubles the number of
steps each program runs and halves the number of runs
relative to the example. settings.timeout will also be
respected as usual.</p>

<p style="margin-top: 1em">Preconditions <br>
While it&rsquo;s possible to use assume() in
RuleBasedStateMachine rules, if you use it in only a few
rules you can quickly run into a situation where few or none
of your rules pass <br>
their assumptions. Thus, Hypothesis provides a
precondition() decorator to avoid this problem. The
precondition() decorator is used on rule-decorated
functions, and must be given <br>
a function that returns True or False based on the
RuleBasedStateMachine instance.</p>


<p style="margin-top: 1em">hypothesis.stateful.precondition(precond)
<br>
Decorator to apply a precondition for rules in a
RuleBasedStateMachine. Specifies a precondition for a rule
to be considered as a valid step in the state machine. The
<br>
given function will be called with the instance of
RuleBasedStateMachine and should return True or False.
Usually it will need to look at attributes on that
instance.</p>

<p style="margin-top: 1em">For example:</p>

<p style="margin-top: 1em">class
MyTestMachine(RuleBasedStateMachine): <br>
state = 1</p>

<p style="margin-top: 1em">@precondition(lambda self:
self.state != 0) <br>
@rule(numerator=integers()) <br>
def divide_with(self, numerator): <br>
self.state = numerator / self.state</p>

<p style="margin-top: 1em">This is better than using assume
in your rule since more valid rules should be able to be
run.</p>

<p style="margin-top: 1em">from hypothesis.stateful import
RuleBasedStateMachine, rule, precondition</p>

<p style="margin-top: 1em">class
NumberModifier(RuleBasedStateMachine):</p>

<p style="margin-top: 1em">num = 0</p>

<p style="margin-top: 1em">@rule() <br>
def add_one(self): <br>
self.num += 1</p>

<p style="margin-top: 1em">@precondition(lambda self:
self.num != 0) <br>
@rule() <br>
def divide_with_one(self): <br>
self.num = 1 / self.num</p>

<p style="margin-top: 1em">By using precondition() here
instead of assume(), Hypothesis can filter the inapplicable
rules before running them. This makes it much more likely
that a useful sequence of steps <br>
will be generated.</p>

<p style="margin-top: 1em">Note that currently
preconditions can&rsquo;t access bundles; if you need to use
preconditions, you should store relevant data on the
instance instead.</p>

<p style="margin-top: 1em">Invariant <br>
Often there are invariants that you want to ensure are met
after every step in a process. It would be possible to add
these as rules that are run, but they would be run zero or
<br>
multiple times between other rules. Hypothesis provides a
decorator that marks a function to be run after every
step.</p>

<p style="margin-top: 1em">hypothesis.stateful.invariant()
<br>
Decorator to apply an invariant for rules in a
RuleBasedStateMachine. The decorated function will be run
after every rule and can raise an exception to indicate
failed <br>
invariants.</p>

<p style="margin-top: 1em">For example:</p>

<p style="margin-top: 1em">class
MyTestMachine(RuleBasedStateMachine): <br>
state = 1</p>

<p style="margin-top: 1em">@invariant() <br>
def is_nonzero(self): <br>
assert self.state != 0</p>

<p style="margin-top: 1em">from hypothesis.stateful import
RuleBasedStateMachine, rule, invariant</p>

<p style="margin-top: 1em">class
NumberModifier(RuleBasedStateMachine):</p>

<p style="margin-top: 1em">num = 0</p>

<p style="margin-top: 1em">@rule() <br>
def add_two(self): <br>
self.num += 2 <br>
if self.num &gt; 50: <br>
self.num += 1</p>

<p style="margin-top: 1em">@invariant() <br>
def divide_with_one(self): <br>
assert self.num % 2 == 0</p>

<p style="margin-top: 1em">NumberTest =
NumberModifier.TestCase</p>

<p style="margin-top: 1em">Invariants can also have
precondition()s applied to them, in which case they will
only be run if the precondition function returns true.</p>

<p style="margin-top: 1em">Note that currently invariants
can&rsquo;t access bundles; if you need to use invariants,
you should store relevant data on the instance instead.</p>

<p style="margin-top: 1em">Generic state machines <br>
The class GenericStateMachine is the underlying machinery of
stateful testing in Hypothesis. In execution it looks much
like the RuleBasedStateMachine but it allows the set of <br>
steps available to depend in essentially arbitrary ways on
what has happened so far. For example, if you wanted to use
Hypothesis to test a game, it could choose each step in the
<br>
machine based on the game to date and the set of actions the
game program is telling it it has available.</p>

<p style="margin-top: 1em">It essentially executes the
following loop:</p>

<p style="margin-top: 1em">machine = MyStateMachine() <br>
try: <br>
machine.check_invariants() <br>
for _ in range(n_steps): <br>
step = machine.steps().example() <br>
machine.execute_step(step) <br>
machine.check_invariants() <br>
finally: <br>
machine.teardown()</p>

<p style="margin-top: 1em">Where steps and execute_step are
methods you must implement, and teardown and check_invarants
are methods you can implement if required. steps returns a
strategy, which is <br>
allowed to depend arbitrarily on the current state of the
test execution. Ideally a good steps implementation should
be robust against minor changes in the state. Steps that
<br>
change a lot between slightly different executions will tend
to produce worse quality examples because they&rsquo;re hard
to simplify.</p>

<p style="margin-top: 1em">The steps method may depend on
external state, but it&rsquo;s not advisable and may produce
flaky tests.</p>

<p style="margin-top: 1em">If any of execute_step,
check_invariants or teardown produces an exception,
Hypothesis will try to find a minimal sequence of values
steps such that the following throws an <br>
exception:</p>

<p style="margin-top: 1em">machine = MyStateMachine() <br>
try: <br>
machine.check_invariants() <br>
for step in steps: <br>
machine.execute_step(step) <br>
machine.check_invariants() <br>
finally: <br>
machine.teardown()</p>

<p style="margin-top: 1em">and such that at every point,
the step executed is one that could plausible have come from
a call to steps in the current state.</p>

<p style="margin-top: 1em">Here&rsquo;s an example of using
stateful testing to test a broken implementation of a set in
terms of a list (note that you could easily do something
close to this example with the <br>
rule based testing instead, and probably should. This is
mostly for illustration purposes):</p>

<p style="margin-top: 1em">import unittest</p>

<p style="margin-top: 1em">from hypothesis.stateful import
GenericStateMachine <br>
from hypothesis.strategies import tuples, sampled_from,
just, integers</p>

<p style="margin-top: 1em">class
BrokenSet(GenericStateMachine): <br>
def __init__(self): <br>
self.data = []</p>

<p style="margin-top: 1em">def steps(self): <br>
add_strategy = tuples(just(&quot;add&quot;), integers())
<br>
if not self.data: <br>
return add_strategy <br>
else: <br>
return ( <br>
add_strategy | <br>
tuples(just(&quot;delete&quot;),
sampled_from(self.data)))</p>

<p style="margin-top: 1em">def execute_step(self, step):
<br>
action, value = step <br>
if action == &rsquo;delete&rsquo;: <br>
try: <br>
self.data.remove(value) <br>
except ValueError: <br>
pass <br>
assert value not in self.data <br>
else: <br>
assert action == &rsquo;add&rsquo; <br>
self.data.append(value) <br>
assert value in self.data</p>

<p style="margin-top: 1em">TestSet = BrokenSet.TestCase</p>

<p style="margin-top: 1em">if __name__ ==
&rsquo;__main__&rsquo;: <br>
unittest.main()</p>

<p style="margin-top: 1em">Note that the strategy changes
each time based on the data that&rsquo;s currently in the
state machine.</p>

<p style="margin-top: 1em">Running this gives us the
following:</p>

<p style="margin-top: 1em">Step #1: (&rsquo;add&rsquo;, 0)
<br>
Step #2: (&rsquo;add&rsquo;, 0) <br>
Step #3: (&rsquo;delete&rsquo;, 0) <br>
F <br>

======================================================================
<br>
FAIL: runTest (hypothesis.stateful.BrokenSet.TestCase) <br>

----------------------------------------------------------------------
<br>
Traceback (most recent call last): <br>
(...) <br>
assert value not in self.data <br>
AssertionError</p>

<p style="margin-top: 1em">So it adds two elements, then
deletes one, and throws an assertion when it finds out that
this only deleted one of the copies of the element.</p>

<p style="margin-top: 1em">More fine grained control <br>
If you want to bypass the TestCase infrastructure you can
invoke these manually. The stateful module exposes the
function run_state_machine_as_test, which takes an arbitrary
<br>
function returning a GenericStateMachine and an optional
settings parameter and does the same as the class based
runTest provided.</p>

<p style="margin-top: 1em">In particular this may be useful
if you wish to pass parameters to a custom __init__ in your
subclass.</p>

<p style="margin-top: 1em">COMPATIBILITY <br>
Hypothesis does its level best to be compatible with
everything you could possibly need it to be compatible with.
Generally you should just try it and expect it to work. If
it <br>
doesn&rsquo;t, you can be surprised and check this document
for the details.</p>

<p style="margin-top: 1em">Python versions <br>
Hypothesis is supported and tested on CPython 2.7 and
CPython 3.4+.</p>

<p style="margin-top: 1em">Hypothesis also supports PyPy2,
and will support PyPy3 when there is a stable release
supporting Python 3.4+. Hypothesis does not currently work
on Jython, though could feasibly <br>
be made to do so. IronPython might work but hasn&rsquo;t
been tested. 32-bit and narrow builds should work, though
this is currently only tested on Windows.</p>

<p style="margin-top: 1em">In general Hypothesis does not
officially support anything except the latest patch release
of any version of Python it supports. Earlier releases
should work and bugs in them <br>
will get fixed if reported, but they&rsquo;re not tested in
CI and no guarantees are made.</p>

<p style="margin-top: 1em">Operating systems <br>
In theory Hypothesis should work anywhere that Python does.
In practice it is only known to work and regularly tested on
OS X, Windows and Linux, and you may experience issues <br>
running it elsewhere.</p>

<p style="margin-top: 1em">If you&rsquo;re using something
else and it doesn&rsquo;t work, do get in touch and
I&rsquo;ll try to help, but unless you can come up with a
way for me to run a CI server on that operating system <br>
it probably won&rsquo;t stay fixed due to the inevitable
march of time.</p>

<p style="margin-top: 1em">Testing frameworks <br>
In general Hypothesis goes to quite a lot of effort to
generate things that look like normal Python test functions
that behave as closely to the originals as possible, so it
<br>
should work sensibly out of the box with every test
framework.</p>

<p style="margin-top: 1em">If your testing relies on doing
something other than calling a function and seeing if it
raises an exception then it probably won&rsquo;t work out of
the box. In particular things like <br>
tests which return generators and expect you to do something
with them (e.g. nose&rsquo;s yield based tests) will not
work. Use a decorator or similar to wrap the test to take
this <br>
form.</p>

<p style="margin-top: 1em">In terms of what&rsquo;s
actually known to work:</p>

<p style="margin-top: 1em">&Acirc;&middot; Hypothesis
integrates as smoothly with py.test and unittest as I can
make it, and this is verified as part of the CI.</p>

<p style="margin-top: 1em">&Acirc;&middot; py.test fixtures
work correctly with Hypothesis based functions, but note
that function based fixtures will only run once for the
whole function, not once per example.</p>

<p style="margin-top: 1em">&Acirc;&middot; Nose works fine
with hypothesis, and this is tested as part of the CI. yield
based tests simply won&rsquo;t work.</p>

<p style="margin-top: 1em">&Acirc;&middot; Integration with
Django&rsquo;s testing requires use of the hypothesis-django
package. The issue is that in Django&rsquo;s tests&rsquo;
normal mode of execution it will reset the database <br>
one per test rather than once per example, which is not what
you want.</p>

<p style="margin-top: 1em">Coverage works out of the box
with Hypothesis (and Hypothesis has 100% branch coverage in
its own tests). However you should probably not use
Coverage, Hypothesis and PyPy <br>
together. Because Hypothesis does quite a lot of CPU heavy
work compared to normal tests, it really exacerbates the
performance problems the two normally have working
together.</p>

<p style="margin-top: 1em">Optional Packages <br>
The supported versions of optional packages, for strategies
in hypothesis.extra, are listed in the documentation for
that extra. Our general goal is to support all versions that
<br>
are supported upstream.</p>

<p style="margin-top: 1em">Regularly verifying this <br>
Everything mentioned above as explicitly supported is
checked on every commit with Travis and Appveyor and goes
green before a release happens, so when I say they&rsquo;re
supported I <br>
really mean it.</p>

<p style="margin-top: 1em">Hypothesis versions <br>
Backwards compatibility is better than backporting fixes, so
we use semantic versioning and only support the most recent
version of Hypothesis. See support for more information.</p>

<p style="margin-top: 1em">SOME MORE EXAMPLES <br>
This is a collection of examples of how to use Hypothesis in
interesting ways. It&rsquo;s small for now but will grow
over time.</p>

<p style="margin-top: 1em">All of these examples are
designed to be run under py.test (nose should probably work
too).</p>

<p style="margin-top: 1em">How not to sort by a partial
order <br>
The following is an example that&rsquo;s been extracted and
simplified from a real bug that occurred in an earlier
version of Hypothesis. The real bug was a lot harder to
find.</p>

<p style="margin-top: 1em">Suppose we&rsquo;ve got the
following type:</p>

<p style="margin-top: 1em">class Node(object): <br>
def __init__(self, label, value): <br>
self.label = label <br>
self.value = tuple(value)</p>

<p style="margin-top: 1em">def __repr__(self): <br>
return &quot;Node(%r, %r)&quot; % (self.label,
self.value)</p>

<p style="margin-top: 1em">def sorts_before(self, other):
<br>
if len(self.value) &gt;= len(other.value): <br>
return False <br>
return other.value[:len(self.value)] == self.value</p>

<p style="margin-top: 1em">Each node is a label and a
sequence of some data, and we have the relationship
sorts_before meaning the data of the left is an initial
segment of the right. So e.g. a node with <br>
value [1, 2] will sort before a node with value [1, 2, 3],
but neither of [1, 2] nor [1, 3] will sort before the
other.</p>

<p style="margin-top: 1em">We have a list of nodes, and we
want to topologically sort them with respect to this
ordering. That is, we want to arrange the list so that if
x.sorts_before(y) then x appears <br>
earlier in the list than y. We naively think that the
easiest way to do this is to extend the partial order
defined here to a total order by breaking ties arbitrarily
and then <br>
using a normal sorting algorithm. So we define the following
code:</p>

<p style="margin-top: 1em">from functools import
total_ordering</p>

<p style="margin-top: 1em">@total_ordering <br>
class TopoKey(object): <br>
def __init__(self, node): <br>
self.value = node</p>

<p style="margin-top: 1em">def __lt__(self, other): <br>
if self.value.sorts_before(other.value): <br>
return True <br>
if other.value.sorts_before(self.value): <br>
return False</p>

<p style="margin-top: 1em">return self.value.label &lt;
other.value.label</p>

<p style="margin-top: 1em">def sort_nodes(xs): <br>
xs.sort(key=TopoKey)</p>

<p style="margin-top: 1em">This takes the order defined by
sorts_before and extends it by breaking ties by comparing
the node labels.</p>

<p style="margin-top: 1em">But now we want to test that it
works.</p>

<p style="margin-top: 1em">First we write a function to
verify that our desired outcome holds:</p>

<p style="margin-top: 1em">def is_prefix_sorted(xs): <br>
for i in range(len(xs)): <br>
for j in range(i+1, len(xs)): <br>
if xs[j].sorts_before(xs[i]): <br>
return False <br>
return True</p>

<p style="margin-top: 1em">This will return false if it
ever finds a pair in the wrong order and return true
otherwise.</p>

<p style="margin-top: 1em">Given this function, what we
want to do with Hypothesis is assert that for all sequences
of nodes, the result of calling sort_nodes on it is
sorted.</p>

<p style="margin-top: 1em">First we need to define a
strategy for Node:</p>

<p style="margin-top: 1em">from hypothesis import settings,
strategy <br>
import hypothesis.strategies as s</p>

<p style="margin-top: 1em">NodeStrategy = s.builds( <br>
Node, <br>
s.integers(), <br>
s.lists(s.booleans(), average_size=5, max_size=10))</p>

<p style="margin-top: 1em">We want to generate short lists
of values so that there&rsquo;s a decent chance of one being
a prefix of the other (this is also why the choice of bool
as the elements). We then define <br>
a strategy which builds a node out of an integer and one of
those short lists of booleans.</p>

<p style="margin-top: 1em">We can now write a test:</p>

<p style="margin-top: 1em">from hypothesis import given</p>

<p style="margin-top: 1em">@given(s.lists(NodeStrategy))
<br>
def test_sorting_nodes_is_prefix_sorted(xs): <br>
sort_nodes(xs) <br>
assert is_prefix_sorted(xs)</p>

<p style="margin-top: 1em">this immediately fails with the
following example:</p>

<p style="margin-top: 1em">[Node(0, (False, True)), Node(0,
(True,)), Node(0, (False,))]</p>

<p style="margin-top: 1em">The reason for this is that
because False is not a prefix of (True, True) nor vice
versa, sorting things the first two nodes are equal because
they have equal labels. This makes <br>
the whole order non-transitive and produces basically
nonsense results.</p>

<p style="margin-top: 1em">But this is pretty unsatisfying.
It only works because they have the same label. Perhaps we
actually wanted our labels to be unique. Lets change the
test to do that.</p>

<p style="margin-top: 1em">def
deduplicate_nodes_by_label(nodes): <br>
table = {} <br>
for node in nodes: <br>
table[node.label] = node <br>
return list(table.values())</p>

<p style="margin-top: 1em">NodeSet =
s.lists(Node).map(deduplicate_nodes_by_label)</p>

<p style="margin-top: 1em">We define a function to
deduplicate nodes by labels, and then map that over a
strategy for lists of nodes to give us a strategy for lists
of nodes with unique labels. We can now <br>
rewrite the test to use that:</p>

<p style="margin-top: 1em">@given(NodeSet) <br>
def test_sorting_nodes_is_prefix_sorted(xs): <br>
sort_nodes(xs) <br>
assert is_prefix_sorted(xs)</p>

<p style="margin-top: 1em">Hypothesis quickly gives us an
example of this still being wrong:</p>

<p style="margin-top: 1em">[Node(0, (False,)), Node(-1,
(True,)), Node(-2, (False, False))])</p>

<p style="margin-top: 1em">Now this is a more interesting
example. None of the nodes will sort equal. What is
happening here is that the first node is strictly less than
the last node because (False,) is a <br>
prefix of (False, False). This is in turn strictly less than
the middle node because neither is a prefix of the other and
-2 &lt; -1. The middle node is then less than the first <br>
node because -1 &lt; 0.</p>

<p style="margin-top: 1em">So, convinced that our
implementation is broken, we write a better one:</p>

<p style="margin-top: 1em">def sort_nodes(xs): <br>
for i in hrange(1, len(xs)): <br>
j = i - 1 <br>
while j &gt;= 0: <br>
if xs[j].sorts_before(xs[j+1]): <br>
break <br>
xs[j], xs[j+1] = xs[j+1], xs[j] <br>
j -= 1</p>

<p style="margin-top: 1em">This is just insertion sort
slightly modified - we swap a node backwards until swapping
it further would violate the order constraints. The reason
this works is because our order <br>
is a partial order already (this wouldn&rsquo;t produce a
valid result for a general topological sorting - you need
the transitivity).</p>

<p style="margin-top: 1em">We now run our test again and it
passes, telling us that this time we&rsquo;ve successfully
managed to sort some nodes without getting it completely
wrong. Go us.</p>

<p style="margin-top: 1em">Time zone arithmetic <br>
This is an example of some tests for pytz which check that
various timezone conversions behave as you would expect them
to. These tests should all pass, and are mostly a
demon&acirc; <br>
stration of some useful sorts of thing to test with
Hypothesis, and how the hypothesis-datetime extra package
works.</p>

<p style="margin-top: 1em">&gt;&gt;&gt; from datetime
import timedelta <br>
&gt;&gt;&gt; from hypothesis.extra.pytz import timezones</p>

<p style="margin-top: 1em">&gt;&gt;&gt; # The datetimes
strategy is naive by default, so tell it to use timezones
<br>
&gt;&gt;&gt; aware_datetimes =
datetimes(timezones=timezones())</p>

<p style="margin-top: 1em">&gt;&gt;&gt;
@given(aware_datetimes, timezones(), timezones()) <br>
... def test_convert_via_intermediary(dt, tz1, tz2): <br>
... &quot;&quot;&quot;Test that converting between timezones
is not affected <br>
... by a detour via another timezone. <br>
... &quot;&quot;&quot; <br>
... assert dt.astimezone(tz1).astimezone(tz2) ==
dt.astimezone(tz2)</p>

<p style="margin-top: 1em">&gt;&gt;&gt;
@given(aware_datetimes, timezones()) <br>
... def test_convert_to_and_fro(dt, tz2): <br>
... &quot;&quot;&quot;If we convert to a new timezone and
back to the old one <br>
... this should leave the result unchanged. <br>
... &quot;&quot;&quot; <br>
... tz1 = dt.tzinfo <br>
... assert dt == dt.astimezone(tz2).astimezone(tz1)</p>

<p style="margin-top: 1em">&gt;&gt;&gt;
@given(aware_datetimes, timezones()) <br>
... def test_adding_an_hour_commutes(dt, tz): <br>
... &quot;&quot;&quot;When converting between timezones it
shouldn&rsquo;t matter <br>
... if we add an hour here or add an hour there. <br>
... &quot;&quot;&quot; <br>
... an_hour = timedelta(hours=1) <br>
... assert (dt + an_hour).astimezone(tz) ==
dt.astimezone(tz) + an_hour</p>

<p style="margin-top: 1em">&gt;&gt;&gt;
@given(aware_datetimes, timezones()) <br>
... def test_adding_a_day_commutes(dt, tz): <br>
... &quot;&quot;&quot;When converting between timezones it
shouldn&rsquo;t matter <br>
... if we add a day here or add a day there. <br>
... &quot;&quot;&quot; <br>
... a_day = timedelta(days=1) <br>
... assert (dt + a_day).astimezone(tz) == dt.astimezone(tz)
+ a_day</p>

<p style="margin-top: 1em">&gt;&gt;&gt; # And we can check
that our tests pass <br>
&gt;&gt;&gt; test_convert_via_intermediary() <br>
&gt;&gt;&gt; test_convert_to_and_fro() <br>
&gt;&gt;&gt; test_adding_an_hour_commutes() <br>
&gt;&gt;&gt; test_adding_a_day_commutes()</p>

<p style="margin-top: 1em">Condorcet&rsquo;s Paradox <br>
A classic paradox in voting theory, called Condorcet&rsquo;s
paradox, is that majority preferences are not transitive.
That is, there is a population and a set of three candidates
A, B <br>
and C such that the majority of the population prefer A to
B, B to C and C to A.</p>

<p style="margin-top: 1em">Wouldn&rsquo;t it be neat if we
could use Hypothesis to provide an example of this?</p>

<p style="margin-top: 1em">Well as you can probably guess
from the presence of this section, we can! This is slightly
surprising because it&rsquo;s not really obvious how we
would generate an election given the <br>
types that Hypothesis knows about.</p>

<p style="margin-top: 1em">The trick here turns out to be
twofold:</p>

<p style="margin-top: 1em">1. We can generate a type that
is much larger than an election, extract an election out of
that, and rely on minimization to throw away all the
extraneous detail.</p>

<p style="margin-top: 1em">2. We can use assume and rely on
Hypothesis&rsquo;s adaptive exploration to focus on the
examples that turn out to generate interesting elections</p>

<p style="margin-top: 1em">Without further ado, here is the
code:</p>

<p style="margin-top: 1em">from hypothesis import given,
assume <br>
from hypothesis.strategies import integers, lists <br>
from collections import Counter</p>

<p style="margin-top: 1em">def candidates(votes): <br>
return {candidate for vote in votes for candidate in
vote}</p>

<p style="margin-top: 1em">def build_election(votes): <br>
&quot;&quot;&quot; <br>
Given a list of lists we extract an election out of this. We
do this <br>
in two phases:</p>

<p style="margin-top: 1em">1. First of all we work out the
full set of candidates present in all <br>
votes and throw away any votes that do not have that whole
set. <br>
2. We then take each vote and make it unique, keeping only
the first <br>
instance of any candidate.</p>

<p style="margin-top: 1em">This gives us a list of total
orderings of some set. It will usually <br>
be a lot smaller than the starting list, but that&rsquo;s
OK. <br>
&quot;&quot;&quot; <br>
all_candidates = candidates(votes) <br>
votes = list(filter(lambda v: set(v) == all_candidates,
votes)) <br>
if not votes: <br>
return [] <br>
rebuilt_votes = [] <br>
for vote in votes: <br>
rv = [] <br>
for v in vote: <br>
if v not in rv: <br>
rv.append(v) <br>
assert len(rv) == len(all_candidates) <br>
rebuilt_votes.append(rv) <br>
return rebuilt_votes</p>


<p style="margin-top: 1em">@given(lists(lists(integers(min_value=1,
max_value=5)))) <br>
def test_elections_are_transitive(election): <br>
election = build_election(election) <br>
# Small elections are unlikely to be interesting <br>
assume(len(election) &gt;= 3) <br>
all_candidates = candidates(election) <br>
# Elections with fewer than three candidates certainly
can&rsquo;t exhibit <br>
# intransitivity <br>
assume(len(all_candidates) &gt;= 3)</p>

<p style="margin-top: 1em"># Now we check if the election
is transitive</p>

<p style="margin-top: 1em"># First calculate the pairwise
counts of how many prefer each candidate <br>
# to the other <br>
counts = Counter() <br>
for vote in election: <br>
for i in range(len(vote)): <br>
for j in range(i+1, len(vote)): <br>
counts[(vote[i], vote[j])] += 1</p>

<p style="margin-top: 1em"># Now look at which pairs of
candidates one has a majority over the <br>
# other and store that. <br>
graph = {} <br>
all_candidates = candidates(election) <br>
for i in all_candidates: <br>
for j in all_candidates: <br>
if counts[(i, j)] &gt; counts[(j, i)]: <br>
graph.setdefault(i, set()).add(j)</p>

<p style="margin-top: 1em"># Now for each triple assert
that it is transitive. <br>
for x in all_candidates: <br>
for y in graph.get(x, ()): <br>
for z in graph.get(y, ()): <br>
assert x not in graph.get(z, ())</p>

<p style="margin-top: 1em">The example Hypothesis gives me
on my first run (your mileage may of course vary) is:</p>

<p style="margin-top: 1em">[[3, 1, 4], [4, 3, 1], [1, 4,
3]]</p>

<p style="margin-top: 1em">Which does indeed do the job:
The majority (votes 0 and 1) prefer 3 to 1, the majority
(votes 0 and 2) prefer 1 to 4 and the majority (votes 1 and
2) prefer 4 to 3. This is in <br>
fact basically the canonical example of the voting paradox,
modulo variations on the names of candidates.</p>

<p style="margin-top: 1em">Fuzzing an HTTP API <br>
Hypothesis&rsquo;s support for testing HTTP services is
somewhat nascent. There are plans for some fully featured
things around this, but right now they&rsquo;re probably
quite far down the <br>
line.</p>

<p style="margin-top: 1em">But you can do a lot yourself
without any explicit support! Here&rsquo;s a script I wrote
to throw random data against the API for an entirely
fictitious service called Waspfinder <br>
(this is only lightly obfuscated and you can easily figure
out who I&rsquo;m actually talking about, but I don&rsquo;t
want you to run this code and hammer their API without their
permis&acirc; <br>
sion).</p>

<p style="margin-top: 1em">All this does is use Hypothesis
to generate random JSON data matching the format their API
asks for and check for 500 errors. More advanced tests which
then use the result and go <br>
on to do other things are definitely also possible.</p>

<p style="margin-top: 1em">import unittest <br>
from hypothesis import given, assume, settings, strategies
as st <br>
from collections import namedtuple <br>
import requests <br>
import os <br>
import random <br>
import time <br>
import math</p>

<p style="margin-top: 1em"># These tests will be quite slow
because we have to talk to an external <br>
# service. Also we&rsquo;ll put in a sleep between calls so
as to not hammer it. <br>
# As a result we reduce the number of test cases and turn
off the timeout. <br>
settings.default.max_examples = 100 <br>
settings.default.timeout = -1</p>

<p style="margin-top: 1em">Goal =
namedtuple(&quot;Goal&quot;, (&quot;slug&quot;,))</p>

<p style="margin-top: 1em"># We just pass in our API
credentials via environment variables. <br>
waspfinder_token = os.getenv(&rsquo;WASPFINDER_TOKEN&rsquo;)
<br>
waspfinder_user = os.getenv(&rsquo;WASPFINDER_USER&rsquo;)
<br>
assert waspfinder_token is not None <br>
assert waspfinder_user is not None</p>

<p style="margin-top: 1em">GoalData =
st.fixed_dictionaries({ <br>
&rsquo;title&rsquo;: st.text(), <br>
&rsquo;goal_type&rsquo;: st.sampled_from([ <br>
&quot;hustler&quot;, &quot;biker&quot;, &quot;gainer&quot;,
&quot;fatloser&quot;, &quot;inboxer&quot;, <br>
&quot;drinker&quot;, &quot;custom&quot;]), <br>
&rsquo;goaldate&rsquo;: st.one_of(st.none(), st.floats()),
<br>
&rsquo;goalval&rsquo;: st.one_of(st.none(), st.floats()),
<br>
&rsquo;rate&rsquo;: st.one_of(st.none(), st.floats()), <br>
&rsquo;initval&rsquo;: st.floats(), <br>
&rsquo;panic&rsquo;: st.floats(), <br>
&rsquo;secret&rsquo;: st.booleans(), <br>
&rsquo;datapublic&rsquo;: st.booleans(), <br>
})</p>

<p style="margin-top: 1em">needs2 =
[&rsquo;goaldate&rsquo;, &rsquo;goalval&rsquo;,
&rsquo;rate&rsquo;]</p>

<p style="margin-top: 1em">class
WaspfinderTest(unittest.TestCase):</p>

<p style="margin-top: 1em">@given(GoalData) <br>
def test_create_goal_dry_run(self, data): <br>
# We want slug to be unique for each run so that multiple
test runs <br>
# don&rsquo;t interfere with eachother. If for some reason
some slugs trigger <br>
# an error and others don&rsquo;t we&rsquo;ll get a Flaky
error, but that&rsquo;s OK. <br>
slug = hex(random.getrandbits(32))[2:]</p>

<p style="margin-top: 1em"># Use assume to guide us through
validation we know about, otherwise <br>
# we&rsquo;ll spend a lot of time generating boring
examples.</p>

<p style="margin-top: 1em"># Title must not be empty <br>
assume(data[&quot;title&quot;])</p>

<p style="margin-top: 1em"># Exactly two of these values
should be not None. The other will be <br>
# inferred by the API.</p>

<p style="margin-top: 1em">assume(len([1 for k in needs2 if
data[k] is not None]) == 2) <br>
for v in data.values(): <br>
if isinstance(v, float): <br>
assume(not math.isnan(v)) <br>
data[&quot;slug&quot;] = slug</p>

<p style="margin-top: 1em"># The API nicely supports a dry
run option, which means we don&rsquo;t have <br>
# to worry about the user account being spammed with lots of
fake goals <br>
# Otherwise we would have to make sure we cleaned up after
ourselves <br>
# in this test. <br>
data[&quot;dryrun&quot;] = True <br>
data[&quot;auth_token&quot;] = waspfinder_token <br>
for d, v in data.items(): <br>
if v is None: <br>
data[d] = &quot;null&quot; <br>
else: <br>
data[d] = str(v) <br>
result = requests.post( <br>
&quot;https://waspfinder.example.com/api/v1/users/&quot;
<br>
&quot;%s/goals.json&quot; % (waspfinder_user,),
data=data)</p>

<p style="margin-top: 1em"># Lets not hammer the API too
badly. This will of course make the <br>
# tests even slower than they otherwise would have been, but
that&rsquo;s <br>
# life. <br>
time.sleep(1.0)</p>

<p style="margin-top: 1em"># For the moment all we&rsquo;re
testing is that this doesn&rsquo;t generate an <br>
# internal error. If we didn&rsquo;t use the dry run option
we could have <br>
# then tried doing more with the result, but this is a good
start. <br>
self.assertNotEqual(result.status_code, 500)</p>

<p style="margin-top: 1em">if __name__ ==
&rsquo;__main__&rsquo;: <br>
unittest.main()</p>

<p style="margin-top: 1em">COMMUNITY <br>
The Hypothesis community is small for the moment but is full
of excellent people who can answer your questions and help
you out. Please do join us.</p>

<p style="margin-top: 1em">The two major places for
community discussion are:</p>

<p style="margin-top: 1em">&Acirc;&middot; The mailing
list.</p>

<p style="margin-top: 1em">&Acirc;&middot; An IRC channel,
#hypothesis on freenode, which is more active than the
mailing list.</p>

<p style="margin-top: 1em">Feel free to use these to ask
for help, provide feedback, or discuss anything remotely
Hypothesis related at all.</p>

<p style="margin-top: 1em">Code of conduct <br>
Hypothesis&rsquo;s community is an inclusive space, and
everyone in it is expected to abide by a code of
conduct.</p>

<p style="margin-top: 1em">At the high level the code of
conduct goes like this:</p>

<p style="margin-top: 1em">1. Be kind</p>

<p style="margin-top: 1em">2. Be respectful</p>

<p style="margin-top: 1em">3. Be helpful</p>

<p style="margin-top: 1em">While it is impossible to
enumerate everything that is unkind, disrespectful or
unhelpful, here are some specific things that are definitely
against the code of conduct:</p>

<p style="margin-top: 1em">1. -isms and -phobias (e.g.
racism, sexism, transphobia and homophobia) are unkind,
disrespectful and unhelpful. Just don&rsquo;t.</p>

<p style="margin-top: 1em">2. All software is broken. This
is not a moral failing on the part of the authors.
Don&rsquo;t give people a hard time for bad code.</p>

<p style="margin-top: 1em">3. It&rsquo;s OK not to know
things. Everybody was a beginner once, nobody should be made
to feel bad for it.</p>

<p style="margin-top: 1em">4. It&rsquo;s OK not to want to
know something. If you think someone&rsquo;s question is
fundamentally flawed, you should still ask permission before
explaining what they should actually be <br>
asking.</p>

<p style="margin-top: 1em">5. Note that &quot;I was just
joking&quot; is not a valid defence.</p>

<p style="margin-top: 1em">What happens when this goes
wrong? <br>
For minor infractions, I&rsquo;ll just call people on it and
ask them to apologise and not do it again. You should feel
free to do this too if you&rsquo;re comfortable doing
so.</p>

<p style="margin-top: 1em">Major infractions and repeat
offenders will be banned from the community.</p>

<p style="margin-top: 1em">Also, people who have a track
record of bad behaviour outside of the Hypothesis community
may be banned even if they obey all these rules if their
presence is making people <br>
uncomfortable.</p>

<p style="margin-top: 1em">At the current volume level
it&rsquo;s not hard for me to pay attention to the whole
community, but if you think I&rsquo;ve missed something
please feel free to alert me. You can either mes&acirc; <br>
sage me as DRMacIver on freenode or send a me an email at
david@drmaciver.com.</p>

<p style="margin-top: 1em">THE PURPOSE OF HYPOTHESIS <br>
What is Hypothesis for?</p>

<p style="margin-top: 1em">From the perspective of a user,
the purpose of Hypothesis is to make it easier for you to
write better tests.</p>

<p style="margin-top: 1em">From my perspective as the
author, that is of course also a purpose of Hypothesis, but
(if you will permit me to indulge in a touch of megalomania
for a moment), the larger pur&acirc; <br>
pose of Hypothesis is to drag the world kicking and
screaming into a new and terrifying age of high quality
software.</p>

<p style="margin-top: 1em">Software is, as they say, eating
the world. Software is also terrible. It&rsquo;s buggy,
insecure and generally poorly thought out. This combination
is clearly a recipe for disaster.</p>

<p style="margin-top: 1em">And the state of software
testing is even worse. Although it&rsquo;s fairly
uncontroversial at this point that you should be testing
your code, can you really say with a straight face <br>
that most projects you&rsquo;ve worked on are adequately
tested?</p>

<p style="margin-top: 1em">A lot of the problem here is
that it&rsquo;s too hard to write good tests. Your tests
encode exactly the same assumptions and fallacies that you
had when you wrote the code, so they <br>
miss exactly the same bugs that you missed when you wrote
the code.</p>

<p style="margin-top: 1em">Meanwhile, there are all sorts
of tools for making testing better that are basically
unused. The original Quickcheck is from 1999 and the
majority of developers have not even <br>
heard of it, let alone used it. There are a bunch of
half-baked implementations for most languages, but very few
of them are worth using.</p>

<p style="margin-top: 1em">The goal of Hypothesis is to
bring advanced testing techniques to the masses, and to
provide an implementation that is so high quality that it is
easier to use them than it is <br>
not to use them. Where I can, I will beg, borrow and steal
every good idea I can find that someone has had to make
software testing better. Where I can&rsquo;t, I will invent
new ones.</p>

<p style="margin-top: 1em">Quickcheck is the start, but I
also plan to integrate ideas from fuzz testing (a planned
future feature is to use coverage information to drive
example selection, and the example <br>
saving database is already inspired by the workflows people
use for fuzz testing), and am open to and actively seeking
out other suggestions and ideas.</p>

<p style="margin-top: 1em">The plan is to treat the social
problem of people not using these ideas as a bug to which
there is a technical solution: Does property-based testing
not match your workflow? <br>
That&rsquo;s a bug, let&rsquo;s fix it by figuring out how
to integrate Hypothesis into it. Too hard to generate custom
data for your application? That&rsquo;s a bug. Let&rsquo;s
fix it by figuring out <br>
how to make it easier, or how to take something you&rsquo;re
already using to specify your data and derive a generator
from that automatically. Find the explanations of these
advanced <br>
ideas hopelessly obtuse and hard to follow? That&rsquo;s a
bug. Let&rsquo;s provide you with an easy API that lets you
test your code better without a PhD in software
verification.</p>

<p style="margin-top: 1em">Grand ambitions, I know, and I
expect ultimately the reality will be somewhat less grand,
but so far in about three months of development, Hypothesis
has become the most solid <br>
implementation of Quickcheck ever seen in a mainstream
language (as long as we don&rsquo;t count Scala as
mainstream yet), and at the same time managed to
significantly push forward <br>
the state of the art, so I think there&rsquo;s reason to be
optimistic.</p>

<p style="margin-top: 1em">TESTIMONIALS <br>
This is a page for listing people who are using Hypothesis
and how excited they are about that. If that&rsquo;s you and
your name is not on the list, this file is in Git and
I&rsquo;d love <br>
it if you sent me a pull request to fix that.</p>

<p style="margin-top: 1em">Stripe <br>
At Stripe we use Hypothesis to test every piece of our
machine learning model training pipeline (powered by
scikit). Before we migrated, our tests were filled with
hand-crafted <br>
pandas Dataframes that weren&rsquo;t representative at all
of our actual very complex data. Because we needed to craft
examples for each test, we took the easy way out and lived
with <br>
extremely low test coverage.</p>

<p style="margin-top: 1em">Hypothesis changed all that.
Once we had our strategies for generating Dataframes of
features it became trivial to slightly customize each
strategy for new tests. Our coverage is <br>
now close to 90%.</p>

<p style="margin-top: 1em">Full-stop, property-based
testing is profoundly more powerful - and has caught or
prevented far more bugs - than our old style of
example-based testing.</p>

<p style="margin-top: 1em">Kristian Glass - Director of
Technology at LaterPay GmbH <br>
Hypothesis has been brilliant for expanding the coverage of
our test cases, and also for making them much easier to read
and understand, so we&rsquo;re sure we&rsquo;re testing the
things we <br>
want in the way we want.</p>

<p style="margin-top: 1em">Seth Morton <br>
When I first heard about Hypothesis, I knew I had to include
it in my two open-source Python libraries, natsort and
fastnumbers . Quite frankly, I was a little appalled at the
<br>
number of bugs and &quot;holes&quot; I found in the code. I
can now say with confidence that my libraries are more
robust to &quot;the wild.&quot; In addition, Hypothesis gave
me the confidence to <br>
expand these libraries to fully support Unicode input, which
I never would have had the stomach for without such thorough
testing capabilities. Thanks!</p>

<p style="margin-top: 1em">Sixty North <br>
At Sixty North we use Hypothesis for testing Segpy an open
source Python library for shifting data between Python data
structures and SEG Y files which contain geophysical data
<br>
from the seismic reflection surveys used in oil and gas
exploration.</p>

<p style="margin-top: 1em">This is our first experience of
property-based testing &acirc; as opposed to example-based
testing. Not only are our tests more powerful, they are also
much better explanations of <br>
what we expect of the production code. In fact, the tests
are much closer to being specifications. Hypothesis has
located real defects in our code which went undetected by
tra&acirc; <br>
ditional test cases, simply because Hypothesis is more
relentlessly devious about test case generation than us mere
humans! We found Hypothesis particularly beneficial for
Segpy <br>
because SEG Y is an antiquated format that uses legacy text
encodings (EBCDIC) and even a legacy floating point format
we implemented from scratch in Python.</p>

<p style="margin-top: 1em">Hypothesis is sure to find a
place in most of our future Python codebases and many
existing ones too.</p>

<p style="margin-top: 1em">mulkieran <br>
Just found out about this excellent QuickCheck for Python
implementation and ran up a few tests for my bytesize
package last night. Refuted a few hypotheses in the
process.</p>

<p style="margin-top: 1em">Looking forward to using it with
a bunch of other projects as well.</p>

<p style="margin-top: 1em">Adam Johnson <br>
I have written a small library to serialize dicts to
MariaDB&rsquo;s dynamic columns binary format,
mariadb-dyncol. When I first developed it, I thought I had
tested it really well - <br>
there were hundreds of test cases, some of them even taken
from MariaDB&rsquo;s test suite itself. I was ready to
release.</p>

<p style="margin-top: 1em">Lucky for me, I tried Hypothesis
with David at the PyCon UK sprints. Wow! It found bug after
bug after bug. Even after a first release, I thought of a
way to make the tests do <br>
more validation, which revealed a further round of bugs!
Most impressively, Hypothesis found a complicated off-by-one
error in a condition with 4095 versus 4096 bytes of data -
<br>
something that I would never have found.</p>

<p style="margin-top: 1em">Long live Hypothesis! (Or at
least, property-based testing).</p>

<p style="margin-top: 1em">Josh Bronson <br>
Adopting Hypothesis improved bidict&rsquo;s test coverage
and significantly increased our ability to make changes to
the code with confidence that correct behavior would be
preserved. <br>
Thank you, David, for the great testing tool.</p>

<p style="margin-top: 1em">Cory Benfield <br>
Hypothesis is the single most powerful tool in my toolbox
for working with algorithmic code, or any software that
produces predictable output from a wide range of sources.
When <br>
using it with Priority, Hypothesis consistently found errors
in my assumptions and extremely subtle bugs that would have
taken months of real-world use to locate. In some cases,
<br>
Hypothesis found subtle deviations from the correct output
of the algorithm that may never have been noticed at
all.</p>

<p style="margin-top: 1em">When it comes to validating the
correctness of your tools, nothing comes close to the
thoroughness and power of Hypothesis.</p>

<p style="margin-top: 1em">Jon Moore <br>
One extremely satisfied user here. Hypothesis is a really
solid implementation of property-based testing, adapted well
to Python, and with good features such as failure-case <br>
shrinkers. I first used it on a project where we needed to
verify that a vendor&rsquo;s Python and non-Python
implementations of an algorithm matched, and it found about
a dozen cases <br>
that previous example-based testing and code inspections had
not. Since then I&rsquo;ve been evangelizing for it at our
firm.</p>

<p style="margin-top: 1em">Russel Winder <br>
I am using Hypothesis as an integral part of my Python
workshops. Testing is an integral part of Python programming
and whilst unittest and, better, py.test can handle
exam&acirc; <br>
ple-based testing, property-based testing is increasingly
far more important than example-base testing, and Hypothesis
fits the bill.</p>

<p style="margin-top: 1em">Wellfire Interactive <br>
We&rsquo;ve been using Hypothesis in a variety of client
projects, from testing Django-related functionality to
domain-specific calculations. It both speeds up and
simplifies the test&acirc; <br>
ing process since there&rsquo;s so much less tedious and
error-prone work to do in identifying edge cases. Test
coverage is nice but test depth is even nicer, and
it&rsquo;s much easier to <br>
get meaningful test depth using Hypothesis.</p>

<p style="margin-top: 1em">Your name goes here <br>
I know there are many more, because I keep finding out about
new people I&rsquo;d never even heard of using Hypothesis.
If you&rsquo;re looking to way to give back to a tool you
love, adding <br>
your name here only takes a moment and would really help a
lot. As per instructions at the top, just send me a pull
request and I&rsquo;ll add you to the list.</p>

<p style="margin-top: 1em">OPEN SOURCE PROJECTS USING
HYPOTHESIS <br>
The following is a non-exhaustive list of open source
projects I know are using Hypothesis. If you&rsquo;re aware
of any others please add them to the list! The only
inclusion crite&acirc; <br>
rion right now is that if it&rsquo;s a Python library then
it should be available on pypi.</p>

<p style="margin-top: 1em">&Acirc;&middot; aur</p>

<p style="margin-top: 1em">&Acirc;&middot; axelrod</p>

<p style="margin-top: 1em">&Acirc;&middot; bidict</p>

<p style="margin-top: 1em">&Acirc;&middot; binaryornot</p>

<p style="margin-top: 1em">&Acirc;&middot; brotlipy</p>

<p style="margin-top: 1em">&Acirc;&middot; chardet</p>

<p style="margin-top: 1em">&Acirc;&middot; cmph-cffi</p>

<p style="margin-top: 1em">&Acirc;&middot; cryptography</p>

<p style="margin-top: 1em">&Acirc;&middot;
dbus-signature-pyparsing</p>

<p style="margin-top: 1em">&Acirc;&middot; fastnumbers</p>

<p style="margin-top: 1em">&Acirc;&middot; flocker</p>

<p style="margin-top: 1em">&Acirc;&middot; flownetpy</p>

<p style="margin-top: 1em">&Acirc;&middot; funsize</p>

<p style="margin-top: 1em">&Acirc;&middot; fusion-index</p>

<p style="margin-top: 1em">&Acirc;&middot; hyper-h2</p>

<p style="margin-top: 1em">&Acirc;&middot;
into-dbus-python</p>

<p style="margin-top: 1em">&Acirc;&middot; justbases</p>

<p style="margin-top: 1em">&Acirc;&middot; justbytes</p>

<p style="margin-top: 1em">&Acirc;&middot;
mariadb-dyncol</p>

<p style="margin-top: 1em">&Acirc;&middot; mercurial</p>

<p style="margin-top: 1em">&Acirc;&middot; natsort</p>

<p style="margin-top: 1em">&Acirc;&middot; pretext</p>

<p style="margin-top: 1em">&Acirc;&middot; priority</p>

<p style="margin-top: 1em">&Acirc;&middot; PyCEbox</p>

<p style="margin-top: 1em">&Acirc;&middot; PyPy</p>

<p style="margin-top: 1em">&Acirc;&middot; pyrsistent</p>

<p style="margin-top: 1em">&Acirc;&middot; pyudev</p>

<p style="margin-top: 1em">&Acirc;&middot; qutebrowser</p>

<p style="margin-top: 1em">&Acirc;&middot; RubyMarshal</p>

<p style="margin-top: 1em">&Acirc;&middot; Segpy</p>

<p style="margin-top: 1em">&Acirc;&middot; simoa</p>

<p style="margin-top: 1em">&Acirc;&middot; srt</p>

<p style="margin-top: 1em">&Acirc;&middot; tchannel</p>

<p style="margin-top: 1em">&Acirc;&middot; vdirsyncer</p>

<p style="margin-top: 1em">&Acirc;&middot;
wcag-contrast-ratio</p>

<p style="margin-top: 1em">&Acirc;&middot; yacluster</p>

<p style="margin-top: 1em">&Acirc;&middot; yturl</p>

<p style="margin-top: 1em">PROJECTS EXTENDING HYPOTHESIS
<br>
The following is a non-exhaustive list of open source
projects that make Hypothesis strategies available. If
you&rsquo;re aware of any others please add them the list!
The only inclu&acirc; <br>
sion criterion right now is that if it&rsquo;s a Python
library then it should be available on pypi.</p>

<p style="margin-top: 1em">&Acirc;&middot;
hs-dbus-signature - strategy to generate arbitrary D-Bus
signatures</p>

<p style="margin-top: 1em">&Acirc;&middot; hypothesis-regex
- strategy to generate strings that match given regular
expression.</p>

<p style="margin-top: 1em">&Acirc;&middot;
lollipop-hypothesis - strategy to generate data based on
Lollipop schema definitions.</p>

<p style="margin-top: 1em">If you&rsquo;re thinking about
writing an extension, consider naming it
hypothesis-{something} - a standard prefix makes the
community more visible and searching for extensions
easier.</p>

<p style="margin-top: 1em">CHANGELOG <br>
This is a record of all past Hypothesis releases and what
went into them, in reverse chronological order. All previous
releases should still be available on pip.</p>

<p style="margin-top: 1em">Hypothesis APIs come in three
flavours:</p>

<p style="margin-top: 1em">&Acirc;&middot; Public:
Hypothesis releases since 1.0 are semantically versioned
with respect to these parts of the API. These will not break
except between major version bumps. All APIs men&acirc; <br>
tioned in this documentation are public unless explicitly
noted otherwise.</p>

<p style="margin-top: 1em">&Acirc;&middot; Semi-public:
These are APIs that are considered ready to use but are not
wholly nailed down yet. They will not break in patch
releases and will usually not break in minor <br>
releases, but when necessary minor releases may break
semi-public APIs.</p>

<p style="margin-top: 1em">&Acirc;&middot; Internal: These
may break at any time and you really should not use them at
all.</p>

<p style="margin-top: 1em">You should generally assume that
an API is internal unless you have specific information to
the contrary.</p>

<p style="margin-top: 1em">3.12.0 - 2017-07-07 <br>
This release makes some major internal changes to how
Hypothesis represents data internally, as a prelude to some
major engine changes that should improve data quality. There
are <br>
no API changes, but it&rsquo;s a significant enough internal
change that a minor version bump seemed warranted.</p>

<p style="margin-top: 1em">User facing impact should be
fairly mild, but includes:</p>

<p style="margin-top: 1em">&Acirc;&middot; All existing
examples in the database will probably be invalidated.
Hypothesis handles this automatically, so you don&rsquo;t
need to do anything, but if you see all your examples <br>
disappear that&rsquo;s why.</p>

<p style="margin-top: 1em">&Acirc;&middot; Almost all data
distributions have changed significantly. Possibly for the
better, possibly for the worse. This may result in new bugs
being found, but it may also result in <br>
Hypothesis being unable to find bugs it previously did.</p>

<p style="margin-top: 1em">&Acirc;&middot; Data generation
may be somewhat faster if your existing bottleneck was in
draw_bytes (which is often the case for large examples).</p>

<p style="margin-top: 1em">&Acirc;&middot; Shrinking will
probably be slower, possibly significantly.</p>

<p style="margin-top: 1em">If you notice any effects you
consider to be a significant regression, please open an
issue about them.</p>

<p style="margin-top: 1em">3.11.6 - 2017-06-19 <br>
This release involves no functionality changes, but is the
first to ship wheels as well as an sdist.</p>

<p style="margin-top: 1em">3.11.5 - 2017-06-18 <br>
This release provides a performance improvement to
shrinking. For cases where there is some non-trivial
&quot;boundary&quot; value (e.g. the bug happens for all
values greater than some <br>
other value), shrinking should now be substantially faster.
Other types of bug will likely see improvements too.</p>

<p style="margin-top: 1em">This may also result in some
changes to the quality of the final examples - it may
sometimes be better, but is more likely to get slightly
worse in some edge cases. If you see <br>
any examples where this happens in practice, please report
them.</p>

<p style="margin-top: 1em">3.11.4 - 2017-06-17 <br>
This is a bugfix release: Hypothesis now prints explicit
examples when running in verbose mode. (issue #313)</p>

<p style="margin-top: 1em">3.11.3 - 2017-06-11 <br>
This is a bugfix release: Hypothesis no longer emits a
warning if you try to use sampled_from() with
python:collections.OrderedDict. (issue #688)</p>

<p style="margin-top: 1em">3.11.2 - 2017-06-10 <br>
This is a documentation release. Several outdated snippets
have been updated or removed, and many cross-references are
now hyperlinks.</p>

<p style="margin-top: 1em">3.11.1 - 2017-05-28 <br>
This is a minor ergonomics release. Tracebacks shown by
pytest no longer include Hypothesis internals for test
functions decorated with @given.</p>

<p style="margin-top: 1em">3.11.0 - 2017-05-23 <br>
This is a feature release, adding datetime-related
strategies to the core strategies.</p>

<p style="margin-top: 1em">timezones() allows you to sample
pytz timezones from the Olsen database. Use directly in a
recipe for tz-aware datetimes, or compose with none() to
allow a mix of aware and <br>
naive output.</p>

<p style="margin-top: 1em">The new dates(), times(),
datetimes(), and timedeltas() strategies are all constrained
by objects of their type. This means that you can generate
dates bounded by a single day <br>
(i.e. a single date), or datetimes constrained to the
microsecond.</p>

<p style="margin-top: 1em">times() and datetimes() take an
optional timezones= argument, which defaults to none() for
naive times. You can use our extra strategy based on pytz,
or roll your own timezones <br>
strategy with dateutil or even the standard library.</p>

<p style="margin-top: 1em">The old dates, times, and
datetimes strategies in hypothesis.extra.datetimes are
deprecated in favor of the new core strategies, which are
more flexible and have no dependencies.</p>

<p style="margin-top: 1em">3.10.0 - 2017-05-22 <br>
Hypothesis now uses python:inspect.getfullargspec()
internally. On Python 2, there are no visible changes.</p>

<p style="margin-top: 1em">On Python 3 @given and
@composite now preserve PEP 3107 annotations on the
decorated function. Keyword-only arguments are now either
handled correctly (e.g. @composite), or <br>
caught in validation instead of silently discarded or
raising an unrelated error later (e.g. @given).</p>

<p style="margin-top: 1em">3.9.1 - 2017-05-22 <br>
This is a bugfix release: the default field mapping for a
DateTimeField in the Django extra now respects the USE_TZ
setting when choosing a strategy.</p>

<p style="margin-top: 1em">3.9.0 - 2017-05-19 <br>
This is feature release, expanding the capabilities of the
decimals() strategy.</p>

<p style="margin-top: 1em">&Acirc;&middot; The new
(optional) places argument allows you to generate decimals
with a certain number of places (e.g. cents, thousandths,
satoshis).</p>

<p style="margin-top: 1em">&Acirc;&middot; If
allow_infinity is None, setting min_bound no longer excludes
positive infinity and setting max_value no longer excludes
negative infinity.</p>

<p style="margin-top: 1em">&Acirc;&middot; All of NaN,
-Nan, sNaN, and -sNaN may now be drawn if allow_nan is True,
or if allow_nan is None and min_value or max_value is
None.</p>

<p style="margin-top: 1em">&Acirc;&middot; min_value and
max_value may be given as decimal strings, e.g.
&quot;1.234&quot;.</p>

<p style="margin-top: 1em">3.8.5 - 2017-05-16 <br>
Hypothesis now imports python:sqlite3 when a SQLite database
is used, rather than at module load, improving compatibility
with Python implementations compiled without SQLite
sup&acirc; <br>
port (such as BSD or Jython).</p>

<p style="margin-top: 1em">3.8.4 - 2017-05-16 <br>
This is a compatibility bugfix release. sampled_from no
longer raises a deprecation warning when sampling from an
Enum, as all enums have a reliable iteration order.</p>

<p style="margin-top: 1em">3.8.3 - 2017-05-09 <br>
This release removes a version check for older versions of
pytest when using the Hypothesis pytest plugin. The pytest
plugin will now run unconditionally on all versions of <br>
pytest. This breaks compatibility with any version of pytest
prior to 2.7.0 (which is more than two years old).</p>

<p style="margin-top: 1em">The primary reason for this
change is that the version check was a frequent source of
breakage when pytest change their versioning scheme. If you
are not working on pytest itself <br>
and are not running a very old version of it, this release
probably doesn&rsquo;t affect you.</p>

<p style="margin-top: 1em">3.8.2 - 2017-04-26 <br>
This is a code reorganisation release that moves some
internal test helpers out of the main source tree so as to
not have changes to them trigger releases in future.</p>

<p style="margin-top: 1em">3.8.1 - 2017-04-26 <br>
This is a documentation release. Almost all code examples
are now doctests checked in CI, eliminating stale
examples.</p>

<p style="margin-top: 1em">3.8.0 - 2017-04-23 <br>
This is a feature release, adding the iterables() strategy,
equivalent to lists(...).map(iter) but with a much more
useful repr. You can use this strategy to check that code
<br>
doesn&rsquo;t accidentally depend on sequence properties
such as indexing support or repeated iteration.</p>

<p style="margin-top: 1em">3.7.4 - 2017-04-22 <br>
This is a bug fix release for a single bug:</p>

<p style="margin-top: 1em">&Acirc;&middot; In 3.7.3, using
@example and a pytest fixture in the same test could cause
the test to fail to fill the arguments, and throw a
TypeError.</p>

<p style="margin-top: 1em">3.7.3 - 2017-04-21 <br>
This release should include no user visible changes and is
purely a refactoring release. This modularises the behaviour
of the core given() function, breaking it up into smaller
<br>
and more accessible parts, but its actual behaviour should
remain unchanged.</p>

<p style="margin-top: 1em">3.7.2 - 2017-04-21 <br>
This reverts an undocumented change in 3.7.1 which broke
installation on debian stable: The specifier for the
hypothesis[django] extra_requires had introduced a wild
card, which <br>
was not supported on the default version of pip.</p>

<p style="margin-top: 1em">3.7.1 - 2017-04-21 <br>
This is a bug fix and internal improvements release.</p>

<p style="margin-top: 1em">&Acirc;&middot; In particular
Hypothesis now tracks a tree of where it has already
explored. This allows it to avoid some classes of duplicate
examples, and significantly improves the perfor&acirc; <br>
mance of shrinking failing examples by allowing it to skip
some shrinks that it can determine can&rsquo;t possibly
work.</p>

<p style="margin-top: 1em">&Acirc;&middot; Hypothesis will
no longer seed the global random arbitrarily unless you have
asked it to using random_module()</p>

<p style="margin-top: 1em">&Acirc;&middot; Shrinking would
previously have not worked correctly in some special cases
on Python 2, and would have resulted in suboptimal
examples.</p>

<p style="margin-top: 1em">3.7.0 - 2017-03-20 <br>
This is a feature release.</p>

<p style="margin-top: 1em">New features:</p>

<p style="margin-top: 1em">&Acirc;&middot; Rule based
stateful testing now has an @invariant decorator that
specifies methods that are run after init and after every
step, allowing you to encode properties that should <br>
be true at all times. Thanks to Tom Prince for this
feature.</p>

<p style="margin-top: 1em">&Acirc;&middot; The decimals()
strategy now supports allow_nan and allow_infinity
flags.</p>

<p style="margin-top: 1em">&Acirc;&middot; There are
significantly more strategies available for numpy, including
for generating arbitrary data types. Thanks to Zac Hatfield
Dodds for this feature.</p>

<p style="margin-top: 1em">&Acirc;&middot; When using the
data() strategy you can now add a label as an argument to
draw(), which will be printed along with the value when an
example fails. Thanks to Peter Inglesby for <br>
this feature.</p>

<p style="margin-top: 1em">Bug fixes:</p>

<p style="margin-top: 1em">&Acirc;&middot; Bug fix:
composite() now preserves functions&rsquo; docstrings.</p>

<p style="margin-top: 1em">&Acirc;&middot; The build is now
reproducible and doesn&rsquo;t depend on the path you build
it from. Thanks to Chris Lamb for this feature.</p>

<p style="margin-top: 1em">&Acirc;&middot; numpy strategies
for the void data type did not work correctly. Thanks to Zac
Hatfield Dodds for this fix.</p>

<p style="margin-top: 1em">There have also been a number of
performance optimizations:</p>

<p style="margin-top: 1em">&Acirc;&middot; The
permutations() strategy is now significantly faster to use
for large lists (the underlying algorithm has gone from
O(n^2) to O(n)).</p>

<p style="margin-top: 1em">&Acirc;&middot; Shrinking of
failing test cases should have got significantly faster in
some circumstances where it was previously struggling for a
long time.</p>

<p style="margin-top: 1em">&Acirc;&middot; Example
generation now involves less indirection, which results in a
small speedup in some cases (small enough that you
won&rsquo;t really notice it except in pathological
cases).</p>

<p style="margin-top: 1em">3.6.1 - 2016-12-20 <br>
This release fixes a dependency problem and makes some small
behind the scenes improvements.</p>

<p style="margin-top: 1em">&Acirc;&middot; The fake-factory
dependency was renamed to faker. If you were depending on it
through hypothesis[django] or hypothesis[fake-factory]
without pinning it yourself then it would <br>
have failed to install properly. This release changes it so
that hypothesis[fakefactory] (which can now also be
installed as hypothesis[faker]) will install the renamed
faker <br>
package instead.</p>

<p style="margin-top: 1em">&Acirc;&middot; This release
also removed the dependency of hypothesis[django] on
hypothesis[fakefactory] - it was only being used for emails.
These now use a custom strategy that isn&rsquo;t from <br>
fakefactory. As a result you should also see performance
improvements of tests which generated User objects or other
things with email fields, as well as better shrinking of
<br>
email addresses.</p>

<p style="margin-top: 1em">&Acirc;&middot; The distribution
of code using nested calls to one_of() or the | operator for
combining strategies has been improved, as branches are now
flattened to give a more uniform dis&acirc; <br>
tribution.</p>

<p style="margin-top: 1em">&Acirc;&middot; Examples using
composite() or .flatmap should now shrink better. In
particular this will affect things which work by first
generating a length and then generating that many <br>
items, which have historically not shrunk very well.</p>

<p style="margin-top: 1em">3.6.0 - 2016-10-31 <br>
This release reverts Hypothesis to its old pretty printing
of lambda functions based on attempting to extract the
source code rather than decompile the bytecode. This is
unfor&acirc; <br>
tunately slightly inferior in some cases and may result in
you occasionally seeing things like lambda x:
&lt;unknown&gt; in statistics reports and strategy
reprs.</p>

<p style="margin-top: 1em">This removes the dependencies on
uncompyle6, xdis and spark-parser.</p>

<p style="margin-top: 1em">The reason for this is that the
new functionality was based on uncompyle6, which turns out
to introduce a hidden GPLed dependency - it in turn depended
on xdis, and although the <br>
library was licensed under the MIT license, it contained
some GPL licensed source code and thus should have been
released under the GPL.</p>

<p style="margin-top: 1em">My interpretation is that
Hypothesis itself was never in violation of the GPL (because
the license it is under, the Mozilla Public License v2, is
fully compatible with being <br>
included in a GPL licensed work), but I have not consulted a
lawyer on the subject. Regardless of the answer to this
question, adding a GPLed dependency will likely cause a lot
<br>
of users of Hypothesis to inadvertently be in violation of
the GPL.</p>

<p style="margin-top: 1em">As a result, if you are running
Hypothesis 3.5.x you really should upgrade to this release
immediately.</p>

<p style="margin-top: 1em">3.5.3 - 2016-10-05 <br>
This is a bug fix release.</p>

<p style="margin-top: 1em">Bugs fixed:</p>

<p style="margin-top: 1em">&Acirc;&middot; If the same test
was running concurrently in two processes and there were
examples already in the test database which no longer
failed, Hypothesis would sometimes fail with a <br>
FileNotFoundError (IOError on Python 2) because an example
it was trying to read was deleted before it was read. (issue
#372).</p>

<p style="margin-top: 1em">&Acirc;&middot; Drawing from an
integers() strategy with both a min_value and a max_value
would reject too many examples needlessly. Now it repeatedly
redraws until satisfied. (pull request <br>
#366. Thanks to Calen Pennington for the contribution).</p>

<p style="margin-top: 1em">3.5.2 - 2016-09-24 <br>
This is a bug fix release.</p>

<p style="margin-top: 1em">&Acirc;&middot; The Hypothesis
pytest plugin broke pytest support for doctests. Now it
doesn&rsquo;t.</p>

<p style="margin-top: 1em">3.5.1 - 2016-09-23 <br>
This is a bug fix release.</p>

<p style="margin-top: 1em">&Acirc;&middot; Hypothesis now
runs cleanly in -B and -BB modes, avoiding mixing bytes and
unicode.</p>

<p style="margin-top: 1em">&Acirc;&middot;
python:unittest.TestCase tests would not have shown up in
the new statistics mode. Now they do.</p>

<p style="margin-top: 1em">&Acirc;&middot; Similarly,
stateful tests would not have shown up in statistics and now
they do.</p>

<p style="margin-top: 1em">&Acirc;&middot; Statistics now
print with pytest node IDs (the names you&rsquo;d get in
pytest verbose mode).</p>

<p style="margin-top: 1em">3.5.0 - 2016-09-22 <br>
This is a feature release.</p>

<p style="margin-top: 1em">&Acirc;&middot; fractions() and
decimals() strategies now support min_value and max_value
parameters. Thanks go to Anne Mulhern for the development of
this feature.</p>

<p style="margin-top: 1em">&Acirc;&middot; The Hypothesis
pytest plugin now supports a --hypothesis-show-statistics
parameter that gives detailed statistics about the tests
that were run. Huge thanks to Jean-Louis Fuchs <br>
and Adfinis-SyGroup for funding the development of this
feature.</p>

<p style="margin-top: 1em">&Acirc;&middot; There is a new
event() function that can be used to add custom
statistics.</p>

<p style="margin-top: 1em">Additionally there have been
some minor bug fixes:</p>

<p style="margin-top: 1em">&Acirc;&middot; In some cases
Hypothesis should produce fewer duplicate examples (this
will mostly only affect cases with a single parameter).</p>

<p style="margin-top: 1em">&Acirc;&middot; py.test command
line parameters are now under an option group for Hypothesis
(thanks to David Keijser for fixing this)</p>

<p style="margin-top: 1em">&Acirc;&middot; Hypothesis would
previously error if you used PEP 3107 function annotations
on your tests under Python 3.4.</p>

<p style="margin-top: 1em">&Acirc;&middot; The repr of many
strategies using lambdas has been improved to include the
lambda body (this was previously supported in many but not
all cases).</p>

<p style="margin-top: 1em">3.4.2 - 2016-07-13 <br>
This is a bug fix release, fixing a number of problems with
the settings system:</p>

<p style="margin-top: 1em">&Acirc;&middot; Test functions
defined using @given can now be called from other threads
(issue #337)</p>

<p style="margin-top: 1em">&Acirc;&middot; Attempting to
delete a settings property would previously have silently
done the wrong thing. Now it raises an AttributeError.</p>

<p style="margin-top: 1em">&Acirc;&middot; Creating a
settings object with a custom database_file parameter was
silently getting ignored and the default was being used
instead. Now it&rsquo;s not.</p>

<p style="margin-top: 1em">3.4.1 - 2016-07-07 <br>
This is a bug fix release for a single bug:</p>

<p style="margin-top: 1em">&Acirc;&middot; On Windows when
running two Hypothesis processes in parallel (e.g. using
pytest-xdist) they could race with each other and one would
raise an exception due to the non-atomic <br>
nature of file renaming on Windows and the fact that you
can&rsquo;t rename over an existing file. This is now
fixed.</p>

<p style="margin-top: 1em">3.4.0 - 2016-05-27 <br>
This release is entirely provided by Lucas Wiman:</p>

<p style="margin-top: 1em">Strategies constructed by
models() will now respect much more of Django&rsquo;s
validations out of the box. Wherever possible full_clean()
should succeed.</p>

<p style="margin-top: 1em">In particular:</p>

<p style="margin-top: 1em">&Acirc;&middot; The max_length,
blank and choices kwargs are now respected.</p>

<p style="margin-top: 1em">&Acirc;&middot; Add support for
DecimalField.</p>

<p style="margin-top: 1em">&Acirc;&middot; If a field
includes validators, the list of validators are used to
filter the field strategy.</p>

<p style="margin-top: 1em">3.3.0 - 2016-05-27 <br>
This release went wrong and is functionally equivalent to
3.2.0. Ignore it.</p>

<p style="margin-top: 1em">3.2.0 - 2016-05-19 <br>
This is a small single-feature release:</p>

<p style="margin-top: 1em">&Acirc;&middot; All tests using
@given now fix the global random seed. This removes the
health check for that. If a non-zero seed is required for
the final falsifying example, it will be <br>
reported. Otherwise Hypothesis will assume randomization was
not a significant factor for the test and be silent on the
subject. If you use random_module() this will continue <br>
to work and will always display the seed.</p>

<p style="margin-top: 1em">3.1.3 - 2016-05-01 <br>
Single bug fix release</p>

<p style="margin-top: 1em">&Acirc;&middot; Another charmap
problem. In 3.1.2 text() and characters() would break on
systems which had /tmp mounted on a different partition than
the Hypothesis storage directory (usually <br>
in home). This fixes that.</p>

<p style="margin-top: 1em">3.1.2 - 2016-04-30 <br>
Single bug fix release:</p>

<p style="margin-top: 1em">&Acirc;&middot; Anything which
used a text() or characters() strategy was broken on Windows
and I hadn&rsquo;t updated appveyor to use the new
repository location so I didn&rsquo;t notice. This is now
<br>
fixed and windows support should work correctly.</p>

<p style="margin-top: 1em">3.1.1 - 2016-04-29 <br>
Minor bug fix release.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix concurrency
issue when running tests that use text() from multiple
processes at once (issue #302, thanks to Alex Chan).</p>

<p style="margin-top: 1em">&Acirc;&middot; Improve
performance of code using lists() with max_size (thanks to
Cristi Cobzarenco).</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix install on
Python 2 with ancient versions of pip so that it installs
the enum34 backport (thanks to Donald Stufft for telling me
how to do this).</p>

<p style="margin-top: 1em">&Acirc;&middot; Remove
duplicated __all__ exports from hypothesis.strategies
(thanks to Pi&Atilde;&laquo;t Delport).</p>

<p style="margin-top: 1em">&Acirc;&middot; Update headers
to point to new repository location.</p>

<p style="margin-top: 1em">&Acirc;&middot; Allow use of
strategies that can&rsquo;t be used in find() (e.g.
choices()) in stateful testing.</p>

<p style="margin-top: 1em">3.1.0 - 2016-03-06 <br>
&Acirc;&middot; Add a nothing() strategy that never
successfully generates values.</p>

<p style="margin-top: 1em">&Acirc;&middot; sampled_from()
and one_of() can both now be called with an empty argument
list, in which case they also never generate any values.</p>

<p style="margin-top: 1em">&Acirc;&middot; one_of() may now
be called with a single argument that is a collection of
strategies as well as as varargs.</p>

<p style="margin-top: 1em">&Acirc;&middot; Add a runner()
strategy which returns the instance of the current test
object if there is one.</p>

<p style="margin-top: 1em">&Acirc;&middot;
&rsquo;Bundle&rsquo; for RuleBasedStateMachine is now a
normal(ish) strategy and can be used as such.</p>

<p style="margin-top: 1em">&Acirc;&middot; Tests using
RuleBasedStateMachine should now shrink significantly
better.</p>

<p style="margin-top: 1em">&Acirc;&middot; Hypothesis now
uses a pretty-printing library internally, compatible with
IPython&rsquo;s pretty printing protocol (actually using the
same code). This may improve the quality of <br>
output in some cases.</p>

<p style="margin-top: 1em">&Acirc;&middot; As a
&rsquo;phases&rsquo; setting that allows more fine grained
control over which parts of the process Hypothesis runs</p>

<p style="margin-top: 1em">&Acirc;&middot; Add a
suppress_health_check setting which allows you to turn off
specific health checks in a fine grained manner.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix a bug where
lists of non fixed size would always draw one more element
than they included. This mostly didn&rsquo;t matter, but if
would cause problems with empty strategies or <br>
ones with side effects.</p>

<p style="margin-top: 1em">&Acirc;&middot; Add a mechanism
to the Django model generator to allow you to explicitly
request the default value (thanks to Jeremy Thurgood for
this one).</p>

<p style="margin-top: 1em">3.0.5 - 2016-02-25 <br>
&Acirc;&middot; Fix a bug where Hypothesis would now error
on py.test development versions.</p>

<p style="margin-top: 1em">3.0.4 - 2016-02-24 <br>
&Acirc;&middot; Fix a bug where Hypothesis would error when
running on Python 2.7.3 or earlier because it was trying to
pass a python:bytearray object to python:struct.unpack()
(which is only <br>
supported since 2.7.4).</p>

<p style="margin-top: 1em">3.0.3 - 2016-02-23 <br>
&Acirc;&middot; Fix version parsing of py.test to work with
py.test release candidates</p>

<p style="margin-top: 1em">&Acirc;&middot; More general
handling of the health check problem where things could fail
because of a cache miss - now one &quot;free&quot; example
is generated before the start of the health check <br>
run.</p>

<p style="margin-top: 1em">3.0.2 - 2016-02-18 <br>
&Acirc;&middot; Under certain circumstances, strategies
involving text() buried inside some other strategy (e.g.
text().filter(...) or recursive(text(), ...)) would cause a
test to fail its <br>
health checks the first time it ran. This was caused by
having to compute some related data and cache it to disk. On
travis or anywhere else where the .hypothesis directory was
<br>
recreated this would have caused the tests to fail their
health check on every run. This is now fixed for all the
known cases, although there could be others lurking.</p>

<p style="margin-top: 1em">3.0.1 - 2016-02-18 <br>
&Acirc;&middot; Fix a case where it was possible to trigger
an &quot;Unreachable&quot; assertion when running certain
flaky stateful tests.</p>

<p style="margin-top: 1em">&Acirc;&middot; Improve
shrinking of large stateful tests by eliminating a case
where it was hard to delete early steps.</p>

<p style="margin-top: 1em">&Acirc;&middot; Improve
efficiency of drawing binary(min_size=n, max_size=n)
significantly by provide a custom implementation for fixed
size blocks that can bypass a lot of machinery.</p>

<p style="margin-top: 1em">&Acirc;&middot; Set default home
directory based on the current working directory at the
point Hypothesis is imported, not whenever the function
first happens to be called.</p>

<p style="margin-top: 1em">3.0.0 - 2016-02-17 <br>
Codename: This really should have been 2.1.</p>

<p style="margin-top: 1em">Externally this looks like a
very small release. It has one small breaking change that
probably doesn&rsquo;t affect anyone at all (some behaviour
that never really worked correctly is <br>
now outright forbidden) but necessitated a major version
bump and one visible new feature.</p>

<p style="margin-top: 1em">Internally this is a complete
rewrite. Almost nothing other than the public API is the
same.</p>

<p style="margin-top: 1em">New features:</p>

<p style="margin-top: 1em">&Acirc;&middot; Addition of
data() strategy which allows you to draw arbitrary data
interactively within the test.</p>

<p style="margin-top: 1em">&Acirc;&middot; New
&quot;exploded&quot; database format which allows you to
more easily check the example database into a source
repository while supporting merging.</p>

<p style="margin-top: 1em">&Acirc;&middot; Better
management of how examples are saved in the database.</p>

<p style="margin-top: 1em">&Acirc;&middot; Health checks
will now raise as errors when they fail. It was too easy to
have the warnings be swallowed entirely.</p>

<p style="margin-top: 1em">New limitations:</p>

<p style="margin-top: 1em">&Acirc;&middot; choices() and
streaming() strategies may no longer be used with find().
Neither may data() (this is the change that necessitated a
major version bump).</p>

<p style="margin-top: 1em">Feature removal:</p>

<p style="margin-top: 1em">&Acirc;&middot; The
ForkingTestCase executor has gone away. It may return in
some more working form at a later date.</p>

<p style="margin-top: 1em">Performance improvements:</p>

<p style="margin-top: 1em">&Acirc;&middot; A new model
which allows flatmap, composite strategies and stateful
testing to perform much better. They should also be more
reliable.</p>

<p style="margin-top: 1em">&Acirc;&middot; Filtering may in
some circumstances have improved significantly. This will
help especially in cases where you have lots of values with
individual filters on them, such as <br>
lists(x.filter(...)).</p>

<p style="margin-top: 1em">&Acirc;&middot; Modest
performance improvements to the general test runner by
avoiding expensive operations</p>

<p style="margin-top: 1em">In general your tests should
have got faster. If they&rsquo;ve instead got significantly
slower, I&rsquo;m interested in hearing about it.</p>

<p style="margin-top: 1em">Data distribution:</p>

<p style="margin-top: 1em">The data distribution should
have changed significantly. This may uncover bugs the
previous version missed. It may also miss bugs the previous
version could have uncovered. <br>
Hypothesis is now producing less strongly correlated data
than it used to, but the correlations are extended over more
of the structure.</p>

<p style="margin-top: 1em">Shrinking:</p>

<p style="margin-top: 1em">Shrinking quality should have
improved. In particular Hypothesis can now perform
simultaneous shrinking of separate examples within a single
test (previously it was only able to <br>
do this for elements of a single collection). In some cases
performance will have improved, in some cases it will have
got worse but generally shouldn&rsquo;t have by much.</p>

<p style="margin-top: 1em">2.0.0 - 2016-01-10 <br>
Codename: A new beginning</p>

<p style="margin-top: 1em">This release cleans up all of
the legacy that accrued in the course of Hypothesis 1.0.
These are mostly things that were emitting deprecation
warnings in 1.19.0, but there were a <br>
few additional changes.</p>

<p style="margin-top: 1em">In particular:</p>

<p style="margin-top: 1em">&Acirc;&middot; non-strategy
values will no longer be converted to strategies when used
in given or find.</p>

<p style="margin-top: 1em">&Acirc;&middot;
FailedHealthCheck is now an error and not a warning.</p>

<p style="margin-top: 1em">&Acirc;&middot; Handling of
non-ascii reprs in user types have been simplified by using
raw strings in more places in Python 2.</p>

<p style="margin-top: 1em">&Acirc;&middot; given no longer
allows mixing positional and keyword arguments.</p>

<p style="margin-top: 1em">&Acirc;&middot; given no longer
works with functions with defaults.</p>

<p style="margin-top: 1em">&Acirc;&middot; given no longer
turns provided arguments into defaults - they will not
appear in the argspec at all.</p>

<p style="margin-top: 1em">&Acirc;&middot; the basic()
strategy no longer exists.</p>

<p style="margin-top: 1em">&Acirc;&middot; the n_ary_tree
strategy no longer exists.</p>

<p style="margin-top: 1em">&Acirc;&middot; the
average_list_length setting no longer exists. Note: If
you&rsquo;re using using recursive() this will cause you a
significant slow down. You should pass explicit average_size
<br>
parameters to collections in recursive calls.</p>

<p style="margin-top: 1em">&Acirc;&middot; @rule can no
longer be applied to the same method twice.</p>

<p style="margin-top: 1em">&Acirc;&middot; Python 2.6 and
3.3 are no longer officially supported, although in practice
they still work fine.</p>

<p style="margin-top: 1em">This also includes two
non-deprecation changes:</p>

<p style="margin-top: 1em">&Acirc;&middot; given&rsquo;s
keyword arguments no longer have to be the rightmost
arguments and can appear anywhere in the method
signature.</p>

<p style="margin-top: 1em">&Acirc;&middot; The max_shrinks
setting would sometimes not have been respected.</p>

<p style="margin-top: 1em">1.19.0 - 2016-01-09 <br>
Codename: IT COMES</p>

<p style="margin-top: 1em">This release heralds the
beginning of a new and terrible age of Hypothesis 2.0.</p>

<p style="margin-top: 1em">It&rsquo;s primary purpose is
some final deprecations prior to said release. The goal is
that if your code emits no warnings under this release then
it will probably run unchanged <br>
under Hypothesis 2.0 (there are some caveats to this: 2.0
will drop support for some Python versions, and if
you&rsquo;re using internal APIs then as usual that may
break without warn&acirc; <br>
ing).</p>

<p style="margin-top: 1em">It does have two new
features:</p>

<p style="margin-top: 1em">&Acirc;&middot; New @seed()
decorator which allows you to manually seed a test. This may
be harmlessly combined with and overrides the derandomize
setting.</p>

<p style="margin-top: 1em">&Acirc;&middot; settings objects
may now be used as a decorator to fix those settings to a
particular @given test.</p>

<p style="margin-top: 1em">API changes (old usage still
works but is deprecated):</p>

<p style="margin-top: 1em">&Acirc;&middot; Settings has
been renamed to settings (lower casing) in order to make the
decorator usage more natural.</p>

<p style="margin-top: 1em">&Acirc;&middot; Functions for
the storage directory that were in hypothesis.settings are
now in a new hypothesis.configuration module.</p>

<p style="margin-top: 1em">Additional deprecations:</p>

<p style="margin-top: 1em">&Acirc;&middot; the
average_list_length setting has been deprecated in favour of
being explicit.</p>

<p style="margin-top: 1em">&Acirc;&middot; the basic()
strategy has been deprecated as it is impossible to support
it under a Conjecture based model, which will hopefully be
implemented at some point in the 2.x series.</p>

<p style="margin-top: 1em">&Acirc;&middot; the n_ary_tree
strategy (which was never actually part of the public API)
has been deprecated.</p>

<p style="margin-top: 1em">&Acirc;&middot; Passing settings
or random as keyword arguments to given is deprecated (use
the new functionality instead)</p>

<p style="margin-top: 1em">Bug fixes:</p>

<p style="margin-top: 1em">&Acirc;&middot; No longer emit
PendingDeprecationWarning for __iter__ and StopIteration in
streaming() values.</p>

<p style="margin-top: 1em">&Acirc;&middot; When running in
health check mode with non strict, don&rsquo;t print quite
so many errors for an exception in reify.</p>

<p style="margin-top: 1em">&Acirc;&middot; When an
assumption made in a test or a filter is flaky, tests will
now raise Flaky instead of UnsatisfiedAssumption.</p>

<p style="margin-top: 1em">1.18.1 - 2015-12-22 <br>
Two behind the scenes changes:</p>

<p style="margin-top: 1em">&Acirc;&middot; Hypothesis will
no longer write generated code to the file system. This will
improve performance on some systems (e.g. if you&rsquo;re
using PythonAnywhere which is running your code <br>
from NFS) and prevent some annoying interactions with
auto-restarting systems.</p>

<p style="margin-top: 1em">&Acirc;&middot; Hypothesis will
cache the creation of some strategies. This can
significantly improve performance for code that uses flatmap
or composite and thus has to instantiate strategies <br>
a lot.</p>

<p style="margin-top: 1em">1.18.0 - 2015-12-21 <br>
Features:</p>

<p style="margin-top: 1em">&Acirc;&middot; Tests and find
are now explicitly seeded off the global random module. This
means that if you nest one inside the other you will now get
a health check error. It also means <br>
that you can control global randomization by seeding
random.</p>

<p style="margin-top: 1em">&Acirc;&middot; There is a new
random_module() strategy which seeds the global random
module for you and handles things so that you don&rsquo;t
get a health check warning if you use it inside your <br>
tests.</p>

<p style="margin-top: 1em">&Acirc;&middot; floats() now
accepts two new arguments: allow_nan and allow_infinity.
These default to the old behaviour, but when set to False
will do what the names suggest.</p>

<p style="margin-top: 1em">Bug fixes:</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix a bug where
tests that used text() on Python 3.4+ would not actually be
deterministic even when explicitly seeded or using the
derandomize mode, because generation depended <br>
on dictionary iteration order which was affected by hash
randomization.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix a bug where
with complicated strategies the timing of the initial health
check could affect the seeding of the subsequent test, which
would also render supposedly determin&acirc; <br>
istic tests non-deterministic in some scenarios.</p>

<p style="margin-top: 1em">&Acirc;&middot; In some
circumstances flatmap() could get confused by two
structurally similar things it could generate and would
produce a flaky test where the first time it produced an
error <br>
but the second time it produced the other value, which was
not an error. The same bug was presumably also possible in
composite().</p>

<p style="margin-top: 1em">&Acirc;&middot; flatmap() and
composite() initial generation should now be moderately
faster. This will be particularly noticeable when you have
many values drawn from the same strategy in a <br>
single run, e.g. constructs like lists(s.flatmap(f)).
Shrinking performance may have suffered, but this
didn&rsquo;t actually produce an interestingly worse result
in any of the <br>
standard scenarios tested.</p>

<p style="margin-top: 1em">1.17.1 - 2015-12-16 <br>
A small bug fix release, which fixes the fact that the
&rsquo;note&rsquo; function could not be used on tests which
used the @example decorator to provide explicit
examples.</p>

<p style="margin-top: 1em">1.17.0 - 2015-12-15 <br>
This is actually the same release as 1.16.1, but 1.16.1 has
been pulled because it contains the following additional
change that was not intended to be in a patch release
(it&rsquo;s <br>
perfectly stable, but is a larger change that should have
required a minor version bump):</p>

<p style="margin-top: 1em">&Acirc;&middot; Hypothesis will
now perform a series of &quot;health checks&quot; as part of
running your tests. These detect and warn about some common
error conditions that people often run into which <br>
wouldn&rsquo;t necessarily have caused the test to fail but
would cause e.g. degraded performance or confusing
results.</p>

<p style="margin-top: 1em">1.16.1 - 2015-12-14 <br>
Note: This release has been removed.</p>

<p style="margin-top: 1em">A small bugfix release that
allows bdists for Hypothesis to be built under 2.7 - the
compat3.py file which had Python 3 syntax wasn&rsquo;t
intended to be loaded under Python 2, but <br>
when building a bdist it was. In particular this would break
running setup.py test.</p>

<p style="margin-top: 1em">1.16.0 - 2015-12-08 <br>
There are no public API changes in this release but it
includes a behaviour change that I wasn&rsquo;t comfortable
putting in a patch release.</p>

<p style="margin-top: 1em">&Acirc;&middot; Functions from
hypothesis.strategies will no longer raise InvalidArgument
on bad arguments. Instead the same errors will be raised
when a test using such a strategy is run. <br>
This may improve startup time in some cases, but the main
reason for it is so that errors in strategies won&rsquo;t
cause errors in loading, and it can interact correctly with
things <br>
like pytest.mark.skipif.</p>

<p style="margin-top: 1em">&Acirc;&middot; Errors caused by
accidentally invoking the legacy API are now much less
confusing, although still throw NotImplementedError.</p>

<p style="margin-top: 1em">&Acirc;&middot;
hypothesis.extra.django is 1.9 compatible.</p>

<p style="margin-top: 1em">&Acirc;&middot; When tests are
run with max_shrinks=0 this will now still rerun the test on
failure and will no longer print &quot;Trying example:&quot;
before each run. Additionally note() will now <br>
work correctly when used with max_shrinks=0.</p>

<p style="margin-top: 1em">1.15.0 - 2015-11-24 <br>
A release with two new features.</p>

<p style="margin-top: 1em">&Acirc;&middot; A
&rsquo;characters&rsquo; strategy for more flexible
generation of text with particular character ranges and
types, kindly contributed by Alexander Shorin.</p>

<p style="margin-top: 1em">&Acirc;&middot; Add support for
preconditions to the rule based stateful testing. Kindly
contributed by Christopher Armstrong</p>

<p style="margin-top: 1em">1.14.0 - 2015-11-01 <br>
New features:</p>

<p style="margin-top: 1em">&Acirc;&middot; Add
&rsquo;note&rsquo; function which lets you include
additional information in the final test run&rsquo;s
output.</p>

<p style="margin-top: 1em">&Acirc;&middot; Add
&rsquo;choices&rsquo; strategy which gives you a choice
function that emulates random.choice.</p>

<p style="margin-top: 1em">&Acirc;&middot; Add
&rsquo;uuid&rsquo; strategy that generates UUIDs&rsquo;</p>

<p style="margin-top: 1em">&Acirc;&middot; Add
&rsquo;shared&rsquo; strategy that lets you create a
strategy that just generates a single shared value for each
test run</p>

<p style="margin-top: 1em">Bugs:</p>

<p style="margin-top: 1em">&Acirc;&middot; Using strategies
of the form streaming(x.flatmap(f)) with find or in stateful
testing would have caused InvalidArgument errors when the
resulting values were used (because code <br>
that expected to only be called within a test context would
be invoked).</p>

<p style="margin-top: 1em">1.13.0 - 2015-10-29 <br>
This is quite a small release, but deprecates some public
API functions and removes some internal API functionality so
gets a minor version bump.</p>

<p style="margin-top: 1em">&Acirc;&middot; All calls to the
&rsquo;strategy&rsquo; function are now deprecated, even
ones which pass just a SearchStrategy instance (which is
still a no-op).</p>

<p style="margin-top: 1em">&Acirc;&middot; Never documented
hypothesis.extra entry_points mechanism has now been removed
( it was previously how hypothesis.extra packages were
loaded and has been deprecated and unused <br>
for some time)</p>

<p style="margin-top: 1em">&Acirc;&middot; Some corner
cases that could previously have produced an OverflowError
when simplifying failing cases using
hypothesis.extra.datetimes (or dates or times) have now been
fixed.</p>

<p style="margin-top: 1em">&Acirc;&middot; Hypothesis load
time for first import has been significantly reduced - it
used to be around 250ms (on my SSD laptop) and now is around
100-150ms. This almost never matters but <br>
was slightly annoying when using it in the console.</p>

<p style="margin-top: 1em">&Acirc;&middot;
hypothesis.strategies.randoms was previously missing from
__all__.</p>

<p style="margin-top: 1em">1.12.0 - 2015-10-18 <br>
&Acirc;&middot; Significantly improved performance of
creating strategies using the functions from the
hypothesis.strategies module by deferring the calculation of
their repr until it was <br>
needed. This is unlikely to have been an performance issue
for you unless you were using flatmap, composite or stateful
testing, but for some cases it could be quite a
signifi&acirc; <br>
cant impact.</p>

<p style="margin-top: 1em">&Acirc;&middot; A number of
cases where the repr of strategies build from lambdas is
improved</p>

<p style="margin-top: 1em">&Acirc;&middot; Add dates() and
times() strategies to hypothesis.extra.datetimes</p>

<p style="margin-top: 1em">&Acirc;&middot; Add new
&rsquo;profiles&rsquo; mechanism to the settings system</p>

<p style="margin-top: 1em">&Acirc;&middot; Deprecates
mutability of Settings, both the Settings.default top level
property and individual settings.</p>

<p style="margin-top: 1em">&Acirc;&middot; A Settings
object may now be directly initialized from a parent
Settings.</p>

<p style="margin-top: 1em">&Acirc;&middot; @given should
now give a better error message if you attempt to use it
with a function that uses destructuring arguments (it still
won&rsquo;t work, but it will error more clearly),</p>

<p style="margin-top: 1em">&Acirc;&middot; A number of
spelling corrections in error messages</p>

<p style="margin-top: 1em">&Acirc;&middot; py.test should
no longer display the intermediate modules Hypothesis
generates when running in verbose mode</p>

<p style="margin-top: 1em">&Acirc;&middot; Hypothesis
should now correctly handle printing objects with non-ascii
reprs on python 3 when running in a locale that cannot
handle ascii printing to stdout.</p>

<p style="margin-top: 1em">&Acirc;&middot; Add a
unique=True argument to lists(). This is equivalent to
unique_by=lambda x: x, but offers a more convenient
syntax.</p>

<p style="margin-top: 1em">1.11.4 - 2015-09-27 <br>
&Acirc;&middot; Hide modifications Hypothesis needs to make
to sys.path by undoing them after we&rsquo;ve imported the
relevant modules. This is a workaround for issues
cryptography experienced on <br>
windows.</p>

<p style="margin-top: 1em">&Acirc;&middot; Slightly
improved performance of drawing from sampled_from on large
lists of alternatives.</p>

<p style="margin-top: 1em">&Acirc;&middot; Significantly
improved performance of drawing from one_of or strategies
using | (note this includes a lot of strategies internally -
floats() and integers() both fall into this <br>
category). There turned out to be a massive performance
regression introduced in 1.10.0 affecting these which
probably would have made tests using Hypothesis
significantly <br>
slower than they should have been.</p>

<p style="margin-top: 1em">1.11.3 - 2015-09-23 <br>
&Acirc;&middot; Better argument validation for datetimes()
strategy - previously setting max_year &lt;
datetime.MIN_YEAR or min_year &gt; datetime.MAX_YEAR would
not have raised an InvalidArgument <br>
error and instead would have behaved confusingly.</p>

<p style="margin-top: 1em">&Acirc;&middot; Compatibility
with being run on pytest &lt; 2.7 (achieved by disabling the
plugin).</p>

<p style="margin-top: 1em">1.11.2 - 2015-09-23 <br>
Bug fixes:</p>

<p style="margin-top: 1em">&Acirc;&middot;
Settings(database=my_db) would not be correctly inherited
when used as a default setting, so that newly created
settings would use the database_file setting and create an
<br>
SQLite example database.</p>

<p style="margin-top: 1em">&Acirc;&middot;
Settings.default.database = my_db would previously have
raised an error and now works.</p>

<p style="margin-top: 1em">&Acirc;&middot; Timeout could
sometimes be significantly exceeded if during simplification
there were a lot of examples tried that didn&rsquo;t trigger
the bug.</p>

<p style="margin-top: 1em">&Acirc;&middot; When loading a
heavily simplified example using a basic() strategy from the
database this could cause Python to trigger a recursion
error.</p>

<p style="margin-top: 1em">&Acirc;&middot; Remove use of
deprecated API in pytest plugin so as to not emit
warning</p>

<p style="margin-top: 1em">Misc:</p>

<p style="margin-top: 1em">&Acirc;&middot;
hypothesis-pytest is now part of hypothesis core. This
should have no externally visible consequences, but you
should update your dependencies to remove hypothesis-pytest
and <br>
depend on only Hypothesis.</p>

<p style="margin-top: 1em">&Acirc;&middot; Better repr for
hypothesis.extra.datetimes() strategies.</p>

<p style="margin-top: 1em">&Acirc;&middot; Add .close()
method to abstract base class for Backend (it was already
present in the main implementation).</p>

<p style="margin-top: 1em">1.11.1 - 2015-09-16 <br>
Bug fixes:</p>

<p style="margin-top: 1em">&Acirc;&middot; When running
Hypothesis tests in parallel (e.g. using pytest-xdist) there
was a race condition caused by code generation.</p>

<p style="margin-top: 1em">&Acirc;&middot; Example
databases are now cached per thread so as to not use sqlite
connections from multiple threads. This should make
Hypothesis now entirely thread safe.</p>

<p style="margin-top: 1em">&Acirc;&middot; floats() with
only min_value or max_value set would have had a very bad
distribution.</p>

<p style="margin-top: 1em">&Acirc;&middot; Running on 3.5,
Hypothesis would have emitted deprecation warnings because
of use of inspect.getargspec</p>

<p style="margin-top: 1em">1.11.0 - 2015-08-31 <br>
&Acirc;&middot; text() with a non-string alphabet would have
used the repr() of the the alphabet instead of its contexts.
This is obviously silly. It now works with any sequence of
things con&acirc; <br>
vertible to unicode strings.</p>

<p style="margin-top: 1em">&Acirc;&middot; @given will now
work on methods whose definitions contains no explicit
positional arguments, only varargs (bug #118). This may have
some knock on effects because it means that <br>
@given no longer changes the argspec of functions other than
by adding defaults.</p>

<p style="margin-top: 1em">&Acirc;&middot; Introduction of
new @composite feature for more natural definition of
strategies you&rsquo;d previously have used flatmap for.</p>

<p style="margin-top: 1em">1.10.6 - 2015-08-26 <br>
Fix support for fixtures on Django 1.7.</p>

<p style="margin-top: 1em">1.10.4 - 2015-08-21 <br>
Tiny bug fix release:</p>

<p style="margin-top: 1em">&Acirc;&middot; If the
database_file setting is set to None, this would have
resulted in an error when running tests. Now it does the
same as setting database to None.</p>

<p style="margin-top: 1em">1.10.3 - 2015-08-19 <br>
Another small bug fix release.</p>

<p style="margin-top: 1em">&Acirc;&middot; lists(elements,
unique_by=some_function, min_size=n) would have raised a
ValidationError if n &gt;
Settings.default.average_list_length because it would have
wanted to use an <br>
average list length shorter than the minimum size of the
list, which is impossible. Now it instead defaults to twice
the minimum size in these circumstances.</p>

<p style="margin-top: 1em">&Acirc;&middot; basic() strategy
would have only ever produced at most ten distinct values
per run of the test (which is bad if you e.g. have it inside
a list). This was obviously silly. It <br>
will now produce a much better distribution of data, both
duplicated and non duplicated.</p>

<p style="margin-top: 1em">1.10.2 - 2015-08-19 <br>
This is a small bug fix release:</p>

<p style="margin-top: 1em">&Acirc;&middot; star imports
from hypothesis should now work correctly.</p>

<p style="margin-top: 1em">&Acirc;&middot; example quality
for examples using flatmap will be better, as the way it had
previously been implemented was causing problems where
Hypothesis was erroneously labelling some <br>
examples as being duplicates.</p>

<p style="margin-top: 1em">1.10.0 - 2015-08-04 <br>
This is just a bugfix and performance release, but it
changes some semi-public APIs, hence the minor version
bump.</p>

<p style="margin-top: 1em">&Acirc;&middot; Significant
performance improvements for strategies which are one_of()
many branches. In particular this included recursive()
strategies. This should take the case where you <br>
use one recursive() strategy as the base strategy of another
from unusably slow (tens of seconds per generated example)
to reasonably fast.</p>

<p style="margin-top: 1em">&Acirc;&middot; Better handling
of just() and sampled_from() for values which have an
incorrect __repr__ implementation that returns non-ASCII
unicode on Python 2.</p>

<p style="margin-top: 1em">&Acirc;&middot; Better
performance for flatmap from changing the internal morpher
API to be significantly less general purpose.</p>

<p style="margin-top: 1em">&Acirc;&middot; Introduce a new
semi-public BuildContext/cleanup API. This allows strategies
to register cleanup activities that should run once the
example is complete. Note that this will <br>
interact somewhat weirdly with find.</p>

<p style="margin-top: 1em">&Acirc;&middot; Better
simplification behaviour for streaming strategies.</p>

<p style="margin-top: 1em">&Acirc;&middot; Don&rsquo;t
error on lambdas which use destructuring arguments in Python
2.</p>

<p style="margin-top: 1em">&Acirc;&middot; Add some better
reprs for a few strategies that were missing good ones.</p>

<p style="margin-top: 1em">&Acirc;&middot; The Random
instances provided by randoms() are now copyable.</p>

<p style="margin-top: 1em">&Acirc;&middot; Slightly more
debugging information about simplify when using a debug
verbosity level.</p>

<p style="margin-top: 1em">&Acirc;&middot; Support using
given for functions with varargs, but not passing arguments
to it as positional.</p>

<p style="margin-top: 1em">1.9.0 - 2015-07-27 <br>
Codename: The great bundling.</p>

<p style="margin-top: 1em">This release contains two fairly
major changes.</p>

<p style="margin-top: 1em">The first is the deprecation of
the hypothesis-extra mechanism. From now on all the packages
that were previously bundled under it other than
hypothesis-pytest (which is a dif&acirc; <br>
ferent beast and will remain separate). The functionality
remains unchanged and you can still import them from exactly
the same location, they just are no longer separate
pack&acirc; <br>
ages.</p>

<p style="margin-top: 1em">The second is that this
introduces a new way of building strategies which lets you
build up strategies recursively from other strategies.</p>

<p style="margin-top: 1em">It also contains the minor
change that calling .example() on a strategy object will
give you examples that are more representative of the actual
data you&rsquo;ll get. There used to be <br>
some logic in there to make the examples artificially simple
but this proved to be a bad idea.</p>

<p style="margin-top: 1em">1.8.5 - 2015-07-24 <br>
This contains no functionality changes but fixes a mistake
made with building the previous package that would have
broken installation on Windows.</p>

<p style="margin-top: 1em">1.8.4 - 2015-07-20 <br>
Bugs fixed:</p>

<p style="margin-top: 1em">&Acirc;&middot; When a call to
floats() had endpoints which were not floats but merely
convertible to one (e.g. integers), these would be included
in the generated data which would cause it to <br>
generate non-floats.</p>

<p style="margin-top: 1em">&Acirc;&middot; Splitting
lambdas used in the definition of flatmap, map or filter
over multiple lines would break the repr, which would in
turn break their usage.</p>

<p style="margin-top: 1em">1.8.3 - 2015-07-20 <br>
&quot;Falsifying example&quot; would not have been printed
when the failure came from an explicit example.</p>

<p style="margin-top: 1em">1.8.2 - 2015-07-18 <br>
Another small bugfix release:</p>

<p style="margin-top: 1em">&Acirc;&middot; When using
ForkingTestCase you would usually not get the falsifying
example printed if the process exited abnormally (e.g. due
to os._exit).</p>

<p style="margin-top: 1em">&Acirc;&middot; Improvements to
the distribution of characters when using text() with a
default alphabet. In particular produces a better
distribution of ascii and whitespace in the alphabet.</p>

<p style="margin-top: 1em">1.8.1 - 2015-07-17 <br>
This is a small release that contains a workaround for
people who have bad reprs returning non ascii text on Python
2.7. This is not a bug fix for Hypothesis per se because
<br>
that&rsquo;s not a thing that is actually supposed to work,
but Hypothesis leans more heavily on repr than is typical so
it&rsquo;s worth having a workaround for.</p>

<p style="margin-top: 1em">1.8.0 - 2015-07-16 <br>
New features:</p>

<p style="margin-top: 1em">&Acirc;&middot; Much more
sensible reprs for strategies, especially ones that come
from hypothesis.strategies. These should now have as reprs
python code that would produce the same strategy.</p>

<p style="margin-top: 1em">&Acirc;&middot; lists() accepts
a unique_by argument which forces the generated lists to be
only contain elements unique according to some function key
(which must return a hashable value).</p>

<p style="margin-top: 1em">&Acirc;&middot; Better error
messages from flaky tests to help you debug things.</p>

<p style="margin-top: 1em">Mostly invisible implementation
details that may result in finding new bugs in your
code:</p>

<p style="margin-top: 1em">&Acirc;&middot; Sets and
dictionary generation should now produce a better range of
results.</p>

<p style="margin-top: 1em">&Acirc;&middot; floats with
bounds now focus more on &rsquo;critical values&rsquo;,
trying to produce values at edge cases.</p>

<p style="margin-top: 1em">&Acirc;&middot; flatmap should
now have better simplification for complicated cases, as
well as generally being (I hope) more reliable.</p>

<p style="margin-top: 1em">Bug fixes:</p>

<p style="margin-top: 1em">&Acirc;&middot; You could not
previously use assume() if you were using the forking
executor.</p>

<p style="margin-top: 1em">1.7.2 - 2015-07-10 <br>
This is purely a bug fix release:</p>

<p style="margin-top: 1em">&Acirc;&middot; When using
floats() with stale data in the database you could sometimes
get values in your tests that did not respect min_value or
max_value.</p>

<p style="margin-top: 1em">&Acirc;&middot; When getting a
Flaky error from an unreliable test it would have
incorrectly displayed the example that caused it.</p>

<p style="margin-top: 1em">&Acirc;&middot; 2.6 dependency
on backports was incorrectly specified. This would only have
caused you problems if you were building a universal wheel
from Hypothesis, which is not how Hypoth&acirc; <br>
esis ships, so unless you&rsquo;re explicitly building
wheels for your dependencies and support Python 2.6 plus a
later version of Python this probably would never have
affected you.</p>

<p style="margin-top: 1em">&Acirc;&middot; If you use
flatmap in a way that the strategy on the right hand side
depends sensitively on the left hand side you may have
occasionally seen Flaky errors caused by producing <br>
unreliable examples when minimizing a bug. This use case may
still be somewhat fraught to be honest. This code is due a
major rearchitecture for 1.8, but in the meantime this <br>
release fixes the only source of this error that I&rsquo;m
aware of.</p>

<p style="margin-top: 1em">1.7.1 - 2015-06-29 <br>
Codename: There is no 1.7.0.</p>

<p style="margin-top: 1em">A slight technical hitch with a
premature upload means there&rsquo;s was a yanked 1.7.0
release. Oops.</p>

<p style="margin-top: 1em">The major feature of this
release is Python 2.6 support. Thanks to Jeff Meadows for
doing most of the work there.</p>

<p style="margin-top: 1em">Other minor features</p>

<p style="margin-top: 1em">&Acirc;&middot; strategies now
has a permutations() function which returns a strategy
yielding permutations of values from a given collection.</p>

<p style="margin-top: 1em">&Acirc;&middot; if you have a
flaky test it will print the exception that it last saw
before failing with Flaky, even if you do not have verbose
reporting on.</p>

<p style="margin-top: 1em">&Acirc;&middot; Slightly
experimental git merge script available as &quot;python -m
hypothesis.tools.mergedbs&quot;. Instructions on how to use
it in the docstring of that file.</p>

<p style="margin-top: 1em">Bug fixes:</p>

<p style="margin-top: 1em">&Acirc;&middot; Better
performance from use of filter. In particular tests which
involve large numbers of heavily filtered strategies should
perform a lot better.</p>

<p style="margin-top: 1em">&Acirc;&middot; floats() with a
negative min_value would not have worked correctly
(worryingly, it would have just silently failed to run any
examples). This is now fixed.</p>

<p style="margin-top: 1em">&Acirc;&middot; tests using
sampled_from would error if the number of sampled elements
was smaller than min_satisfying_examples.</p>

<p style="margin-top: 1em">1.6.2 - 2015-06-08 <br>
This is just a few small bug fixes:</p>

<p style="margin-top: 1em">&Acirc;&middot; Size bounds were
not validated for values for a binary() strategy when
reading examples from the database.</p>

<p style="margin-top: 1em">&Acirc;&middot; sampled_from is
now in __all__ in hypothesis.strategies</p>

<p style="margin-top: 1em">&Acirc;&middot; floats no longer
consider negative integers to be simpler than positive
non-integers</p>

<p style="margin-top: 1em">&Acirc;&middot; Small floating
point intervals now correctly count members, so if you have
a floating point interval so narrow there are only a handful
of values in it, this will no longer <br>
cause an error when Hypothesis runs out of values.</p>

<p style="margin-top: 1em">1.6.1 - 2015-05-21 <br>
This is a small patch release that fixes a bug where 1.6.0
broke the use of flatmap with the deprecated API and assumed
the passed in function returned a SearchStrategy instance
<br>
rather than converting it to a strategy.</p>

<p style="margin-top: 1em">1.6.0 - 2015-05-21 <br>
This is a smallish release designed to fix a number of bugs
and smooth out some weird behaviours.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix a critical
bug in flatmap where it would reuse old strategies. If all
your flatmap code was pure you&rsquo;re fine. If it&rsquo;s
not, I&rsquo;m surprised it&rsquo;s working at all. In
particular <br>
if you want to use flatmap with django models, you
desperately need to upgrade to this version.</p>

<p style="margin-top: 1em">&Acirc;&middot; flatmap
simplification performance should now be better in some
cases where it previously had to redo work.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix for a bug
where invalid unicode data with surrogates could be
generated during simplification (it was already filtered out
during actual generation).</p>

<p style="margin-top: 1em">&Acirc;&middot; The Hypothesis
database is now keyed off the name of the test instead of
the type of data. This makes much more sense now with the
new strategies API and is generally more <br>
robust. This means you will lose old examples on
upgrade.</p>

<p style="margin-top: 1em">&Acirc;&middot; The database
will now not delete values which fail to deserialize
correctly, just skip them. This is to handle cases where
multiple incompatible strategies share the same key.</p>

<p style="margin-top: 1em">&Acirc;&middot; find now also
saves and loads values from the database, keyed off a hash
of the function you&rsquo;re finding from.</p>

<p style="margin-top: 1em">&Acirc;&middot; Stateful tests
now serialize and load values from the database. They should
have before, really. This was a bug.</p>

<p style="margin-top: 1em">&Acirc;&middot; Passing a
different verbosity level into a test would not have worked
entirely correctly, leaving off some messages. This is now
fixed.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix a bug where
derandomized tests with unicode characters in the function
body would error on Python 2.7.</p>

<p style="margin-top: 1em">1.5.0 - 2015-05-14 <br>
Codename: Strategic withdrawal.</p>

<p style="margin-top: 1em">The purpose of this release is a
radical simplification of the API for building strategies.
Instead of the old approach of @strategy.extend and things
that get converted to <br>
strategies, you just build strategies directly.</p>

<p style="margin-top: 1em">The old method of defining
strategies will still work until Hypothesis 2.0, because
it&rsquo;s a major breaking change, but will now emit
deprecation warnings.</p>

<p style="margin-top: 1em">The new API is also a lot more
powerful as the functions for defining strategies give you a
lot of dials to turn. See the updated data section for
details.</p>

<p style="margin-top: 1em">Other changes:</p>

<p style="margin-top: 1em">&Acirc;&middot; Mixing keyword
and positional arguments in a call to @given is deprecated
as well.</p>

<p style="margin-top: 1em">&Acirc;&middot; There is a new
setting called &rsquo;strict&rsquo;. When set to True,
Hypothesis will raise warnings instead of merely printing
them. Turning it on by default is inadvisable because it
<br>
means that Hypothesis minor releases can break your code,
but it may be useful for making sure you catch all uses of
deprecated APIs.</p>

<p style="margin-top: 1em">&Acirc;&middot; max_examples in
settings is now interpreted as meaning the maximum number of
unique (ish) examples satisfying assumptions. A new setting
max_iterations which defaults to a <br>
larger value has the old interpretation.</p>

<p style="margin-top: 1em">&Acirc;&middot; Example
generation should be significantly faster due to a new
faster parameter selection algorithm. This will mostly show
up for simple data types - for complex ones the <br>
parameter selection is almost certainly dominated.</p>

<p style="margin-top: 1em">&Acirc;&middot; Simplification
has some new heuristics that will tend to cut down on cases
where it could previously take a very long time.</p>

<p style="margin-top: 1em">&Acirc;&middot; timeout would
previously not have been respected in cases where there were
a lot of duplicate examples. You probably wouldn&rsquo;t
have previously noticed this because max_exam&acirc; <br>
ples counted duplicates, so this was very hard to hit in a
way that mattered.</p>

<p style="margin-top: 1em">&Acirc;&middot; A number of
internal simplifications to the SearchStrategy API.</p>

<p style="margin-top: 1em">&Acirc;&middot; You can now
access the current Hypothesis version as
hypothesis.__version__.</p>

<p style="margin-top: 1em">&Acirc;&middot; A top level
function is provided for running the stateful tests without
the TestCase infrastructure.</p>

<p style="margin-top: 1em">1.4.0 - 2015-05-04 <br>
Codename: What a state.</p>

<p style="margin-top: 1em">The big feature of this release
is the new and slightly experimental stateful testing API.
You can read more about that in the appropriate section.</p>

<p style="margin-top: 1em">Two minor features the were
driven out in the course of developing this:</p>

<p style="margin-top: 1em">&Acirc;&middot; You can now set
settings.max_shrinks to limit the number of times Hypothesis
will try to shrink arguments to your test. If this is set to
&lt;= 0 then Hypothesis will not rerun <br>
your test and will just raise the failure directly. Note
that due to technical limitations if max_shrinks is &lt;= 0
then Hypothesis will print every example it calls your test
<br>
with rather than just the failing one. Note also that I
don&rsquo;t consider settings max_shrinks to zero a sensible
way to run your tests and it should really be considered a
debug <br>
feature.</p>

<p style="margin-top: 1em">&Acirc;&middot; There is a new
debug level of verbosity which is even more verbose than
verbose. You probably don&rsquo;t want this.</p>

<p style="margin-top: 1em">Breakage of semi-public
SearchStrategy API:</p>

<p style="margin-top: 1em">&Acirc;&middot; It is now a
required invariant of SearchStrategy that if u simplifies to
v then it is not the case that strictly_simpler(u, v). i.e.
simplifying should not increase the com&acirc; <br>
plexity even though it is not required to decrease it.
Enforcing this invariant lead to finding some bugs where
simplifying of integers, floats and sets was suboptimal.</p>

<p style="margin-top: 1em">&Acirc;&middot; Integers in
basic data are now required to fit into 64 bits. As a result
python integer types are now serialized as strings, and some
types have stopped using quite so need&acirc; <br>
lessly large random seeds.</p>

<p style="margin-top: 1em">Hypothesis Stateful testing was
then turned upon Hypothesis itself, which lead to an amazing
number of minor bugs being found in Hypothesis itself.</p>

<p style="margin-top: 1em">Bugs fixed (most but not all
from the result of stateful testing) include:</p>

<p style="margin-top: 1em">&Acirc;&middot; Serialization of
streaming examples was flaky in a way that you would
probably never notice: If you generate a template, simplify
it, serialize it, deserialize it, serialize it <br>
again and then deserialize it you would get the original
stream instead of the simplified one.</p>

<p style="margin-top: 1em">&Acirc;&middot; If you reduced
max_examples below the number of examples already saved in
the database, you would have got a ValueError. Additionally,
if you had more than max_examples in the <br>
database all of them would have been considered.</p>

<p style="margin-top: 1em">&Acirc;&middot; @given will no
longer count duplicate examples (which it never called your
function with) towards max_examples. This may result in your
tests running slower, but that&rsquo;s proba&acirc; <br>
bly just because they&rsquo;re trying more examples.</p>

<p style="margin-top: 1em">&Acirc;&middot; General
improvements to example search which should result in better
performance and higher quality examples. In particular
parameters which have a history of producing useless <br>
results will be more aggressively culled. This is useful
both because it decreases the chance of useless examples and
also because it&rsquo;s much faster to not check parameters
<br>
which we were unlikely to ever pick!</p>

<p style="margin-top: 1em">&Acirc;&middot; integers_from
and lists of types with only one value (e.g. [None]) would
previously have had a very high duplication rate so you were
probably only getting a handful of exam&acirc; <br>
ples. They now have a much lower duplication rate, as well
as the improvements to search making this less of a problem
in the first place.</p>

<p style="margin-top: 1em">&Acirc;&middot; You would
sometimes see simplification taking significantly longer
than your defined timeout. This would happen because timeout
was only being checked after each successful <br>
simplification, so if Hypothesis was spending a lot of time
unsuccessfully simplifying things it wouldn&rsquo;t stop in
time. The timeout is now applied for unsuccessful
simplifica&acirc; <br>
tions too.</p>

<p style="margin-top: 1em">&Acirc;&middot; In Python 2.7,
integers_from strategies would have failed during
simplification with an OverflowError if their starting point
was at or near to the maximum size of a 64-bit <br>
integer.</p>

<p style="margin-top: 1em">&Acirc;&middot; flatmap and map
would have failed if called with a function without a
__name__ attribute.</p>

<p style="margin-top: 1em">&Acirc;&middot; If max_examples
was less than min_satisfying_examples this would always
error. Now min_satisfying_examples is capped to
max_examples. Note that if you have assumptions to
sat&acirc; <br>
isfy here this will still cause an error.</p>

<p style="margin-top: 1em">Some minor quality
improvements:</p>

<p style="margin-top: 1em">&Acirc;&middot; Lists of
streams, flatmapped strategies and basic strategies should
now now have slightly better simplification.</p>

<p style="margin-top: 1em">1.3.0 - 2015-05-22 <br>
New features:</p>

<p style="margin-top: 1em">&Acirc;&middot; New verbosity
level API for printing intermediate results and
exceptions.</p>

<p style="margin-top: 1em">&Acirc;&middot; New specifier
for strings generated from a specified alphabet.</p>

<p style="margin-top: 1em">&Acirc;&middot; Better error
messages for tests that are failing because of a lack of
enough examples.</p>

<p style="margin-top: 1em">Bug fixes:</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix error where
use of ForkingTestCase would sometimes result in too many
open files.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix error where
saving a failing example that used flatmap could error.</p>

<p style="margin-top: 1em">&Acirc;&middot; Implement
simplification for sampled_from, which apparently never
supported it previously. Oops.</p>

<p style="margin-top: 1em">General improvements:</p>

<p style="margin-top: 1em">&Acirc;&middot; Better range of
examples when using one_of or sampled_from.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix some
pathological performance issues when simplifying lists of
complex values.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix some
pathological performance issues when simplifying examples
that require unicode strings with high codepoints.</p>

<p style="margin-top: 1em">&Acirc;&middot; Random will now
simplify to more readable examples.</p>

<p style="margin-top: 1em">1.2.1 - 2015-04-16 <br>
A small patch release for a bug in the new executors
feature. Tests which require doing something to their result
in order to fail would have instead reported as flaky.</p>

<p style="margin-top: 1em">1.2.0 - 2015-04-15 <br>
Codename: Finders keepers.</p>

<p style="margin-top: 1em">A bunch of new features and
improvements.</p>

<p style="margin-top: 1em">&Acirc;&middot; Provide a
mechanism for customizing how your tests are executed.</p>

<p style="margin-top: 1em">&Acirc;&middot; Provide a test
runner that forks before running each example. This allows
better support for testing native code which might trigger a
segfault or a C level assertion failure.</p>

<p style="margin-top: 1em">&Acirc;&middot; Support for
using Hypothesis to find examples directly rather than as
just as a test runner.</p>

<p style="margin-top: 1em">&Acirc;&middot; New streaming
type which lets you generate infinite lazily loaded streams
of data - perfect for if you need a number of examples but
don&rsquo;t know how many.</p>

<p style="margin-top: 1em">&Acirc;&middot; Better support
for large integer ranges. You can now use integers_in_range
with ranges of basically any size. Previously large ranges
would have eaten up all your memory and <br>
taken forever.</p>

<p style="margin-top: 1em">&Acirc;&middot; Integers produce
a wider range of data than before - previously they would
only rarely produce integers which didn&rsquo;t fit into a
machine word. Now it&rsquo;s much more common. This <br>
percolates to other numeric types which build on
integers.</p>

<p style="margin-top: 1em">&Acirc;&middot; Better
validation of arguments to @given. Some situations that
would previously have caused silently wrong behaviour will
now raise an error.</p>

<p style="margin-top: 1em">&Acirc;&middot; Include +/-
sys.float_info.max in the set of floating point edge cases
that Hypothesis specifically tries.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix some bugs in
floating point ranges which happen when given +/-
sys.float_info.max as one of the endpoints... (really any
two floats that are sufficiently far apart so that <br>
x, y are finite but y - x is infinite). This would have
resulted in generating infinite values instead of ones
inside the range.</p>

<p style="margin-top: 1em">1.1.1 - 2015-04-07 <br>
Codename: Nothing to see here</p>

<p style="margin-top: 1em">This is just a patch release put
out because it fixed some internal bugs that would block the
Django integration release but did not actually affect
anything anyone could previ&acirc; <br>
ously have been using. It also contained a minor quality fix
for floats that I&rsquo;d happened to have finished in
time.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix some
internal bugs with object lifecycle management that were
impossible to hit with the previously released versions but
broke hypothesis-django.</p>

<p style="margin-top: 1em">&Acirc;&middot; Bias floating
point numbers somewhat less aggressively towards very small
numbers</p>

<p style="margin-top: 1em">1.1.0 - 2015-04-06 <br>
Codename: No-one mention the M word.</p>

<p style="margin-top: 1em">&Acirc;&middot; Unicode strings
are more strongly biased towards ascii characters.
Previously they would generate all over the space. This is
mostly so that people who try to shape their uni&acirc; <br>
code strings with assume() have less of a bad time.</p>

<p style="margin-top: 1em">&Acirc;&middot; A number of
fixes to data deserialization code that could theoretically
have caused mysterious bugs when using an old version of a
Hypothesis example database with a newer ver&acirc; <br>
sion. To the best of my knowledge a change that could have
triggered this bug has never actually been seen in the wild.
Certainly no-one ever reported a bug of this nature.</p>

<p style="margin-top: 1em">&Acirc;&middot; Out of the box
support for Decimal and Fraction.</p>

<p style="margin-top: 1em">&Acirc;&middot; new dictionary
specifier for dictionaries with variable keys.</p>

<p style="margin-top: 1em">&Acirc;&middot; Significantly
faster and higher quality simplification, especially for
collections of data.</p>

<p style="margin-top: 1em">&Acirc;&middot; New filter() and
flatmap() methods on Strategy for better ways of building
strategies out of other strategies.</p>

<p style="margin-top: 1em">&Acirc;&middot; New
BasicStrategy class which allows you to define your own
strategies from scratch without needing an existing matching
strategy or being exposed to the full horror or <br>
non-public nature of the SearchStrategy interface.</p>

<p style="margin-top: 1em">1.0.0 - 2015-03-27 <br>
Codename: Blast-off!</p>

<p style="margin-top: 1em">There are no code changes in
this release. This is precisely the 0.9.2 release with some
updated documentation.</p>

<p style="margin-top: 1em">0.9.2 - 2015-03-26 <br>
Codename: T-1 days.</p>

<p style="margin-top: 1em">&Acirc;&middot; floats_in_range
would not actually have produced floats_in_range unless that
range happened to be (0, 1). Fix this.</p>

<p style="margin-top: 1em">0.9.1 - 2015-03-25 <br>
Codename: T-2 days.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix a bug where
if you defined a strategy using map on a lambda then the
results would not be saved in the database.</p>

<p style="margin-top: 1em">&Acirc;&middot; Significant
performance improvements when simplifying examples using
lists, strings or bounded integer ranges.</p>

<p style="margin-top: 1em">0.9.0 - 2015-03-23 <br>
Codename: The final countdown</p>

<p style="margin-top: 1em">This release could also be
called 1.0-RC1.</p>

<p style="margin-top: 1em">It contains a teeny tiny bugfix,
but the real point of this release is to declare feature
freeze. There will be zero functionality changes between
0.9.0 and 1.0 unless something <br>
goes really really wrong. No new features will be added, no
breaking API changes will occur, etc. This is the final
shakedown before I declare Hypothesis stable and ready to
use <br>
and throw a party to celebrate.</p>

<p style="margin-top: 1em">Bug bounty for any bugs found
between now and 1.0: I will buy you a drink (alcoholic,
caffeinated, or otherwise) and shake your hand should we
ever find ourselves in the same <br>
city at the same time.</p>

<p style="margin-top: 1em">The one tiny bugfix:</p>

<p style="margin-top: 1em">&Acirc;&middot; Under pypy,
databases would fail to close correctly when garbage
collected, leading to a memory leak and a confusing error
message if you were repeatedly creating databases and <br>
not closing them. It is very unlikely you were doing this
and the chances of you ever having noticed this bug are very
low.</p>

<p style="margin-top: 1em">0.7.2 - 2015-03-22 <br>
Codename: Hygienic macros or bust</p>

<p style="margin-top: 1em">&Acirc;&middot; You can now name
an argument to @given &rsquo;f&rsquo; and it won&rsquo;t
break (issue #38)</p>

<p style="margin-top: 1em">&Acirc;&middot;
strategy_test_suite is now named strategy_test_suite as the
documentation claims and not in fact
strategy_test_suitee</p>

<p style="margin-top: 1em">&Acirc;&middot; Settings objects
can now be used as a context manager to temporarily override
the default values inside their context.</p>

<p style="margin-top: 1em">0.7.1 - 2015-03-21 <br>
Codename: Point releases go faster</p>

<p style="margin-top: 1em">&Acirc;&middot; Better string
generation by parametrizing by a limited alphabet</p>

<p style="margin-top: 1em">&Acirc;&middot; Faster string
simplification - previously if simplifying a string with
high range unicode characters it would try every unicode
character smaller than that. This was pretty <br>
pointless. Now it stops after it&rsquo;s a short range (it
can still reach smaller ones through recursive calls because
of other simplifying operations).</p>

<p style="margin-top: 1em">&Acirc;&middot; Faster list
simplification by first trying a binary chop down the
middle</p>

<p style="margin-top: 1em">&Acirc;&middot; Simultaneous
simplification of identical elements in a list. So if a bug
only triggers when you have duplicates but you drew e.g.
[-17, -17], this will now simplify to [0, 0].</p>

<p style="margin-top: 1em">0.7.0, - 2015-03-20 <br>
Codename: Starting to look suspiciously real</p>

<p style="margin-top: 1em">This is probably the last minor
release prior to 1.0. It consists of stability improvements,
a few usability things designed to make Hypothesis easier to
try out, and filing off <br>
some final rough edges from the API.</p>

<p style="margin-top: 1em">&Acirc;&middot; Significant
speed and memory usage improvements</p>

<p style="margin-top: 1em">&Acirc;&middot; Add an example()
method to strategy objects to give an example of the sort of
data that the strategy generates.</p>

<p style="margin-top: 1em">&Acirc;&middot; Remove
.descriptor attribute of strategies</p>

<p style="margin-top: 1em">&Acirc;&middot; Rename
descriptor_test_suite to strategy_test_suite</p>

<p style="margin-top: 1em">&Acirc;&middot; Rename the few
remaining uses of descriptor to specifier (descriptor
already has a defined meaning in Python)</p>

<p style="margin-top: 1em">0.6.0 - 2015-03-13 <br>
Codename: I&rsquo;m sorry, were you using that API?</p>

<p style="margin-top: 1em">This is primarily a
&quot;simplify all the weird bits of the API&quot; release.
As a result there are a lot of breaking changes. If you just
use @given with core types then you&rsquo;re probably <br>
fine.</p>

<p style="margin-top: 1em">In particular:</p>

<p style="margin-top: 1em">&Acirc;&middot; Stateful testing
has been removed from the API</p>

<p style="margin-top: 1em">&Acirc;&middot; The way the
database is used has been rendered less useful (sorry). The
feature for reassembling values saved from other tests
doesn&rsquo;t currently work. This will probably be <br>
brought back in post 1.0.</p>

<p style="margin-top: 1em">&Acirc;&middot;
SpecificationMapper is no longer a thing. Instead there is
an ExtMethod called strategy which you extend to specify how
to convert other types to strategies.</p>

<p style="margin-top: 1em">&Acirc;&middot; Settings are now
extensible so you can add your own for configuring a
strategy</p>

<p style="margin-top: 1em">&Acirc;&middot;
MappedSearchStrategy no longer needs an unpack method</p>

<p style="margin-top: 1em">&Acirc;&middot; Basically all
the SearchStrategy internals have changed massively. If you
implemented SearchStrategy directly rather than using
MappedSearchStrategy talk to me about fixing it.</p>

<p style="margin-top: 1em">&Acirc;&middot; Change to the
way extra packages work. You now specify the package. This
must have a load() method. Additionally any modules in the
package will be loaded in under hypothe&acirc; <br>
sis.extra</p>

<p style="margin-top: 1em">Bug fixes:</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix for a bug
where calling falsify on a lambda with a non-ascii character
in its body would error.</p>

<p style="margin-top: 1em">Hypothesis Extra:</p>

<p style="margin-top: 1em">hypothesis-fakefactory: An
extension for using faker data in hypothesis. Depends <br>
on fake-factory.</p>

<p style="margin-top: 1em">0.5.0 - 2015-02-10 <br>
Codename: Read all about it.</p>

<p style="margin-top: 1em">Core hypothesis:</p>

<p style="margin-top: 1em">&Acirc;&middot; Add support back
in for pypy and python 3.2</p>

<p style="margin-top: 1em">&Acirc;&middot; @given functions
can now be invoked with some arguments explicitly provided.
If all arguments that hypothesis would have provided are
passed in then no falsification is run.</p>

<p style="margin-top: 1em">&Acirc;&middot; Related to the
above, this means that you can now use pytest fixtures and
mark.parametrize with Hypothesis without either interfering
with the other.</p>

<p style="margin-top: 1em">&Acirc;&middot; Breaking change:
@given no longer works for functions with varargs (varkwargs
are fine). This might be added back in at a later date.</p>

<p style="margin-top: 1em">&Acirc;&middot; Windows is now
fully supported. A limited version (just the tests with none
of the extras) of the test suite is run on windows with each
commit so it is now a first class citi&acirc; <br>
zen of the Hypothesis world.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix a bug for
fuzzy equality of equal complex numbers with different reprs
(this can happen when one coordinate is zero). This
shouldn&rsquo;t affect users - that feature isn&rsquo;t used
<br>
anywhere public facing.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix generation
of floats on windows and 32-bit builds of python. I was
using some struct.pack logic that only worked on certain
word sizes.</p>

<p style="margin-top: 1em">&Acirc;&middot; When a test
times out and hasn&rsquo;t produced enough examples this now
raises a Timeout subclass of Unfalsifiable.</p>

<p style="margin-top: 1em">&Acirc;&middot; Small search
spaces are better supported. Previously something like a
@given(bool, bool) would have failed because it
couldn&rsquo;t find enough examples. Hypothesis is now aware
of <br>
the fact that these are small search spaces and will not
error in this case.</p>

<p style="margin-top: 1em">&Acirc;&middot; Improvements to
parameter search in the case of hard to satisfy assume.
Hypothesis will now spend less time exploring parameters
that are unlikely to provide anything useful.</p>

<p style="margin-top: 1em">&Acirc;&middot; Increase chance
of generating &quot;nasty&quot; floats</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix a bug that
would have caused unicode warnings if you had a sampled_from
that was mixing unicode and byte strings.</p>

<p style="margin-top: 1em">&Acirc;&middot; Added a standard
test suite that you can use to validate a custom strategy
you&rsquo;ve defined is working correctly.</p>

<p style="margin-top: 1em">Hypothesis extra:</p>

<p style="margin-top: 1em">First off, introducing
Hypothesis extra packages!</p>

<p style="margin-top: 1em">These are packages that are
separated out from core Hypothesis because they have one or
more dependencies. Every hypothesis-extra package is pinned
to a specific point release of <br>
Hypothesis and will have some version requirements on its
dependency. They use entry_points so you will usually not
need to explicitly import them, just have them installed on
<br>
the path.</p>

<p style="margin-top: 1em">This release introduces two of
them:</p>

<p style="margin-top: 1em">hypothesis-datetime:</p>

<p style="margin-top: 1em">Does what it says on the tin:
Generates datetimes for Hypothesis. Just install the package
and datetime support will start working.</p>

<p style="margin-top: 1em">Depends on pytz for timezone
support</p>

<p style="margin-top: 1em">hypothesis-pytest:</p>

<p style="margin-top: 1em">A very rudimentary pytest
plugin. All it does right now is hook the display of
falsifying examples into pytest reporting.</p>

<p style="margin-top: 1em">Depends on pytest.</p>

<p style="margin-top: 1em">0.4.3 - 2015-02-05 <br>
Codename: TIL narrow Python builds are a thing</p>

<p style="margin-top: 1em">This just fixes the one bug.</p>

<p style="margin-top: 1em">&Acirc;&middot; Apparently there
is such a thing as a &quot;narrow python build&quot; and OS
X ships with these by default for python 2.7. These are
builds where you only have two bytes worth of uni&acirc;
<br>
code. As a result, generating unicode was completely broken
on OS X. Fix this by only generating unicode codepoints in
the range supported by the system.</p>

<p style="margin-top: 1em">0.4.2 - 2015-02-04 <br>
Codename: O(dear)</p>

<p style="margin-top: 1em">This is purely a bugfix
release:</p>

<p style="margin-top: 1em">&Acirc;&middot; Provide sensible
external hashing for all core types. This will significantly
improve performance of tracking seen examples which happens
in literally every falsification run. <br>
For Hypothesis fixing this cut 40% off the runtime of the
test suite. The behaviour is quadratic in the number of
examples so if you&rsquo;re running the default
configuration this <br>
will be less extreme (Hypothesis&rsquo;s test suite runs at
a higher number of examples than default), but you should
still see a significant improvement.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix a bug in
formatting of complex numbers where the string could get
incorrectly truncated.</p>

<p style="margin-top: 1em">0.4.1 - 2015-02-03 <br>
Codename: Cruel and unusual edge cases</p>

<p style="margin-top: 1em">This release is mostly about
better test case generation.</p>

<p style="margin-top: 1em">Enhancements:</p>

<p style="margin-top: 1em">&Acirc;&middot; Has a cool
release name</p>

<p style="margin-top: 1em">&Acirc;&middot; text_type (str
in python 3, unicode in python 2) example generation now
actually produces interesting unicode instead of boring
ascii strings.</p>

<p style="margin-top: 1em">&Acirc;&middot; floating point
numbers are generated over a much wider range, with
particular attention paid to generating nasty numbers - nan,
infinity, large and small values, etc.</p>

<p style="margin-top: 1em">&Acirc;&middot; examples can be
generated using pieces of examples previously saved in the
database. This allows interesting behaviour that has
previously been discovered to be propagated to <br>
other examples.</p>

<p style="margin-top: 1em">&Acirc;&middot; improved
parameter exploration algorithm which should allow it to
more reliably hit interesting edge cases.</p>

<p style="margin-top: 1em">&Acirc;&middot; Timeout can now
be disabled entirely by setting it to any value &lt;= 0.</p>

<p style="margin-top: 1em">Bug fixes:</p>

<p style="margin-top: 1em">&Acirc;&middot; The descriptor
on a OneOfStrategy could be wrong if you had descriptors
which were equal but should not be coalesced. e.g. a
strategy for one_of((frozenset({int}), {int})) <br>
would have reported its descriptor as {int}. This is
unlikely to have caused you any problems</p>

<p style="margin-top: 1em">&Acirc;&middot; If you had
strategies that could produce NaN (which float previously
couldn&rsquo;t but e.g. a Just(float(&rsquo;nan&rsquo;))
could) then this would have sent hypothesis into an infinite
loop <br>
that would have only been terminated when it hit the
timeout.</p>

<p style="margin-top: 1em">&Acirc;&middot; Given elements
that can take a long time to minimize, minimization of
floats or tuples could be quadratic or worse in the that
value. You should now see much better performance <br>
for simplification, albeit at some cost in quality.</p>

<p style="margin-top: 1em">Other:</p>

<p style="margin-top: 1em">&Acirc;&middot; A lot of
internals have been been rewritten. This shouldn&rsquo;t
affect you at all, but it opens the way for certain of
hypothesis&rsquo;s oddities to be a lot more extensible by
users. <br>
Whether this is a good thing may be up for debate...</p>

<p style="margin-top: 1em">0.4.0 - 2015-01-21 <br>
FLAGSHIP FEATURE: Hypothesis now persists examples for later
use. It stores data in a local SQLite database and will
reuse it for all tests of the same type.</p>

<p style="margin-top: 1em">LICENSING CHANGE: Hypothesis is
now released under the Mozilla Public License 2.0. This
applies to all versions from 0.4.0 onwards until further
notice. The previous license <br>
remains applicable to all code prior to 0.4.0.</p>

<p style="margin-top: 1em">Enhancements:</p>

<p style="margin-top: 1em">&Acirc;&middot; Printing of
failing examples. I was finding that the pytest runner was
not doing a good job of displaying these, and that
Hypothesis itself could do much better.</p>

<p style="margin-top: 1em">&Acirc;&middot; Drop dependency
on six for cross-version compatibility. It was easy enough
to write the shim for the small set of features that we care
about and this lets us avoid a moder&acirc; <br>
ately complex dependency.</p>

<p style="margin-top: 1em">&Acirc;&middot; Some
improvements to statistical distribution of selecting from
small (&lt;= 3 elements)</p>

<p style="margin-top: 1em">&Acirc;&middot; Improvements to
parameter selection for finding examples.</p>

<p style="margin-top: 1em">Bugs fixed:</p>

<p style="margin-top: 1em">&Acirc;&middot;
could_have_produced for lists, dicts and other collections
would not have examined the elements and thus when using a
union of different types of list this could result in <br>
Hypothesis getting confused and passing a value to the wrong
strategy. This could potentially result in exceptions being
thrown from within simplification.</p>

<p style="margin-top: 1em">&Acirc;&middot; sampled_from
would not work correctly on a single element list.</p>

<p style="margin-top: 1em">&Acirc;&middot; Hypothesis could
get very confused by values which are equal despite having
different types being used in descriptors. Hypothesis now
has its own more specific version of <br>
equality it uses for descriptors and tracking. It is always
more fine grained than Python equality: Things considered !=
are not considered equal by hypothesis, but some things <br>
that are considered == are distinguished. If your test suite
uses both frozenset and set tests this bug is probably
affecting you.</p>

<p style="margin-top: 1em">0.3.2 - 2015-01-16 <br>
&Acirc;&middot; Fix a bug where if you specified
floats_in_range with integer arguments Hypothesis would
error in example simplification.</p>

<p style="margin-top: 1em">&Acirc;&middot; Improve the
statistical distribution of the floats you get for the
floats_in_range strategy. I&rsquo;m not sure whether this
will affect users in practice but it took my tests for <br>
various conditions from flaky to rock solid so it at the
very least improves discovery of the artificial cases
I&rsquo;m looking for.</p>

<p style="margin-top: 1em">&Acirc;&middot; Improved repr()
for strategies and RandomWithSeed instances.</p>

<p style="margin-top: 1em">&Acirc;&middot; Add detection
for flaky test cases where hypothesis managed to find an
example which breaks it but on the final invocation of the
test it does not raise an error. This will <br>
typically happen with too much recursion errors but could
conceivably happen in other circumstances too.</p>

<p style="margin-top: 1em">&Acirc;&middot; Provide a
&quot;derandomized&quot; mode. This allows you to run
hypothesis with zero real randomization, making your build
nice and deterministic. The tests run with a seed calculated
<br>
from the function they&rsquo;re testing so you should still
get a good distribution of test cases.</p>

<p style="margin-top: 1em">&Acirc;&middot; Add a mechanism
for more conveniently defining tests which just sample from
some collection.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix for a really
subtle bug deep in the internals of the strategy table. In
some circumstances if you were to define instance strategies
for both a parent class and one or <br>
more of its subclasses you would under some circumstances
get the strategy for the wrong superclass of an instance. It
is very unlikely anyone has ever encountered this in the
<br>
wild, but it is conceivably possible given that a mix of
namedtuple and tuple are used fairly extensively inside
hypothesis which do exhibit this pattern of strategy.</p>

<p style="margin-top: 1em">0.3.1 - 2015-01-13 <br>
&Acirc;&middot; Support for generation of frozenset and
Random values</p>

<p style="margin-top: 1em">&Acirc;&middot; Correct handling
of the case where a called function mutates it argument.
This involved introducing a notion of a strategies knowing
how to copy their argument. The default <br>
method should be entirely acceptable and the worst case is
that it will continue to have the old behaviour if you
don&rsquo;t mark your strategy as mutable, so this
shouldn&rsquo;t break <br>
anything.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix for a bug
where some strategies did not correctly implement
could_have_produced. It is very unlikely that any of these
would have been seen in the wild, and the conse&acirc; <br>
quences if they had been would have been minor.</p>

<p style="margin-top: 1em">&Acirc;&middot; Re-export the
@given decorator from the main hypothesis namespace.
It&rsquo;s still available at the old location too.</p>

<p style="margin-top: 1em">&Acirc;&middot; Minor
performance optimisation for simplifying long lists.</p>

<p style="margin-top: 1em">0.3.0 - 2015-01-12 <br>
&Acirc;&middot; Complete redesign of the data generation
system. Extreme breaking change for anyone who was
previously writing their own SearchStrategy implementations.
These will not work any <br>
more and you&rsquo;ll need to modify them.</p>

<p style="margin-top: 1em">&Acirc;&middot; New settings
system allowing more global and modular control of Verifier
behaviour.</p>

<p style="margin-top: 1em">&Acirc;&middot; Decouple
SearchStrategy from the StrategyTable. This leads to much
more composable code which is a lot easier to
understand.</p>

<p style="margin-top: 1em">&Acirc;&middot; A significant
amount of internal API renaming and moving. This may also
break your code.</p>

<p style="margin-top: 1em">&Acirc;&middot; Expanded
available descriptors, allowing for generating integers or
floats in a specific range.</p>

<p style="margin-top: 1em">&Acirc;&middot; Significantly
more robust. A very large number of small bug fixes, none of
which anyone is likely to have ever noticed.</p>

<p style="margin-top: 1em">&Acirc;&middot; Deprecation of
support for pypy and python 3 prior to 3.3. 3.3 and 3.4.
Supported versions are 2.7.x, 3.3.x, 3.4.x. I expect all of
these to remain officially supported for a <br>
very long time. I would not be surprised to add pypy support
back in later but I&rsquo;m not going to do so until I know
someone cares about it. In the meantime it will probably
<br>
still work.</p>

<p style="margin-top: 1em">0.2.2 - 2015-01-08 <br>
&Acirc;&middot; Fix an embarrassing complete failure of the
installer caused by my being bad at version control</p>

<p style="margin-top: 1em">0.2.1 - 2015-01-07 <br>
&Acirc;&middot; Fix a bug in the new stateful testing
feature where you could make __init__ a @requires method.
Simplification would not always work if the prune method was
able to success&acirc; <br>
fully shrink the test.</p>

<p style="margin-top: 1em">0.2.0 - 2015-01-07 <br>
&Acirc;&middot; It&rsquo;s aliiive.</p>

<p style="margin-top: 1em">&Acirc;&middot; Improve python 3
support using six.</p>

<p style="margin-top: 1em">&Acirc;&middot; Distinguish
between byte and unicode types.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix issues where
FloatStrategy could raise.</p>

<p style="margin-top: 1em">&Acirc;&middot; Allow stateful
testing to request constructor args.</p>

<p style="margin-top: 1em">&Acirc;&middot; Fix for issue
where test annotations would timeout based on when the
module was loaded instead of when the test started</p>

<p style="margin-top: 1em">0.1.4 - 2013-12-14 <br>
&Acirc;&middot; Make verification runs time bounded with a
configurable timeout</p>

<p style="margin-top: 1em">0.1.3 - 2013-05-03 <br>
&Acirc;&middot; Bugfix: Stateful testing behaved incorrectly
with subclassing.</p>

<p style="margin-top: 1em">&Acirc;&middot; Complex number
support</p>

<p style="margin-top: 1em">&Acirc;&middot; support for
recursive strategies</p>

<p style="margin-top: 1em">&Acirc;&middot; different error
for hypotheses with unsatisfiable assumptions</p>

<p style="margin-top: 1em">0.1.2 - 2013-03-24 <br>
&Acirc;&middot; Bugfix: Stateful testing was not minimizing
correctly and could throw exceptions.</p>

<p style="margin-top: 1em">&Acirc;&middot; Better support
for recursive strategies.</p>

<p style="margin-top: 1em">&Acirc;&middot; Support for
named tuples.</p>

<p style="margin-top: 1em">&Acirc;&middot; Much faster
integer generation.</p>

<p style="margin-top: 1em">0.1.1 - 2013-03-24 <br>
&Acirc;&middot; Python 3.x support via 2to3.</p>

<p style="margin-top: 1em">&Acirc;&middot; Use new style
classes (oops).</p>

<p style="margin-top: 1em">0.1.0 - 2013-03-23 <br>
&Acirc;&middot; Introduce stateful testing.</p>

<p style="margin-top: 1em">&Acirc;&middot; Massive rewrite
of internals to add flags and strategies.</p>

<p style="margin-top: 1em">0.0.5 - 2013-03-13 <br>
&Acirc;&middot; No changes except trying to fix
packaging</p>

<p style="margin-top: 1em">0.0.4 - 2013-03-13 <br>
&Acirc;&middot; No changes except that I checked in a
failing test case for 0.0.3 so had to replace the release.
Doh</p>

<p style="margin-top: 1em">0.0.3 - 2013-03-13 <br>
&Acirc;&middot; Improved a few internals.</p>

<p style="margin-top: 1em">&Acirc;&middot; Opened up
creating generators from instances as a general API.</p>

<p style="margin-top: 1em">&Acirc;&middot; Test
integration.</p>

<p style="margin-top: 1em">0.0.2 - 2013-03-12 <br>
&Acirc;&middot; Starting to tighten up on the internals.</p>

<p style="margin-top: 1em">&Acirc;&middot; Change API to
allow more flexibility in configuration.</p>

<p style="margin-top: 1em">&Acirc;&middot; More
testing.</p>

<p style="margin-top: 1em">0.0.1 - 2013-03-10 <br>
&Acirc;&middot; Initial release.</p>

<p style="margin-top: 1em">&Acirc;&middot; Basic working
prototype. Demonstrates idea, probably shouldn&rsquo;t be
used.</p>

<p style="margin-top: 1em">ONGOING HYPOTHESIS DEVELOPMENT
<br>
Hypothesis development is managed by me, David R. MacIver. I
am the primary author of Hypothesis.</p>

<p style="margin-top: 1em">However, I no longer do unpaid
feature development on Hypothesis. My roles as leader of the
project are:</p>

<p style="margin-top: 1em">1. Helping other people do
feature development on Hypothesis</p>

<p style="margin-top: 1em">2. Fixing bugs and other code
health issues</p>

<p style="margin-top: 1em">3. Improving documentation</p>

<p style="margin-top: 1em">4. General release management
work</p>

<p style="margin-top: 1em">5. Planning the general roadmap
of the project</p>

<p style="margin-top: 1em">6. Doing sponsored development
on tasks that are too large or in depth for other people to
take on</p>

<p style="margin-top: 1em">So all new features must either
be sponsored or implemented by someone else. That being
said, the maintenance team takes an active role in
shepherding pull requests and helping <br>
people write a new feature (see CONTRIBUTING.rst for details
and pull request #154 for an example of how the process
goes). This isn&rsquo;t &quot;patches welcome&quot;,
it&rsquo;s &quot;we will help you <br>
write a patch&quot;.</p>

<p style="margin-top: 1em">Release Policy <br>
Hypothesis releases follow semantic versioning.</p>

<p style="margin-top: 1em">We maintain
backwards-compatibility wherever possible, and use
deprecation warnings to mark features that have been
superseded by a newer alternative. If you want to detect
<br>
this, the strict setting upgrades all Hypothesis warnings to
errors.</p>

<p style="margin-top: 1em">We use continuous deployment to
ensure that you can always use our newest and shiniest
features - every change to the source tree is automatically
built and published on PyPI as <br>
soon as it&rsquo;s merged onto master, after code review and
passing our extensive test suite.</p>

<p style="margin-top: 1em">Project Roadmap <br>
Hypothesis does not have a long-term release plan. However
some visibility into our plans for future compatibility may
be useful:</p>

<p style="margin-top: 1em">&Acirc;&middot; We value
compatibility, and maintain it as far as practical. This
generally excludes things which are end-of-life upstream, or
have an unstable API.</p>

<p style="margin-top: 1em">&Acirc;&middot; We would like to
drop Python 2 support when it it reaches end of life in
2020. Ongoing support is likely to depend on commercial
funding.</p>

<p style="margin-top: 1em">&Acirc;&middot; We intend to
support PyPy3 as soon as it supports a recent enough version
of Python 3. See issue #602.</p>

<p style="margin-top: 1em">HELP AND SUPPORT <br>
For questions you are happy to ask in public, the Hypothesis
community is a friendly place where I or others will be more
than happy to help you out. You&rsquo;re also welcome to ask
<br>
questions on Stack Overflow. If you do, please tag them with
&rsquo;python-hypothesis&rsquo; so someone sees them.</p>

<p style="margin-top: 1em">For bugs and enhancements,
please file an issue on the GitHub issue tracker. Note that
as per the development policy, enhancements will probably
not get implemented unless <br>
you&rsquo;re willing to pay for development or implement
them yourself (with assistance from me). Bugs will tend to
get fixed reasonably promptly, though it is of course on a
best <br>
effort basis.</p>

<p style="margin-top: 1em">To see the versions of Python,
optional dependencies, test runners, and operating systems
Hypothesis supports (meaning incompatibility is treated as a
bug), see supported.</p>

<p style="margin-top: 1em">If you need to ask questions
privately or want more of a guarantee of bugs being fixed
promptly, please contact me on
hypothesis-support@drmaciver.com to talk about availability
<br>
of support contracts.</p>

<p style="margin-top: 1em">PACKAGING GUIDELINES <br>
Downstream packagers often want to package Hypothesis. Here
are some guidelines.</p>

<p style="margin-top: 1em">The primary guideline is this:
If you are not prepared to keep up with the Hypothesis
release schedule, don&rsquo;t. You will annoy me and are
doing your users a disservice.</p>

<p style="margin-top: 1em">Hypothesis has quite a frequent
release schedule. It&rsquo;s very rare that it goes a month
without a release, and there are often multiple releases in
a given month.</p>

<p style="margin-top: 1em">Many people not only fail to
follow the release schedule but also seem included to
package versions which are months out of date even at the
point of packaging. This will cause <br>
me to be very annoyed with you and you will consequently get
very little co-operation from me.</p>

<p style="margin-top: 1em">If you are prepared to keep up
with the Hypothesis release schedule, the rest of this
document outlines some information you might find
useful.</p>

<p style="margin-top: 1em">Release tarballs <br>
These are available from the GitHub releases page. The
tarballs on pypi are intended for installation from a Python
tool such as pip and should not be considered complete <br>
releases. Requests to include additional files in them will
not be granted. Their absence is not a bug.</p>

<p style="margin-top: 1em">Dependencies <br>
Python versions <br>
Hypothesis is designed to work with a range of Python
versions. Currently supported are:</p>

<p style="margin-top: 1em">&Acirc;&middot; pypy-2.6.1
(earlier versions of pypy may work)</p>

<p style="margin-top: 1em">&Acirc;&middot; CPython
2.7.x</p>

<p style="margin-top: 1em">&Acirc;&middot; CPython
3.4.x</p>

<p style="margin-top: 1em">&Acirc;&middot; CPython
3.5.x</p>

<p style="margin-top: 1em">&Acirc;&middot; CPython
3.6.x</p>

<p style="margin-top: 1em">If you feel the need to have
separate Python 3 and Python 2 packages you can, but
Hypothesis works unmodified on either.</p>

<p style="margin-top: 1em">Other Python libraries <br>
Hypothesis has optional dependencies on the following
libraries:</p>

<p style="margin-top: 1em">&Acirc;&middot; pytz (almost any
version should work)</p>

<p style="margin-top: 1em">&Acirc;&middot; faker, version
0.7</p>

<p style="margin-top: 1em">&Acirc;&middot; Django, all
supported versions</p>

<p style="margin-top: 1em">&Acirc;&middot; numpy, 1.10 or
later (earlier versions will probably work fine)</p>

<p style="margin-top: 1em">&Acirc;&middot; py.test (2.7.0
or greater). This is a mandatory dependency for testing
Hypothesis itself but optional for users.</p>

<p style="margin-top: 1em">The way this works when
installing Hypothesis normally is that these features become
available if the relevant library is installed.</p>

<p style="margin-top: 1em">Testing Hypothesis <br>
If you want to test Hypothesis as part of your packaging you
will probably not want to use the mechanisms Hypothesis
itself uses for running its tests, because it has a lot of
<br>
logic for installing and testing against different versions
of Python.</p>

<p style="margin-top: 1em">The tests must be run with
py.test. A version more recent than 2.7.0 is strongly
encouraged, but it may work with earlier versions (however
py.test specific logic is disabled <br>
before 2.7.0).</p>

<p style="margin-top: 1em">Tests are organised into a
number of top level subdirectories of the tests/
directory.</p>

<p style="margin-top: 1em">&Acirc;&middot; cover: This is a
small, reasonably fast, collection of tests designed to give
100% coverage of all but a select subset of the files when
run under Python 3.</p>

<p style="margin-top: 1em">&Acirc;&middot; nocover: This is
a much slower collection of tests that should not be run
under coverage for performance reasons.</p>

<p style="margin-top: 1em">&Acirc;&middot; py2: Tests that
can only be run under Python 2</p>

<p style="margin-top: 1em">&Acirc;&middot; py3: Tests that
can only be run under Python 3</p>

<p style="margin-top: 1em">&Acirc;&middot; datetime: This
tests the subset of Hypothesis that depends on pytz</p>

<p style="margin-top: 1em">&Acirc;&middot; fakefactory:
This tests the subset of Hypothesis that depends on
fakefactory.</p>

<p style="margin-top: 1em">&Acirc;&middot; django: This
tests the subset of Hypothesis that depends on django (this
also depends on fakefactory).</p>

<p style="margin-top: 1em">An example invocation for
running the coverage subset of these tests:</p>

<p style="margin-top: 1em">pip install -e . <br>
pip install pytest # you will probably want to use your own
packaging here <br>
python -m pytest tests/cover</p>

<p style="margin-top: 1em">Examples <br>
&Acirc;&middot; arch linux</p>

<p style="margin-top: 1em">&Acirc;&middot; fedora</p>

<p style="margin-top: 1em">&Acirc;&middot; gentoo (slightly
behind at the time of this writing)</p>

<p style="margin-top: 1em">AUTHOR <br>
David R. MacIver</p>

<p style="margin-top: 1em">COPYRIGHT <br>
2013-2017, David R. MacIver</p>

<p style="margin-top: 1em">3.12.0 Jul 11, 2017
HYPOTHESIS(1)</p>
<hr>
</body>
</html>
