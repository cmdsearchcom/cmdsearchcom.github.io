<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 15:53:33 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>AVOCADO(1) AVOCADO(1)</p>

<p style="margin-top: 1em">NAME <br>
avocado - test runner command line tool</p>

<p style="margin-top: 1em">SYNOPSIS <br>
avocado [-h] [-v] [--config [CONFIG_FILE]] [--show
[STREAM[:LVL]]] [-s] <br>

{config,diff,distro,exec-path,list,multiplex,plugins,run,sysinfo}
...</p>

<p style="margin-top: 1em">DESCRIPTION <br>
Avocado is a modern test framework that is built on the
experience accumulated with autotest
(http://autotest.github.io).</p>

<p style="margin-top: 1em">avocado is also the name of its
test runner command line tool, described in this man
page.</p>

<p style="margin-top: 1em">For more information about the
Avocado project, please check its website:
http://avocado-framework.github.io/</p>

<p style="margin-top: 1em">OPTIONS <br>
The following list of options are builtin, application level
avocado options. Most other options are implemented via
plugins and will depend on them being loaded (avocado <br>
--help):</p>

<p style="margin-top: 1em">-h, --help show this help
message and exit <br>
-v, --version show program&rsquo;s version number and exit
<br>
--config [CONFIG_FILE] <br>
Use custom configuration from a file <br>
--show [STREAM[:LVL]] <br>
List of comma separated builtin logs, or logging <br>
streams optionally followed by LEVEL (DEBUG,INFO,...). <br>
Builtin streams are: &quot;test&quot;: test output;
&quot;debug&quot;: <br>
tracebacks and other debugging info; &quot;app&quot;: <br>
application output; &quot;early&quot;: early logging of
other <br>
streams, including test (very verbose); &quot;remote&quot;:
<br>
fabric/paramiko debug; &quot;all&quot;: all builtin streams;
<br>
&quot;none&quot;: disables regular output (leaving only
errors <br>
enabled). By default: &rsquo;app&rsquo; <br>
-s, --silent disables regular output (leaving only errors
enabled)</p>

<p style="margin-top: 1em">Real use of avocado depends on
running avocado subcommands. This a typical list of avocado
subcommands:</p>

<p style="margin-top: 1em">config Shows avocado config keys
<br>
diff Shows the difference between 2 jobs. <br>
distro Shows detected Linux distribution <br>
exec-path Returns path to avocado bash libraries and exits.
<br>
list List available tests <br>
multiplex Tool to analyze and visualize test variants and
params <br>
plugins Displays plugin information <br>
run Runs one or more tests (native test, test alias, <br>
binary or script) <br>
sysinfo Collect system information</p>

<p style="margin-top: 1em">To get usage instructions for a
given subcommand, run it with --help. Example:</p>

<p style="margin-top: 1em">$ avocado run --help</p>

<p style="margin-top: 1em">Options for subcommand run
(avocado run --help):</p>

<p style="margin-top: 1em">positional arguments: <br>
TEST_REFERENCE List of test references (aliases or
paths)</p>

<p style="margin-top: 1em">optional arguments: <br>
-h, --help show this help message and exit <br>
-d, --dry-run Instead of running the test only list them and
log <br>
their params. <br>
--force-job-id UNIQUE_JOB_ID <br>
Forces the use of a particular job ID. Used internally <br>
when interacting with an avocado server. You should <br>
not use this option unless you know exactly what <br>
you&rsquo;re doing <br>
--job-results-dir DIRECTORY <br>
Forces to use of an alternate job results directory. <br>
--job-timeout SECONDS <br>
Set the maximum amount of time (in SECONDS) that tests <br>
are allowed to execute. Values &lt;= zero means &quot;no
<br>
timeout&quot;. You can also use suffixes, like: s <br>
(seconds), m (minutes), h (hours). <br>
--failfast {on,off} Enable or disable the job interruption
on first failed <br>
test. <br>
--keep-tmp {on,off} Keep job temporary files (useful for
avocado <br>
debugging). Defaults to off. <br>
--sysinfo {on,off} Enable or disable system information
(hardware <br>
details, profilers, etc.). Current: on <br>
--execution-order {tests-per-variant,variants-per-test} <br>
How to iterate through test suite and variants</p>

<p style="margin-top: 1em">output and result format: <br>
-s, --silent Silence stdout <br>
--show-job-log Display only the job log on stdout. Useful
for test <br>
debugging purposes. No output will be displayed if you <br>
also specify --silent <br>
--store-logging-stream [STREAM[:LEVEL] [STREAM[:LEVEL] ...]]
<br>
Store given logging STREAMs in <br>
$JOB_RESULTS_DIR/$STREAM.$LEVEL. <br>
--html FILE Enable HTML output to the FILE where the result
should <br>
be written. The value - (output to stdout) is not <br>
supported since not all HTML resources can be embedded <br>
into a single file (page resources will be copied to <br>
the output file dir) <br>
--open-browser Open the generated report on your preferred
browser. <br>
This works even if --html was not explicitly passed, <br>
since an HTML report is always generated on the job <br>
results dir. Current: False <br>
--html-job-result {on,off} <br>
Enables default HTML result in the job results <br>
directory. File will be located at <br>
&quot;html/results.html&quot;. <br>
--journal Records test status changes (for use with avocado-
<br>
journal-replay and avocado-server) <br>
--json FILE Enable JSON result format and write it to FILE.
Use <br>
&rsquo;-&rsquo; to redirect to the standard output. <br>
--json-job-result {on,off} <br>
Enables default JSON result in the job results <br>
directory. File will be named &quot;results.json&quot;. <br>
--tap FILE Enable TAP result output and write it to FILE.
Use &rsquo;-&rsquo; <br>
to redirect to the standard output. <br>
--tap-job-result {on,off} <br>
Enables default TAP result in the job results <br>
directory. File will be named &quot;results.tap&quot;. <br>
--xunit FILE Enable xUnit result format and write it to
FILE. Use <br>
&rsquo;-&rsquo; to redirect to the standard output. <br>
--xunit-job-result {on,off} <br>
Enables default xUnit result in the job results <br>
directory. File will be named &quot;results.xml&quot;. <br>
-z, --archive Archive (ZIP) files generated by tests</p>

<p style="margin-top: 1em">output check arguments: <br>
--output-check-record {none,all,stdout,stderr} <br>
Record output streams of your tests to reference files <br>
(valid options: none (do not record output streams), <br>
all (record both stdout and stderr), stdout (record <br>
only stderr), stderr (record only stderr). Current: <br>
none <br>
--output-check {on,off} <br>
Enable or disable test output (stdout/stderr) check. <br>
If this option is off, no output will be checked, even <br>
if there are reference files present for the test. <br>
Current: on (output check enabled)</p>

<p style="margin-top: 1em">loader options: <br>
--loaders [LOADERS [LOADERS ...]] <br>
Overrides the priority of the test loaders. You can <br>
specify either @loader_name or TEST_TYPE. By default <br>
it tries all available loaders according to priority <br>
set in settings-&gt;plugins.loaders. <br>
--external-runner EXECUTABLE <br>
Path to an specific test runner that allows the use of <br>
its own tests. This should be used for running tests <br>
that do not conform to Avocado&rsquo; SIMPLE testinterface
<br>
and can not run standalone. Note: the use of <br>
--external-runner overwrites the --loaders to <br>
&quot;external_runner&quot; <br>
--external-runner-chdir {runner,test} <br>
Change directory before executing tests. This option <br>
may be necessary because of requirements and/or <br>
limitations of the external test runner. If the <br>
external runner requires to be run from its own base <br>
directory,use &quot;runner&quot; here. If the external
runner <br>
runs tests based on files and requires to be run from <br>
the directory where those files are located, use <br>
&quot;test&quot; here and specify the test directory with
the <br>
option &quot;--external-runner-testdir&quot;. Defaults to
&quot;None&quot; <br>
--external-runner-testdir DIRECTORY <br>
Where test files understood by the external test <br>
runner are located in the filesystem. Obviously this <br>
assumes and only applies to external test runners that <br>
run tests from files</p>

<p style="margin-top: 1em">filtering parameters: <br>
--filter-by-tags TAGS <br>
Filter INSTRUMENTED tests based on &quot;:avocado: <br>
tags=tag1,tag2&quot; notation in their class docstring <br>
--filter-by-tags-include-empty <br>
Include all tests without tags during filtering. This <br>
effectively means they will be kept in the test suite <br>
found previously to filtering.</p>

<p style="margin-top: 1em">test execution inside docker
container: <br>
--docker IMAGE Name of the docker image torun tests on. <br>
--docker-cmd CMD Override the docker command, eg.
&rsquo;sudo docker&rsquo; or <br>
other base docker options like hypervisor. Default: <br>
&rsquo;docker&rsquo; <br>
--docker-options OPT Extra options for docker run cmd. (see:
man docker- <br>
run) <br>
--docker-no-cleanup Preserve container after test</p>

<p style="margin-top: 1em">keep environment variables: <br>
--env-keep ENV_KEEP Keep environment variables in remote
executions</p>

<p style="margin-top: 1em">GNU Debugger support: <br>
--gdb-run-bin EXECUTABLE[:BREAKPOINT] <br>
Run a given executable inside the GNU debugger, <br>
pausing at a given breakpoint (defaults to &quot;main&quot;)
<br>
--gdb-prerun-commands EXECUTABLE:COMMANDS <br>
After loading an executable in GDB, but before <br>
actually running it, execute the GDB commands in the <br>
given file. EXECUTABLE is optional, if omitted <br>
COMMANDS will apply to all executables <br>
--gdb-coredump {on,off} <br>
Automatically generate a core dump when the inferior <br>
process received a fatal signal such as SIGSEGV or <br>
SIGABRT</p>

<p style="margin-top: 1em">test execution on a remote
machine: <br>
--remote-hostname REMOTE_HOSTNAME <br>
Specify the hostname to login on remote machine <br>
--remote-port REMOTE_PORT <br>
Specify the port number to login on remote machine. <br>
Current: 22 <br>
--remote-username REMOTE_USERNAME <br>
Specify the username to login on remote machine. <br>
Current: apahim <br>
--remote-password REMOTE_PASSWORD <br>
Specify the password to login on remote machine <br>
--remote-key-file REMOTE_KEY_FILE <br>
Specify an identity file with a private key instead of <br>
a password (Example: .pem files from Amazon EC2) <br>
--remote-timeout SECONDS <br>
Amount of time (in seconds) to wait for a successful <br>
connection to the remote machine. Defaults to 60 <br>
seconds.</p>

<p style="margin-top: 1em">job replay: <br>
--replay REPLAY_JOBID <br>
Replay a job identified by its (partial) hash id. Use <br>
&quot;--replay latest&quot; to replay the latest job. <br>
--replay-test-status REPLAY_TESTSTATUS <br>
Filter tests to replay by test status <br>
--replay-ignore REPLAY_IGNORE <br>
Ignore variants (variants) and/or configuration <br>
(config) from the source job</p>

<p style="margin-top: 1em">resultsdb options: <br>
--resultsdb-api RESULTSDB_API <br>
Specify the resultsdb API url <br>
--resultsdb-logs RESULTSDB_LOGS <br>
Specify the URL where the logs are published</p>

<p style="margin-top: 1em">test execution on a Virtual
Machine: <br>
--vm-domain VM_DOMAIN <br>
Specify Libvirt Domain Name <br>
--vm-hypervisor-uri VM_HYPERVISOR_URI <br>
Specify hypervisor URI driver connection. Current: <br>
qemu:///system <br>
--vm-hostname VM_HOSTNAME <br>
Specify VM hostname to login. By default Avocado <br>
attempts to automatically find the VM IP address. <br>
--vm-port VM_PORT Specify the port number to login on VM.
Current: 22 <br>
--vm-username VM_USERNAME <br>
Specify the username to login on VM <br>
--vm-password VM_PASSWORD <br>
Specify the password to login on VM <br>
--vm-key-file VM_KEY_FILE <br>
Specify an identity file with a private key instead of <br>
a password (Example: .pem files from Amazon EC2) <br>
--vm-cleanup Restore VM to a previous state, before running
tests <br>
--vm-timeout SECONDS Amount of time (in seconds) to wait for
a successful <br>
connection to the virtual machine. Defaults to 120 <br>
seconds.</p>

<p style="margin-top: 1em">wrapper support: <br>
--wrapper SCRIPT[:EXECUTABLE] <br>
Use a script to wrap executables run by a test. The <br>
wrapper is either a path to a script (AKA a global <br>
wrapper) or a path to a script followed by colon <br>
symbol (:), plus a shell like glob to the target <br>
EXECUTABLE. Multiple wrapper options are allowed, but <br>
only one global wrapper can be defined.</p>

<p style="margin-top: 1em">yaml to mux options: <br>
-m [FILE [FILE ...]], --mux-yaml [FILE [FILE ...]] <br>
Location of one or more Avocado multiplex (.yaml) <br>
FILE(s) (order dependent) <br>
--mux-filter-only [MUX_FILTER_ONLY [MUX_FILTER_ONLY ...]]
<br>
Filter only path(s) from multiplexing <br>
--mux-filter-out [MUX_FILTER_OUT [MUX_FILTER_OUT ...]] <br>
Filter out path(s) from multiplexing <br>
--mux-path [MUX_PATH [MUX_PATH ...]] <br>
List of default paths used to determine path priority <br>
when querying for parameters <br>
--mux-inject [MUX_INJECT [MUX_INJECT ...]] <br>
Inject [path:]key:node values into the final multiplex <br>
tree.</p>

<p style="margin-top: 1em">yaml to mux options
[deprecated]: <br>
--multiplex [FILE [FILE ...]] <br>
DEPRECATED: Location of one or more Avocado multiplex <br>
(.yaml) FILE(s) (order dependent) <br>
--filter-only [FILTER_ONLY [FILTER_ONLY ...]] <br>
DEPRECATED: Filter only path(s) from multiplexing (use <br>
--mux-filter-only instead) <br>
--filter-out [FILTER_OUT [FILTER_OUT ...]] <br>
DEPRECATED: Filter out path(s) from multiplexing (use <br>
--mux-filter-out instead)</p>

<p style="margin-top: 1em">Options for subcommand config
(avocado config --help):</p>

<p style="margin-top: 1em">optional arguments: <br>
-h, --help show this help message and exit <br>
--datadir Shows the data directories currently being used by
<br>
avocado <br>
--paginator {on,off} Turn the paginator on/off. Current:
on</p>

<p style="margin-top: 1em">Options for subcommand diff
(avocado diff --help):</p>

<p style="margin-top: 1em">positional arguments: <br>
&lt;JOB&gt; A job reference, identified by a (partial)
unique ID <br>
(SHA1) or test results directory.</p>

<p style="margin-top: 1em">optional arguments: <br>
-h, --help show this help message and exit <br>
--html FILE Enable HTML output to the FILE where the result
should <br>
be written. <br>
--open-browser Generate and open a HTML report in your
preferred <br>
browser. If no --html file is provided, create a <br>
temporary file. <br>
--diff-filter DIFF_FILTER <br>
Comma separated filter of diff sections: <br>
(no)cmdline,(no)time,(no)variants,(no)results, <br>
(no)config,(no)sysinfo (defaults to all enabled). <br>
--paginator {on,off} Turn the paginator on/off. Current: on
<br>
--create-reports Create temporary files with job reports (to
be used by <br>
other diff tools)</p>

<p style="margin-top: 1em">Options for subcommand distro
(avocado distro --help):</p>

<p style="margin-top: 1em">optional arguments: <br>
-h, --help show this help message and exit <br>
--distro-def-create Creates a distro definition file based
on the path <br>
given <br>
--distro-def-name DISTRO_DEF_NAME <br>
Distribution short name <br>
--distro-def-version DISTRO_DEF_VERSION <br>
Distribution major version number <br>
---distro-def-release DISTRO_DEF_RELEASE <br>
Distribution release version number <br>
--distro-def-arch DISTRO_DEF_ARCH <br>
Primary architecture that the distro targets <br>
--distro-def-path DISTRO_DEF_PATH <br>
Top level directory of the distro installation files <br>
--distro-def-type {deb,rpm} <br>
Distro type (one of: deb, rpm)</p>

<p style="margin-top: 1em">Options for subcommand exec-path
(avocado exec-path --help):</p>

<p style="margin-top: 1em">optional arguments: <br>
-h, --help show this help message and exit</p>

<p style="margin-top: 1em">Options for subcommand list
(avocado list --help):</p>

<p style="margin-top: 1em">positional arguments: <br>
reference List of test references (aliases or paths). If
empty, <br>
avocado will list tests on the configured test source, <br>
(see &rsquo;avocado config --datadir&rsquo;) Also, if there
are <br>
other test loader plugins active, tests from those <br>
plugins might also show up (behavior may vary among <br>
plugins)</p>

<p style="margin-top: 1em">optional arguments: <br>
-h, --help show this help message and exit <br>
-V, --verbose Whether to show extra information (headers and
<br>
summary). Current: False <br>
--paginator {on,off} Turn the paginator on/off. Current:
on</p>

<p style="margin-top: 1em">loader options: <br>
--loaders [LOADERS [LOADERS ...]] <br>
Overrides the priority of the test loaders. You can <br>
specify either @loader_name or TEST_TYPE. By default <br>
it tries all available loaders according to priority <br>
set in settings-&gt;plugins.loaders. <br>
--external-runner EXECUTABLE <br>
Path to an specific test runner that allows the use of <br>
its own tests. This should be used for running tests <br>
that do not conform to Avocado&rsquo; SIMPLE testinterface
<br>
and can not run standalone. Note: the use of <br>
--external-runner overwrites the --loaders to <br>
&quot;external_runner&quot; <br>
--external-runner-chdir {runner,test} <br>
Change directory before executing tests. This option <br>
may be necessary because of requirements and/or <br>
limitations of the external test runner. If the <br>
external runner requires to be run from its own base <br>
directory,use &quot;runner&quot; here. If the external
runner <br>
runs tests based on files and requires to be run from <br>
the directory where those files are located, use <br>
&quot;test&quot; here and specify the test directory with
the <br>
option &quot;--external-runner-testdir&quot;. Defaults to
&quot;None&quot; <br>
--external-runner-testdir DIRECTORY <br>
Where test files understood by the external test <br>
runner are located in the filesystem. Obviously this <br>
assumes and only applies to external test runners that <br>
run tests from files</p>

<p style="margin-top: 1em">filtering parameters: <br>
--filter-by-tags TAGS <br>
Filter INSTRUMENTED tests based on &quot;:avocado: <br>
tags=tag1,tag2&quot; notation in their class docstring <br>
--filter-by-tags-include-empty <br>
Include all tests without tags during filtering. This <br>
effectively means they will be kept in the test suite <br>
found previously to filtering.</p>

<p style="margin-top: 1em">Options for subcommand multiplex
(avocado multiplex --help):</p>

<p style="margin-top: 1em">optional arguments: <br>
-h, --help show this help message and exit <br>
--summary SUMMARY Verbosity of the variants summary.
(positive integer - <br>
0, 1, ... - or none, brief, normal, verbose, full, <br>
max) <br>
--variants VARIANTS Verbosity of the list of variants.
(positive integer - <br>
0, 1, ... - or none, brief, normal, verbose, full, <br>
max) <br>
--system-wide Combine the files with the default tree. <br>
-c, --contents [obsoleted by --variants] Shows the node
content <br>
(variables)</p>

<p style="margin-top: 1em">environment view options: <br>
-d, --debug Debug the multiplex tree.</p>

<p style="margin-top: 1em">tree view options: <br>
-t, --tree [obsoleted by --summary] Shows the multiplex tree
<br>
structure <br>
-i, --inherit [obsoleted by --summary] Show the inherited
values</p>

<p style="margin-top: 1em">yaml to mux options: <br>
-m [FILE [FILE ...]], --mux-yaml [FILE [FILE ...]] <br>
Location of one or more Avocado multiplex (.yaml) <br>
FILE(s) (order dependent) <br>
--mux-filter-only [MUX_FILTER_ONLY [MUX_FILTER_ONLY ...]]
<br>
Filter only path(s) from multiplexing <br>
--mux-filter-out [MUX_FILTER_OUT [MUX_FILTER_OUT ...]] <br>
Filter out path(s) from multiplexing <br>
--mux-path [MUX_PATH [MUX_PATH ...]] <br>
List of default paths used to determine path priority <br>
when querying for parameters <br>
--mux-inject [MUX_INJECT [MUX_INJECT ...]] <br>
Inject [path:]key:node values into the final multiplex <br>
tree.</p>

<p style="margin-top: 1em">yaml to mux options
[deprecated]: <br>
--multiplex [FILE [FILE ...]] <br>
DEPRECATED: Location of one or more Avocado multiplex <br>
(.yaml) FILE(s) (order dependent) <br>
--filter-only [FILTER_ONLY [FILTER_ONLY ...]] <br>
DEPRECATED: Filter only path(s) from multiplexing (use <br>
--mux-filter-only instead) <br>
--filter-out [FILTER_OUT [FILTER_OUT ...]] <br>
DEPRECATED: Filter out path(s) from multiplexing (use <br>
--mux-filter-out instead)</p>

<p style="margin-top: 1em">Options for subcommand plugins
(avocado plugins --help):</p>

<p style="margin-top: 1em">optional arguments: <br>
-h, --help show this help message and exit <br>
--paginator {on,off} Turn the paginator on/off. Current:
on</p>

<p style="margin-top: 1em">Options for subcommand sysinfo
(avocado sysinfo --help):</p>

<p style="margin-top: 1em">positional arguments: <br>
sysinfodir Dir where to dump sysinfo</p>

<p style="margin-top: 1em">optional arguments: <br>
-h, --help show this help message and exit</p>

<p style="margin-top: 1em">RUNNING A TEST <br>
The most common use of the avocado command line tool is to
run a test:</p>

<p style="margin-top: 1em">$ avocado run sleeptest.py</p>

<p style="margin-top: 1em">This command will run the
sleeptest.py test, as found on the standard test
directories. The output should be similar to:</p>

<p style="margin-top: 1em">JOB ID : &lt;id&gt; <br>
JOB LOG :
/home/&lt;user&gt;/avocado/job-results/job-&lt;date&gt;-&lt;shortid&gt;/job.log
<br>
(1/1) sleeptest.py:SleepTest.test: PASS (1.01 s) <br>
RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 |
INTERRUPT 0 <br>
JOB TIME : 1.11 s</p>

<p style="margin-top: 1em">The test directories will vary
depending on you system and installation method used. Still,
it&rsquo;s pretty easy to find that out as shown in the next
section.</p>

<p style="margin-top: 1em">DEBUGGING TESTS <br>
When you are developing new tests, frequently you want to
look at the straight output of the job log in the stdout,
without having to tail the job log. In order to do that, you
<br>
can use --show-job-log to the avocado test runner:</p>

<p style="margin-top: 1em">$ scripts/avocado run
examples/tests/sleeptest.py --show-job-log <br>
... <br>
PARAMS (key=timeout, path=*, default=None) =&gt; None <br>
START 1-sleeptest.py:SleepTest.test <br>
PARAMS (key=sleep_length, path=*, default=1) =&gt; 1 <br>
Sleeping for 1.00 seconds <br>
Not logging /var/log/messages (lack of permissions) <br>
PASS 1-sleeptest.py:SleepTest.test <br>
...</p>

<p style="margin-top: 1em">Let&rsquo;s say you are
debugging a test particularly large, with lots of debug
output and you want to reduce this output to only messages
with level &rsquo;INFO&rsquo; and higher. You can set <br>
job-log-level to info to reduce the amount of output.</p>

<p style="margin-top: 1em">Edit your
~/.config/avocado/avocado.conf file and add:</p>

<p style="margin-top: 1em">[job.output] <br>
loglevel = info</p>

<p style="margin-top: 1em">Running the same example with
this option will give you:</p>

<p style="margin-top: 1em">$ scripts/avocado run sleeptest
--show-job-log <br>
... <br>
START 1-sleeptest.py:SleepTest.test <br>
PASS 1-sleeptest.py:SleepTest.test <br>
...</p>

<p style="margin-top: 1em">The levels you can choose are
the levels available in the python logging system
https://docs.python.org/2/library/logging.html#logging-levels,
translated to lowercase strings, so <br>
&rsquo;notset&rsquo;, &rsquo;debug&rsquo;,
&rsquo;info&rsquo;, &rsquo;warning&rsquo;,
&rsquo;error&rsquo;, &rsquo;critical&rsquo;, in order of
severity.</p>

<p style="margin-top: 1em">As you can see, the UI output is
suppressed and only the job log goes to stdout, making this
a useful feature for test development/debugging.</p>

<p style="margin-top: 1em">SILENCING RUNNER STDOUT <br>
You may specify --silent, that means avocado will turn off
all runner stdout. Even if you specify things like
--show-job-log in the CLI, --silent will have precedence and
you <br>
will not get application stdout. Note that --silent does not
affect on disk job logs, those continue to be generated
normally.</p>

<p style="margin-top: 1em">SILENCING SYSINFO REPORT <br>
You may specify --sysinfo=off and avocado will not collect
profilers, hardware details and other system information,
inside the job result directory.</p>

<p style="margin-top: 1em">LISTING TESTS <br>
The avocado command line tool also has a list command, that
lists the known tests in a given path, be it a path to an
individual test, or a path to a directory. If no arguments
<br>
provided, avocado will inspect the contents of the test
location being used by avocado (if you are in doubt about
which one is that, you may use avocado config --datadir).
The <br>
output looks like:</p>

<p style="margin-top: 1em">$ avocado list <br>
INSTRUMENTED /usr/share/avocado/tests/abort.py <br>
INSTRUMENTED /usr/share/avocado/tests/datadir.py <br>
INSTRUMENTED /usr/share/avocado/tests/doublefail.py <br>
INSTRUMENTED /usr/share/avocado/tests/doublefree.py <br>
INSTRUMENTED /usr/share/avocado/tests/errortest.py <br>
INSTRUMENTED /usr/share/avocado/tests/failtest.py <br>
INSTRUMENTED /usr/share/avocado/tests/fiotest.py <br>
INSTRUMENTED /usr/share/avocado/tests/gdbtest.py <br>
INSTRUMENTED /usr/share/avocado/tests/gendata.py <br>
INSTRUMENTED /usr/share/avocado/tests/linuxbuild.py <br>
INSTRUMENTED /usr/share/avocado/tests/multiplextest.py <br>
INSTRUMENTED /usr/share/avocado/tests/passtest.py <br>
INSTRUMENTED /usr/share/avocado/tests/skiptest.py <br>
INSTRUMENTED /usr/share/avocado/tests/sleeptenmin.py <br>
INSTRUMENTED /usr/share/avocado/tests/sleeptest.py <br>
INSTRUMENTED /usr/share/avocado/tests/synctest.py <br>
INSTRUMENTED /usr/share/avocado/tests/timeouttest.py <br>
INSTRUMENTED /usr/share/avocado/tests/trinity.py <br>
INSTRUMENTED /usr/share/avocado/tests/warntest.py <br>
INSTRUMENTED /usr/share/avocado/tests/whiteboard.py</p>

<p style="margin-top: 1em">Here, INSTRUMENTED means that
the files there are python files with an avocado test class
in them, therefore, that they are what we call instrumented
tests. This means those <br>
tests can use all avocado APIs and facilities. Let&rsquo;s
try to list a directory with a bunch of executable shell
scripts:</p>

<p style="margin-top: 1em">$ avocado list
examples/wrappers/ <br>
SIMPLE examples/wrappers/dummy.sh <br>
SIMPLE examples/wrappers/ltrace.sh <br>
SIMPLE examples/wrappers/perf.sh <br>
SIMPLE examples/wrappers/strace.sh <br>
SIMPLE examples/wrappers/time.sh <br>
SIMPLE examples/wrappers/valgrind.sh</p>

<p style="margin-top: 1em">Here, SIMPLE means that those
files are executables, that avocado will simply execute and
return PASS or FAIL depending on their return codes (PASS
-&gt; 0, FAIL -&gt; any integer dif&acirc; <br>
ferent than 0). You can also provide the --verbose, or -V
flag to display files that were detected but are not avocado
tests, along with summary information:</p>

<p style="margin-top: 1em">$ avocado list
examples/gdb-prerun-scripts/ -V <br>
Type Test Tag(s) <br>
NOT_A_TEST examples/gdb-prerun-scripts/README <br>
NOT_A_TEST examples/gdb-prerun-scripts/pass-sigusr1</p>

<p style="margin-top: 1em">TEST TYPES SUMMARY <br>
================== <br>
SIMPLE: 0 <br>
INSTRUMENTED: 0 <br>
MISSING: 0 <br>
NOT_A_TEST: 2</p>

<p style="margin-top: 1em">That summarizes the basic
commands you should be using more frequently when you start
with avocado. Let&rsquo;s talk now about how avocado stores
test results.</p>

<p style="margin-top: 1em">EXPLORING RESULTS <br>
When avocado runs tests, it saves all its results on your
system:</p>

<p style="margin-top: 1em">JOB ID : &lt;id&gt; <br>
JOB LOG :
/home/&lt;user&gt;/avocado/job-results/job-&lt;date&gt;-&lt;shortid&gt;/job.log</p>

<p style="margin-top: 1em">For your convenience, avocado
maintains a link to the latest job run (an avocado run
command in this context), so you can always use
&quot;latest&quot; to browse your test results:</p>

<p style="margin-top: 1em">$ ls
/home/&lt;user&gt;/avocado/job-results/latest <br>
id <br>
jobdata <br>
job.log <br>
results.json <br>
results.tap <br>
results.xml <br>
sysinfo <br>
test-results</p>

<p style="margin-top: 1em">The main log file is job.log,
but every test has its own results directory:</p>

<p style="margin-top: 1em">$ ls -1
~/avocado/job-results/latest/test-results/ <br>
1-sleeptest.py:SleepTest.test</p>

<p style="margin-top: 1em">Since this is a directory, it
should have content similar to:</p>

<p style="margin-top: 1em">$ ls -1
~/avocado/job-results/latest/test-results/1-sleeptest.py
SleepTest.test/ <br>
data <br>
debug.log <br>
remote.log <br>
stderr <br>
stdout <br>
sysinfo <br>
whiteboard</p>

<p style="margin-top: 1em">MULTIPLEX <br>
Avocado has a powerful tool that enables multiple test
scenarios to be run using a single, unmodified test. This
mechanism uses a YAML file called the &rsquo;multiplex
file&rsquo;, that <br>
tells avocado how to multiply all possible test scenarios
automatically.</p>

<p style="margin-top: 1em">A command by the same name,
multiplex, is available on the avocado command line tool,
and enables you to see all the test scenarios that can be
run:</p>

<p style="margin-top: 1em">$ avocado multiplex -m
examples/tests/sleeptest.py.data/sleeptest.yaml -c <br>
Variants generated:</p>

<p style="margin-top: 1em">Variant 1: /run/short <br>
/run/short:sleep_length =&gt; 0.5</p>

<p style="margin-top: 1em">Variant 2: /run/medium <br>
/run/medium:sleep_length =&gt; 1</p>

<p style="margin-top: 1em">Variant 3: /run/long <br>
/run/long:sleep_length =&gt; 5</p>

<p style="margin-top: 1em">Variant 4: /run/longest <br>
/run/longest:sleep_length =&gt; 10</p>

<p style="margin-top: 1em">This is a sample that varies the
parameter sleep_length through the scenarios /run/short
(sleeps for 0.5 s), /run/medium (sleeps for 1 s), /run/long
(sleeps for 5s), /run/longest <br>
(sleeps for 10s). The YAML file (multiplex file) that
produced the output above is:</p>

<p style="margin-top: 1em">!mux <br>
short: <br>
sleep_length: 0.5 <br>
medium: <br>
sleep_length: 1 <br>
long: <br>
sleep_length: 5 <br>
longest: <br>
sleep_length: 10</p>

<p style="margin-top: 1em">You can execute sleeptest in all
variations exposed above with:</p>

<p style="margin-top: 1em">$ avocado run sleeptest.py -m
examples/tests/sleeptest.py.data/sleeptest.yaml</p>

<p style="margin-top: 1em">And the output should look
like:</p>

<p style="margin-top: 1em">JOB ID : &lt;id&gt; <br>
JOB LOG :
/home/&lt;user&gt;/avocado/job-results/job-&lt;date&gt;-&lt;shortid&gt;/job.log
<br>
(1/4) sleeptest.py:SleepTest.test;1: PASS (0.51 s) <br>
(2/4) sleeptest.py:SleepTest.test;2: PASS (1.01 s) <br>
(3/4) sleeptest.py:SleepTest.test;3: PASS (5.02 s) <br>
(4/4) sleeptest.py:SleepTest.test;4: PASS (10.01 s) <br>
RESULTS : PASS 4 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 |
INTERRUPT 0 <br>
JOB TIME : 16.65 s</p>

<p style="margin-top: 1em">The multiplex plugin and the
test runner supports two kinds of global filters, through
the command line options --mux-filter-only and
--mux-filter-out. The mux-filter-only <br>
exclusively includes one or more paths and the
mux-filter-out removes one or more paths from being
processed.</p>

<p style="margin-top: 1em">From the previous example, if we
are interested to use the variants /run/medium and
/run/longest, we do the following command line:</p>

<p style="margin-top: 1em">$ avocado run sleeptest.py -m
examples/tests/sleeptest.py.data/sleeptest.yaml
--mux-filter-only /run/medium /run/longest</p>

<p style="margin-top: 1em">And if you want to remove /small
from the variants created, we do the following:</p>

<p style="margin-top: 1em">$ avocado run sleeptest.py -m
examples/tests/sleeptest.py.data/sleeptest.yaml
--mux-filter-out /run/medium</p>

<p style="margin-top: 1em">Note that both --mux-filter-only
and --mux-filter-out filters can be arranged in the same
command line.</p>

<p style="margin-top: 1em">The multiplexer also supports
default paths. The base path is /run/* but it can be
overridden by --mux-path, which accepts multiple arguments.
What it does: it splits leaves by <br>
the provided paths. Each query goes one by one through those
sub-trees and first one to hit the match returns the result.
It might not solve all problems, but it can help to
com&acirc; <br>
bine existing YAML files with your ones:</p>

<p style="margin-top: 1em">qa: # large and complex
read-only file, content injected into /qa <br>
tests: <br>
timeout: 10 <br>
... <br>
my_variants: !mux # your YAML file injected into
/my_variants <br>
short: <br>
timeout: 1 <br>
long: <br>
timeout: 1000</p>

<p style="margin-top: 1em">You want to use an existing test
which uses params.get(&rsquo;timeout&rsquo;,
&rsquo;*&rsquo;). Then you can use --mux-path
&rsquo;/my_variants/*&rsquo; &rsquo;/qa/*&rsquo; and
it&rsquo;ll first look in your variants. If no matches <br>
are found, then it would proceed to /qa/*</p>

<p style="margin-top: 1em">Keep in mind that only slices
defined in mux-path are taken into account for relative
paths (the ones starting with *).</p>

<p style="margin-top: 1em">DEBUGGING EXECUTABLES RUN AS
PART OF A TEST <br>
One interesting avocado feature is the ability to
automatically and transparently run executables that are
used on a given test inside the GNU debugger.</p>

<p style="margin-top: 1em">Suppose you are running a test
that uses an external, compiled, image converter. Now
suppose you&rsquo;re feeding it with different types of
images, including broken image files, and <br>
it fails at a given point. You wish you could connect to the
debugger at that given source location while your test is
running. This is how to do just that with avocado:</p>

<p style="margin-top: 1em">$ avocado run
--gdb-run-bin=convert:convert_ppm_to_raw converttest.py</p>

<p style="margin-top: 1em">The job starts running just as
usual, and so does your test:</p>

<p style="margin-top: 1em">JOB ID : &lt;id&gt; <br>
JOB LOG :
/home/&lt;user&gt;/avocado/job-results/job-&lt;date&gt;-&lt;shortid&gt;/job.log
<br>
TESTS : 1 <br>
(1/1) converttest.py:ConvertTest.test: /</p>

<p style="margin-top: 1em">The convert executable though,
automatically runs inside GDB. Avocado will stop when the
given breakpoint is reached:</p>

<p style="margin-top: 1em">TEST PAUSED because of debugger
breakpoint. To DEBUG your application run: <br>

/home/&lt;user&gt;/avocado/job-results/job-&lt;date&gt;-&lt;shortid&gt;/test-results/converttest.py/data/convert.gdb.sh</p>

<p style="margin-top: 1em">NOTE: please use *disconnect*
command in gdb before exiting, or else the debugged process
will be KILLED</p>

<p style="margin-top: 1em">From this point, you can run the
generated script (convert.gdb.sh) to debug you
application.</p>

<p style="margin-top: 1em">As noted, it is strongly
recommended that you disconnect from gdb while your
executable is still running. That is, if the executable
finished running while you are debugging it, <br>
avocado has no way to know about its status.</p>

<p style="margin-top: 1em">Avocado will automatically send
a continue command to the debugger when you disconnect from
and exit gdb.</p>

<p style="margin-top: 1em">If, for some reason you have a
custom GDB, or your system does not put GDB on what avocado
believes to be the standard location (/usr/bin/gdb), you can
override that in the sec&acirc; <br>
tion gdb.paths of your documentation:</p>

<p style="margin-top: 1em">[gdb.paths] <br>
gdb = /usr/bin/gdb <br>
gdbserver = /usr/bin/gdbserver</p>

<p style="margin-top: 1em">So running avocado after setting
those will use the appropriate gdb/gdbserver path.</p>

<p style="margin-top: 1em">If you are debugging a special
application and need to setup GDB in custom ways by running
GDB commands, you can do that with the --gdb-prerun-commands
option:</p>

<p style="margin-top: 1em">$ avocado run
--gdb-run-bin=foo:bar
--gdb-prerun-commands=/tmp/disable-signals footest.py</p>

<p style="margin-top: 1em">In this example,
/tmp/disable-signals is a simple text file containing two
lines:</p>

<p style="margin-top: 1em">signal SIGUSR1 pass <br>
signal SIGUSR1 nostop</p>

<p style="margin-top: 1em">Each line is a GDB command, so
you can have from simple to very complex debugging
environments configured like that.</p>

<p style="margin-top: 1em">WRAP EXECUTABLE RUN BY TESTS
<br>
Avocado allows the instrumentation of executables being run
by a test in a transparent way. The user specifies a script
(&quot;the wrapper&quot;) to be used to run the actual
program <br>
called by the test.</p>

<p style="margin-top: 1em">If the instrumentation script is
implemented correctly, it should not interfere with the test
behavior. That is, the wrapper should avoid changing the
return status, standard <br>
output and standard error messages of the original
executable.</p>

<p style="margin-top: 1em">The user can be specific about
which program to wrap (with a shell-like glob), or if that
is omitted, a global wrapper that will apply to all programs
called by the test.</p>

<p style="margin-top: 1em">So, for every executable run by
the test, the program name will be compared to the pattern
to decide whether to wrap it or not. You can have multiples
wrappers and patterns <br>
defined.</p>

<p style="margin-top: 1em">Examples:</p>

<p style="margin-top: 1em">$ avocado run datadir.py
--wrapper examples/wrappers/strace.sh</p>

<p style="margin-top: 1em">Any command created by the test
datadir will be wrapped on strace.sh.</p>

<p style="margin-top: 1em">$ avocado run datadir.py
--wrapper examples/wrappers/ltrace.sh:*make --wrapper
examples/wrappers/perf.sh:*datadir</p>

<p style="margin-top: 1em">Any command that matches the
pattern *make will be wrapper on ltrace.sh and the pattern
*datadir will trigger the execution of perf.sh.</p>

<p style="margin-top: 1em">Note that it is not possible to
use --gdb-run-bin together with --wrapper, they are
incompatible.</p>

<p style="margin-top: 1em">RUNNING TESTS WITH AN EXTERNAL
RUNNER <br>
It&rsquo;s quite common to have organically grown test
suites in most software projects. These usually include a
custom built, very specific test runner that knows how to
find and run <br>
their own tests.</p>

<p style="margin-top: 1em">Still, running those tests
inside Avocado may be a good idea for various reasons,
including being able to have results in different human and
machine readable formats, collecting <br>
system information alongside those tests (the
Avocado&rsquo;s sysinfo functionality), and more.</p>

<p style="margin-top: 1em">Avocado makes that possible by
means of its &quot;external runner&quot; feature. The most
basic way of using it is:</p>

<p style="margin-top: 1em">$ avocado run
--external-runner=/path/to/external_runner foo bar baz</p>

<p style="margin-top: 1em">In this example, Avocado will
report individual test results for tests foo, bar and baz.
The actual results will be based on the return code of
individual executions of <br>
/path/to/external_runner foo, /path/to/external_runner bar
and finally /path/to/external_runner baz.</p>

<p style="margin-top: 1em">As another way to explain an
show how this feature works, think of the &quot;external
runner&quot; as some kind of interpreter and the individual
tests as anything that this interpreter <br>
recognizes and is able to execute. A UNIX shell, say /bin/sh
could be considered an external runner, and files with shell
code could be considered tests:</p>

<p style="margin-top: 1em">$ echo &quot;exit 0&quot; &gt;
/tmp/pass <br>
$ echo &quot;exit 1&quot; &gt; /tmp/fail <br>
$ avocado run --external-runner=/bin/sh /tmp/pass /tmp/fail
<br>
JOB ID : &lt;id&gt; <br>
JOB LOG :
/home/&lt;user&gt;/avocado/job-results/job-&lt;date&gt;-&lt;shortid&gt;/job.log
<br>
TESTS : 2 <br>
(1/2) /tmp/pass: PASS (0.01 s) <br>
(2/2) /tmp/fail: FAIL (0.01 s) <br>
RESULTS : PASS 1 | ERROR 0 | FAIL 1 | SKIP 0 | WARN 0 |
INTERRUPT 0 <br>
JOB TIME : 0.11 s</p>

<p style="margin-top: 1em">This example is pretty obvious,
and could be achieved by giving /tmp/pass and /tmp/fail
shell &quot;shebangs&quot; (#!/bin/sh), making them
executable (chmod +x /tmp/pass /tmp/fail), and <br>
running them as &quot;SIMPLE&quot; tests.</p>

<p style="margin-top: 1em">But now consider the following
example:</p>

<p style="margin-top: 1em">$ avocado run
--external-runner=/bin/curl
http://local-avocado-server:9405/jobs/
http://remote-avocado-server:9405/jobs/ <br>
JOB ID : &lt;id&gt; <br>
JOB LOG :
/home/&lt;user&gt;/avocado/job-results/job-&lt;date&gt;-&lt;shortid&gt;/job.log
<br>
TESTS : 2 <br>
(1/2) http://local-avocado-server:9405/jobs/: PASS (0.02 s)
<br>
(2/2) http://remote-avocado-server:9405/jobs/: FAIL (3.02 s)
<br>
RESULTS : PASS 1 | ERROR 0 | FAIL 1 | SKIP 0 | WARN 0 |
INTERRUPT 0 <br>
JOB TIME : 3.14 s</p>

<p style="margin-top: 1em">This effectively makes /bin/curl
an &quot;external test runner&quot;, responsible for trying
to fetch those URLs, and reporting PASS or FAIL for each of
them.</p>

<p style="margin-top: 1em">RECORDING TEST REFERENCE OUTPUT
<br>
As a tester, you may want to check if the output of a given
application matches an expected output. In order to help
with this common use case, we offer the option --out&acirc;
<br>
put-check-record [mode] to the test runner. If this option
is used, it will store the stdout or stderr of the process
(or both, if you specified all) being executed to reference
<br>
files: stdout.expected and stderr.expected.</p>

<p style="margin-top: 1em">Those files will be recorded in
the test data dir. The data dir is in the same directory as
the test source file, named [source_file_name.data].
Let&rsquo;s take as an example the test <br>
synctest.py. In a fresh checkout of avocado, you can
see:</p>


<p style="margin-top: 1em">examples/tests/synctest.py.data/stderr.expected
<br>
examples/tests/synctest.py.data/stdout.expected</p>

<p style="margin-top: 1em">From those 2 files, only
stdout.expected is non empty:</p>

<p style="margin-top: 1em">$ cat
examples/tests/synctest.py.data/stdout.expected <br>
PAR : waiting <br>
PASS : sync interrupted</p>

<p style="margin-top: 1em">The output files were originally
obtained using the test runner and passing the option
--output-check-record all to the test runner:</p>

<p style="margin-top: 1em">$ avocado run
--output-check-record all examples/tests/synctest.py <br>
JOB ID : &lt;id&gt; <br>
JOB LOG :
/home/&lt;user&gt;/avocado/job-results/job-&lt;date&gt;-&lt;shortid&gt;/job.log
<br>
(1/1) examples/tests/synctest.py:SyncTest.test: PASS (4.00
s) <br>
RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 |
INTERRUPT 0 <br>
JOB TIME : 4.10 s</p>

<p style="margin-top: 1em">After the reference files are
added, the check process is transparent, in the sense that
you do not need to provide special flags to the test runner.
Now, every time the test is <br>
executed, after it is done running, it will check if the
outputs are exactly right before considering the test as
PASSed. If you want to override the default behavior and
skip <br>
output check entirely, you may provide the flag
--output-check=off to the test runner.</p>

<p style="margin-top: 1em">The avocado.utils.process APIs
have a parameter allow_output_check (defaults to all), so
that you can select which process outputs will go to the
reference files, should you <br>
chose to record them. You may choose all, for both stdout
and stderr, stdout, for the stdout only, stderr, for only
the stderr only, or none, to allow neither of them to be
<br>
recorded and checked.</p>

<p style="margin-top: 1em">This process works fine also
with simple tests, executables that return 0 (PASSed) or !=
0 (FAILed). Let&rsquo;s consider our bogus example:</p>

<p style="margin-top: 1em">$ cat output_record.sh <br>
#!/bin/bash <br>
echo &quot;Hello, world!&quot;</p>

<p style="margin-top: 1em">Let&rsquo;s record the output
(both stdout and stderr) for this one:</p>

<p style="margin-top: 1em">$ avocado run output_record.sh
--output-check-record all <br>
JOB ID : &lt;id&gt; <br>
JOB LOG :
/home/&lt;user&gt;/avocado/job-results/job-&lt;date&gt;-&lt;shortid&gt;/job.log
<br>
TESTS : 1 <br>
(1/1) home/$USER/Code/avocado/output_record.sh: PASS (0.01
s) <br>
RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 |
INTERRUPT 0 <br>
JOB TIME : 0.11 s</p>

<p style="margin-top: 1em">After this is done, you&rsquo;ll
notice that a the test data directory appeared in the same
level of our shell script, containing 2 files:</p>

<p style="margin-top: 1em">$ ls output_record.sh.data/ <br>
stderr.expected stdout.expected</p>

<p style="margin-top: 1em">Let&rsquo;s look what&rsquo;s in
each of them:</p>

<p style="margin-top: 1em">$ cat
output_record.sh.data/stdout.expected <br>
Hello, world! <br>
$ cat output_record.sh.data/stderr.expected <br>
$</p>

<p style="margin-top: 1em">Now, every time this test runs,
it&rsquo;ll take into account the expected files that were
recorded, no need to do anything else but run the test.</p>

<p style="margin-top: 1em">RUNNING REMOTE TESTS <br>
Avocado allows you to execute tests on a remote machine by
means of a SSH network connection. The remote machine must
be configured to accept remote connections and the Avocado
<br>
framework have to be installed in both origin and remote
machines.</p>

<p style="margin-top: 1em">When running tests on remote
machine, the test sources and its data (if any present) are
transferred to the remote target, just before the test
execution. After the test execu&acirc; <br>
tion, all test results are transferred back to the origin
machine.</p>

<p style="margin-top: 1em">Here is how to run the sleeptest
example test in a remote machine with IP address
192.168.0.123 (standard port 22), remote user name fedora
and remote user password 123456:</p>

<p style="margin-top: 1em">$ avocado run sleeptest.py
--remote-hostname 192.168.0.123 --remote-username fedora
--remote-password 123456</p>

<p style="margin-top: 1em">The output should look like:</p>

<p style="margin-top: 1em">JOB ID : &lt;id&gt; <br>
JOB LOG :
/home/&lt;user&gt;/avocado/job-results/job-&lt;date&gt;-&lt;shortid&gt;/job.log
<br>
LOGIN : fedora@localhost:22 (TIMEOUT: 60 seconds) <br>
(1/1) sleeptest.py:SleepTest.test: PASS (1.02 s) <br>
RESULTS : PASS 1 | ERROR 0 | FAIL 0 | SKIP 0 | WARN 0 |
INTERRUPT 0 <br>
JOB TIME : 1.12 s</p>

<p style="margin-top: 1em">For more information, please
consult the topic Remote Machine Plugin on Avocado&rsquo;s
online documentation.</p>

<p style="margin-top: 1em">LINUX DISTRIBUTION UTILITIES
<br>
Avocado has some planned features that depend on knowing the
Linux Distribution being used on the system. The most basic
command prints the detected Linux Distribution:</p>

<p style="margin-top: 1em">$ avocado distro <br>
Detected distribution: fedora (x86_64) version 21 release
0</p>

<p style="margin-top: 1em">Other features are available
with the same command when command line options are given,
as shown by the --help option.</p>

<p style="margin-top: 1em">For instance, it possible to
create a so-called &quot;Linux Distribution Definition&quot;
file, by inspecting an installation tree. The installation
tree could be the contents of the offi&acirc; <br>
cial installation ISO or a local network mirror.</p>

<p style="margin-top: 1em">These files let Avocado pinpoint
if a given installed package is part of the original Linux
Distribution or something else that was installed from an
external repository or even <br>
manually. This, in turn, can help detecting regressions in
base system pacakges that affected a given test result.</p>

<p style="margin-top: 1em">To generate a definition file
run:</p>

<p style="margin-top: 1em">$ avocado distro
--distro-def-create --distro-def-name avocadix
--distro-def-version 1 --distro-def-arch x86_64
--distro-def-type rpm --distro-def-path /mnt/dvd</p>

<p style="margin-top: 1em">And the output will be something
like:</p>

<p style="margin-top: 1em">Loading distro information from
tree... Please wait... <br>
Distro information saved to
&quot;avocadix-1-x86_64.distro&quot;</p>

<p style="margin-top: 1em">FILES <br>
System wide configuration file <br>
/etc/avocado/avocado.conf</p>

<p style="margin-top: 1em">Extra configuration files <br>
/etc/avocado/conf.d/</p>

<p style="margin-top: 1em">User configuration file <br>
~/.config/avocado/avocado.conf</p>

<p style="margin-top: 1em">BUGS <br>
If you find a bug, please report it over our github page as
an issue:
https://github.com/avocado-framework/avocado/issues</p>

<p style="margin-top: 1em">LICENSE <br>
Avocado is released under GPLv2 (explicit version)
http://gnu.org/licenses/gpl-2.0.html. Even though most of
the current code is licensed under a &quot;and any later
version&quot; clause, <br>
some parts are specifically bound to the version 2 of the
license and therefore that&rsquo;s the official license of
the prject itself. For more details, please see the LICENSE
file in <br>
the project source code directory.</p>

<p style="margin-top: 1em">MORE INFORMATION <br>
For more information please check Avocado&rsquo;s project
website, located at http://avocado-framework.github.io/.
There you&rsquo;ll find links to online documentation,
source code and com&acirc; <br>
munity resources.</p>

<p style="margin-top: 1em">AUTHOR <br>
Avocado Development Team
&lt;avocado-devel@redhat.com&gt;</p>
 
<p style="margin-top: 1em">AVOCADO(1)</p>
<hr>
</body>
</html>
