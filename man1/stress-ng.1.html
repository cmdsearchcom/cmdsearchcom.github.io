<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 16:39:19 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>STRESS-NG(1) General Commands Manual STRESS-NG(1)</p>

<p style="margin-top: 1em">NAME <br>
stress-ng - a tool to load and stress a computer system</p>

<p style="margin-top: 1em">SYNOPSIS <br>
stress-ng [OPTION [ARG]] ...</p>

<p style="margin-top: 1em">DESCRIPTION <br>
stress-ng will stress test a computer system in various
selectable ways. It was designed to exercise various
physical subsystems of a computer as well as the various
operating <br>
system kernel interfaces. stress-ng also has a wide range of
CPU specific stress tests that exercise floating point,
integer, bit manipulation and control flow.</p>

<p style="margin-top: 1em">stress-ng was originally
intended to make a machine work hard and trip hardware
issues such as thermal overruns as well as operating system
bugs that only occur when a system is <br>
being thrashed hard. Use stress-ng with caution as some of
the tests can make a system run hot on poorly designed
hardware and also can cause excessive system thrashing which
may <br>
be difficult to stop.</p>

<p style="margin-top: 1em">stress-ng can also measure test
throughput rates; this can be useful to observe performance
changes across different operating system releases or types
of hardware. However, it <br>
has never been intended to be used as a precise benchmark
test suite, so do NOT use it in this manner.</p>

<p style="margin-top: 1em">Running stress-ng with root
privileges will adjust out of memory settings on Linux
systems to make the stressors unkillable in low memory
situations, so use this judiciously. <br>
With the appropriate privilege, stress-ng can allow the
ionice class and ionice levels to be adjusted, again, this
should be used with care.</p>

<p style="margin-top: 1em">One can specify the number of
processes to invoke per type of stress test; specifying a
negative or zero value will select the number of processors
available as defined by <br>
sysconf(_SC_NPROCESSORS_CONF).</p>

<p style="margin-top: 1em">OPTIONS <br>
General stress-ng control options:</p>

<p style="margin-top: 1em">--aggressive <br>
enables more file, cache and memory aggressive options. This
may slow tests down, increase latencies and reduce the
number of bogo ops as well as changing the balance of <br>
user time vs system time used depending on the type of
stressor being used.</p>

<p style="margin-top: 1em">-a N, --all N <br>
start N instances of each stressor. If N is less than zero,
then the number of CPUs online is used for the number of
instances. If N is zero, then the number of CPUs in <br>
the system is used.</p>

<p style="margin-top: 1em">-b N, --backoff N <br>
wait N microseconds between the start of each stress worker
process. This allows one to ramp up the stress tests over
time.</p>

<p style="margin-top: 1em">--class name <br>
specify the class of stressors to run. Stressors are
classified into one or more of the following classes: cpu,
cpu-cache, device, io, interrupt, filesystem, memory,
net&acirc; <br>
work, os, pipe, scheduler and vm. Some stressors fall into
just one class. For example the &rsquo;get&rsquo; stressor
is just in the &rsquo;os&rsquo; class. Other stressors fall
into more than <br>
one class, for example, the &rsquo;lsearch&rsquo; stressor
falls into the &rsquo;cpu&rsquo;, &rsquo;cpu-cache&rsquo;
and &rsquo;memory&rsquo; classes as it exercises all these
three. Selecting a specific class will run <br>
all the stressors that fall into that class only when run
with the --sequential option.</p>

<p style="margin-top: 1em">-n, --dry-run <br>
parse options, but do not run stress tests. A no-op.</p>

<p style="margin-top: 1em">-h, --help <br>
show help.</p>

<p style="margin-top: 1em">--ignite-cpu <br>
alter kernel controls to try and maximize the CPU. This
requires root privilege to alter various /sys interface
controls. Currently this only works for Intel P-State <br>
enabled x86 systems on Linux.</p>

<p style="margin-top: 1em">--ionice-class class <br>
specify ionice class (only on Linux). Can be idle (default),
besteffort, be, realtime, rt.</p>

<p style="margin-top: 1em">--ionice-level level <br>
specify ionice level (only on Linux). For idle, 0 is the
only possible option. For besteffort or realtime values 0
(highest priority) to 7 (lowest priority). See ionice(1)
<br>
for more details.</p>

<p style="margin-top: 1em">-k, --keep-name <br>
by default, stress-ng will attempt to change the name of the
stress processes according to their functionality; this
option disables this and keeps the process names to be <br>
the name of the parent process, that is, stress-ng.</p>

<p style="margin-top: 1em">--log-brief <br>
by default stress-ng will report the name of the program,
the message type and the process id as a prefix to all
output. The --log-brief option will output messages
with&acirc; <br>
out these fields to produce a less verbose output.</p>

<p style="margin-top: 1em">--log-file filename <br>
write messages to the specified log file.</p>

<p style="margin-top: 1em">--maximize <br>
overrides the default stressor settings and instead sets
these to the maximum settings allowed. These defaults can
always be overridden by the per stressor settings <br>
options if required.</p>

<p style="margin-top: 1em">--metrics <br>
output number of bogo operations in total performed by the
stress processes. Note that these are not a reliable metric
of performance or throughput and have not been <br>
designed to be used for benchmarking whatsoever. The metrics
are just a useful way to observe how a system behaves when
under various kinds of load.</p>

<p style="margin-top: 1em">The following columns of
information are output:</p>

<p style="margin-top: 1em">Column Heading Explanation <br>
bogo ops number of iterations of the stressor during the
run. This is metric of how much overall &quot;work&quot; has
been achieved in bogo operations. <br>
real time (secs) average wall clock duration (in seconds) of
the stressor. This is the total wall clock time of all the
instances of that particular stressor <br>
divided by the number of these stressors being run. <br>
usr time (secs) total user time (in seconds) consumed
running all the instances of the stressor. <br>
sys time (secs) total system time (in seconds) consumed
running all the instances of the stressor. <br>
bogo ops/s (real time) total bogo operations per second
based on wall clock run time. The wall clock time reflects
the apparent run time. The more processors one has <br>
on a system the more the work load can be distributed onto
these and hence the wall clock time will reduce and the bogo
ops rate will increase. <br>
This is essentially the &quot;apparent&quot; bogo ops rate
of the system. <br>
bogo ops/s (usr+sys time) total bogo operations per second
based on cumulative user and system time. This is the real
bogo ops rate of the system taking into considera&acirc;
<br>
tion the actual time execution time of the stressor across
all the processors. Generally this will decrease as one adds
more concurrent stres&acirc; <br>
sors due to contention on cache, memory, execution units,
buses and I/O devices.</p>

<p style="margin-top: 1em">--metrics-brief <br>
enable metrics and only output metrics that are
non-zero.</p>

<p style="margin-top: 1em">--minimize <br>
overrides the default stressor settings and instead sets
these to the minimum settings allowed. These defaults can
always be overridden by the per stressor settings <br>
options if required.</p>

<p style="margin-top: 1em">--no-advise <br>
from version 0.02.26 stress-ng automatically calls
madvise(2) with random advise options before each mmap and
munmap to stress the the vm subsystem a little harder. The
<br>
--no-advise option turns this default off.</p>

<p style="margin-top: 1em">--no-rand-seed <br>
Do not seed the stress-ng psuedo-random number generator
with a quasi random start seed, but instead seed it with
constant values. This forces tests to run each time using
<br>
the same start conditions which can be useful when one
requires reproduceable stress tests.</p>

<p style="margin-top: 1em">--page-in <br>
touch allocated pages that are not in core, forcing them to
be paged back in. This is a useful option to force all the
allocated pages to be paged in when using the <br>
bigheap, mmap and vm stressors. It will severely degrade
performance when the memory in the system is less than the
allocated buffer sizes. This uses mincore(2) to <br>
determine the pages that are not in core and hence need
touching to page them back in.</p>

<p style="margin-top: 1em">--pathological <br>
enable stressors that are known to hang systems. Some
stressors can quickly consume resources in such a way that
they can rapidly hang a system before the kernel can OOM
<br>
kill them. These stressors are not enabled by default, this
option enables them, but you probably don&rsquo;t want to do
this. You have been warned.</p>

<p style="margin-top: 1em">--perf measure processor and
system activity using perf events. Linux only and caveat
emptor, according to perf_event_open(2): &quot;Always
double-check your results! Various general&acirc; <br>
ized events have had wrong values.&quot;. Note that with
Linux 4.7 one needs to have CAP_SYS_ADMIN capabilities for
this option to work, or adjust /proc/sys/ker&acirc; <br>
nel/perf_event_paranoid to below 2 to use this without
CAP_SYS_ADMIN.</p>

<p style="margin-top: 1em">-q, --quiet <br>
do not show any output.</p>

<p style="margin-top: 1em">-r N, --random N <br>
start N random stress workers. If N is 0, then the number of
configured processors is used for N.</p>

<p style="margin-top: 1em">--sched scheduler <br>
select the named scheduler (only on Linux). To see the list
of available schedulers use: stress-ng --sched which</p>

<p style="margin-top: 1em">--sched-prio prio <br>
select the scheduler priority level (only on Linux). If the
scheduler does not support this then the default priority
level of 0 is chosen.</p>

<p style="margin-top: 1em">--sequential N <br>
sequentially run all the stressors one by one for a default
of 60 seconds. The number of instances of each of the
individual stressors to be started is N. If N is less <br>
than zero, then the number of CPUs online is used for the
number of instances. If N is zero, then the number of CPUs
in the system is used. Use the --timeout option to <br>
specify the duration to run each stressor.</p>

<p style="margin-top: 1em">--stressors <br>
output the names of the available stressors.</p>

<p style="margin-top: 1em">--syslog <br>
log output (except for verbose -v messages) to the
syslog.</p>

<p style="margin-top: 1em">--taskset list <br>
set CPU affinity based on the list of CPUs provided;
stress-ng is bound to just use these CPUs (Linux only). The
CPUs to be used are specified by a comma separated list of
<br>
CPU (0 to N-1). One can specify a range of CPUs using
&rsquo;-&rsquo;, for example: --taskset 0,2-3,6,7-11</p>

<p style="margin-top: 1em">--temp-path path <br>
specify a path for stress-ng temporary directories and
temporary files; the default path is the current working
directory. This path must have read and write access for
<br>
the stress-ng stress processes.</p>

<p style="margin-top: 1em">--thrash <br>
This can only be used when running on Linux and with root
privilege. This option starts a background thrasher process
that works through all the processes on a system and <br>
tries to page as many pages in the processes as possible.
This will cause considerable amount of thrashing of swap on
an over-committed system.</p>

<p style="margin-top: 1em">-t N, --timeout N <br>
stop stress test after N seconds. One can also specify the
units of time in seconds, minutes, hours, days or years with
the suffix s, m, h, d or y.</p>

<p style="margin-top: 1em">--timer-slack N <br>
adjust the per process timer slack to N nanoseconds (Linux
only). Increasing the timer slack allows the kernel to
coalesce timer events by adding some fuzzinesss to timer
<br>
expiration times and hence reduce wakeups. Conversely,
decreasing the timer slack will increase wakeups. A value of
0 for the timer-slack will set the system default of <br>
50,000 nanoseconds.</p>

<p style="margin-top: 1em">--times <br>
show the cumulative user and system times of all the child
processes at the end of the stress run. The percentage of
utilisation of available CPU time is also calculated <br>
from the number of on-line CPUs in the system.</p>

<p style="margin-top: 1em">--tz collect temperatures from
the available thermal zones on the machine (Linux only).
Some devices may have one or more thermal zones, where as
others may have none.</p>

<p style="margin-top: 1em">-v, --verbose <br>
show all debug, warnings and normal information output.</p>

<p style="margin-top: 1em">--verify <br>
verify results when a test is run. This is not available on
all tests. This will sanity check the computations or memory
contents from a test run and report to stderr any <br>
unexpected failures.</p>

<p style="margin-top: 1em">-V, --version <br>
show version.</p>

<p style="margin-top: 1em">-x, --exclude list <br>
specify a list of one or more stressors to exclude (that is,
do not run them). This is useful to exclude specific
stressors when one selects many stressors to run using <br>
the --class option, --sequential, --all and --random
options. Example, run the cpu class stressors concurrently
and exclude the numa and search stressors:</p>

<p style="margin-top: 1em">stress-ng --class cpu --all 1 -x
numa,bsearch,hsearch,lsearch</p>

<p style="margin-top: 1em">-Y, --yaml filename <br>
output gathered statistics to a YAML formatted file named
&rsquo;filename&rsquo;.</p>

<p style="margin-top: 1em">Stressor specific options:</p>

<p style="margin-top: 1em">--affinity N <br>
start N workers that rapidly change CPU affinity (only on
Linux). Rapidly switching CPU affinity can contribute to
poor cache behaviour.</p>

<p style="margin-top: 1em">--affinity-ops N <br>
stop affinity workers after N bogo affinity operations (only
on Linux).</p>

<p style="margin-top: 1em">--affinity-rand <br>
switch CPU affinity randomly rather than the default of
sequentially.</p>

<p style="margin-top: 1em">--af-alg N <br>
start N workers that exercise the AF_ALG socket domain by
hashing and encrypting various sized random messages. This
exercises the SHA1, SHA224, SHA256, SHA384, SHA512, <br>
MD4, MD5, RMD128, RMD160, RMD256, RMD320, WP256, WP384,
WP512, TGR128, TGR160, TGR192 hashes and the cbc(aes),
lrw(aes), ofb(aes), xts(twofish), xts(serpent), xts(cast6),
<br>
xts(camellia), lrw(twofish), lrw(cast6), lrw(camellia),
salsa20 skcipers.</p>

<p style="margin-top: 1em">--af-alg-ops N <br>
stop af-alg workers after N AF_ALG messages are hashed.</p>

<p style="margin-top: 1em">--aio N <br>
start N workers that issue multiple small asynchronous I/O
writes and reads on a relatively small temporary file using
the POSIX aio interface. This will just hit the <br>
file system cache and soak up a lot of user and kernel time
in issuing and handling I/O requests. By default, each
worker process will handle 16 concurrent I/O requests.</p>

<p style="margin-top: 1em">--aio-ops N <br>
stop POSIX asynchronous I/O workers after N bogo
asynchronous I/O requests.</p>

<p style="margin-top: 1em">--aio-requests N <br>
specify the number of POSIX asynchronous I/O requests each
worker should issue, the default is 16; 1 to 4096 are
allowed.</p>

<p style="margin-top: 1em">--aiol N <br>
start N workers that issue multiple 4K random asynchronous
I/O writes using the Linux aio system calls io_setup(2),
io_submit(2), io_getevents(2) and io_destroy(2). By <br>
default, each worker process will handle 16 concurrent I/O
requests.</p>

<p style="margin-top: 1em">--aiol-ops N <br>
stop Linux asynchronous I/O workers after N bogo
asynchronous I/O requests.</p>

<p style="margin-top: 1em">--aiol-requests N <br>
specify the number of Linux asynchronous I/O requests each
worker should issue, the default is 16; 1 to 4096 are
allowed.</p>

<p style="margin-top: 1em">--apparmor N <br>
start N workers that exercise various parts of the AppArmor
interface. Currently one needs root permission to run this
particular test. This test is only available on <br>
Linux systems with AppArmor support.</p>

<p style="margin-top: 1em">--apparmor-ops <br>
stop the AppArmor workers after N bogo operations.</p>

<p style="margin-top: 1em">--atomic N <br>
start N workers that exercise various GCC __atomic_*() built
in operations on 8, 16, 32 and 64 bit intergers that are
shared among the N workers. This stressor is only <br>
available for builds using GCC 4.7.4 or higher. The stressor
forces many front end cache stalls and cache references.</p>

<p style="margin-top: 1em">--atomic-ops N <br>
stop the atomic workers after N bogo atomic operations.</p>

<p style="margin-top: 1em">-B N, --bigheap N <br>
start N workers that grow their heaps by reallocating
memory. If the out of memory killer (OOM) on Linux kills the
worker or the allocation fails then the allocating <br>
process starts all over again. Note that the OOM adjustment
for the worker is set so that the OOM killer will treat
these workers as the first candidate processes to <br>
kill.</p>

<p style="margin-top: 1em">--bigheap-ops N <br>
stop the big heap workers after N bogo allocation operations
are completed.</p>

<p style="margin-top: 1em">--bigheap-growth N <br>
specify amount of memory to grow heap by per iteration. Size
can be from 4K to 64MB. Default is 64K.</p>

<p style="margin-top: 1em">--bind-mount N <br>
start N workers that repeatedly bind mount / to / inside a
user namespace. This can consume resources rapidly, forcing
out of memory situations. Do not use this stressor <br>
unless you want to risk hanging your machine.</p>

<p style="margin-top: 1em">--bind-mount-ops N <br>
stop after N bind mount bogo operations.</p>

<p style="margin-top: 1em">--brk N <br>
start N workers that grow the data segment by one page at a
time using multiple brk(2) calls. Each successfully
allocated new page is touched to ensure it is resident in
<br>
memory. If an out of memory condition occurs then the test
will reset the data segment to the point before it started
and repeat the data segment resizing over again. <br>
The process adjusts the out of memory setting so that it may
be killed by the out of memory (OOM) killer before other
processes. If it is killed by the OOM killer then it <br>
will be automatically re-started by a monitoring parent
process.</p>

<p style="margin-top: 1em">--brk-ops N <br>
stop the brk workers after N bogo brk operations.</p>

<p style="margin-top: 1em">--brk-notouch <br>
do not touch each newly allocated data segment page. This
disables the default of touching each newly allocated page
and hence avoids the kernel from necessarily backing <br>
the page with real physical memory.</p>

<p style="margin-top: 1em">--bsearch N <br>
start N workers that binary search a sorted array of 32 bit
integers using bsearch(3). By default, there are 65536
elements in the array. This is a useful method to
exer&acirc; <br>
cise random access of memory and processor cache.</p>

<p style="margin-top: 1em">--bsearch-ops N <br>
stop the bsearch worker after N bogo bsearch operations are
completed.</p>

<p style="margin-top: 1em">--bsearch-size N <br>
specify the size (number of 32 bit integers) in the array to
bsearch. Size can be from 1K to 4M.</p>

<p style="margin-top: 1em">-C N, --cache N <br>
start N workers that perform random wide spread memory read
and writes to thrash the CPU cache. The code does not
intelligently determine the CPU cache configuration and <br>
so it may be sub-optimal in producing hit-miss read/write
activity for some processors.</p>

<p style="margin-top: 1em">--cache-fence <br>
force write serialization on each store operation (x86
only). This is a no-op for non-x86 architectures.</p>

<p style="margin-top: 1em">--cache-flush <br>
force flush cache on each store operation (x86 only). This
is a no-op for non-x86 architectures.</p>

<p style="margin-top: 1em">--cache-level N <br>
specify level of cache to exercise (1=L1 cache, 2=L2 cache,
3=L3/LLC cache (the default)). If the cache hierarchy cannot
be determined, built-in defaults will apply.</p>

<p style="margin-top: 1em">--cache-no-affinity <br>
do not change processor affinity when --cache is in
effect.</p>

<p style="margin-top: 1em">--cache-ops N <br>
stop cache thrash workers after N bogo cache thrash
operations.</p>

<p style="margin-top: 1em">--cache-prefetch <br>
force read prefetch on next read address on architectures
that support prefetching.</p>

<p style="margin-top: 1em">--cache-ways N <br>
specify the number of cache ways to exercise. This allows a
subset of the overall cache size to be exercised.</p>

<p style="margin-top: 1em">--cap N <br>
start N workers that read per process capabililties via
calls to capget(2) (Linux only).</p>

<p style="margin-top: 1em">--cap-ops N <br>
stop after N cap bogo operations.</p>

<p style="margin-top: 1em">--chdir N <br>
start N workers that change directory between 8192
directories using chdir(2).</p>

<p style="margin-top: 1em">--chdir-ops N <br>
stop after N chdir bogo operations.</p>

<p style="margin-top: 1em">--chmod N <br>
start N workers that change the file mode bits via chmod(2)
and fchmod(2) on the same file. The greater the value for N
then the more contention on the single file. The <br>
stressor will work through all the combination of mode
bits.</p>

<p style="margin-top: 1em">--chmod-ops N <br>
stop after N chmod bogo operations.</p>

<p style="margin-top: 1em">--chown N <br>
start N workers that exercise chown(2) on the same file. The
greater the value for N then the more contention on the
single file.</p>

<p style="margin-top: 1em">--chown-ops N <br>
stop the chown workers after N bogo chown(2) operations.</p>

<p style="margin-top: 1em">--clock N <br>
start N workers exercising clocks and POSIX timers. For all
known clock types this will exercise clock_getres(2),
clock_gettime(2) and clock_nanosleep(2). For all known <br>
timers it will create a 50000ns timer and busy poll this
until it expires. This stressor will cause frequent context
switching.</p>

<p style="margin-top: 1em">--clock-ops N <br>
stop clock stress workers after N bogo operations.</p>

<p style="margin-top: 1em">--clone N <br>
start N workers that create clones (via the clone(2) system
call). This will rapidly try to create a default of 8192
clones that immediately die and wait in a zombie state <br>
until they are reaped. Once the maximum number of clones is
reached (or clone fails because one has reached the maximum
allowed) the oldest clone thread is reaped and a <br>
new clone is then created in a first-in first-out manner,
and then repeated. A random clone flag is selected for each
clone to try to exercise different clone operarions. <br>
The clone stressor is a Linux only option.</p>

<p style="margin-top: 1em">--clone-ops N <br>
stop clone stress workers after N bogo clone operations.</p>

<p style="margin-top: 1em">--clone-max N <br>
try to create as many as N clone threads. This may not be
reached if the system limit is less than N.</p>

<p style="margin-top: 1em">--context N <br>
start N workers that run three threads that use
swapcontext(3) to implement the thread-to-thread context
switching. This exercises rapid process context saving and
restor&acirc; <br>
ing and is bandwidth limited by register and memory save and
restore rates.</p>

<p style="margin-top: 1em">--context-ops N <br>
stop N context workers after N bogo context switches. In
this stressor, 1 bogo op is equivalent to 1000 swapcontext
calls.</p>

<p style="margin-top: 1em">--copy-file N <br>
start N stressors that copy a file using the Linux
copy_file_range(2) system call. 2MB chunks of data are
copyied from random locations from one file to random
locations <br>
to a destination file. By default, the files are 256 MB in
size. Data is sync&rsquo;d to the filesystem after each
copy_file_range(2) call.</p>

<p style="margin-top: 1em">--copy-file-ops N <br>
stop after N copy_file_range() calls.</p>

<p style="margin-top: 1em">--copy-file-bytes N <br>
copy file size, the default is 256 MB. One can specify the
size in units of Bytes, KBytes, MBytes and GBytes using the
suffix b, k, m or g.</p>

<p style="margin-top: 1em">-c N, --cpu N <br>
start N workers exercising the CPU by sequentially working
through all the different CPU stress methods. Instead of
exercising all the CPU stress methods, one can specify <br>
a specific CPU stress method with the --cpu-method
option.</p>

<p style="margin-top: 1em">--cpu-ops N <br>
stop cpu stress workers after N bogo operations.</p>

<p style="margin-top: 1em">-l P, --cpu-load P <br>
load CPU with P percent loading for the CPU stress workers.
0 is effectively a sleep (no load) and 100 is full loading.
The loading loop is broken into compute time <br>
(load%) and sleep time (100% - load%). Accuracy depends on
the overall load of the processor and the responsiveness of
the scheduler, so the actual load may be different <br>
from the desired load. Note that the number of bogo CPU
operations may not be linearly scaled with the load as some
systems employ CPU frequency scaling and so heavier <br>
loads produce an increased CPU frequency and greater CPU
bogo operations.</p>

<p style="margin-top: 1em">--cpu-load-slice S <br>
note - this option is only useful when --cpu-load is less
than 100%. The CPU load is broken into multiple busy and
idle cycles. Use this option to specify the duration of <br>
a busy time slice. A negative value for S specifies the
number of iterations to run before idling the CPU (e.g. -30
invokes 30 iterations of a CPU stress loop). A zero <br>
value selects a random busy time between 0 and 0.5 seconds.
A positive value for S specifies the number of milliseconds
to run before idling the CPU (e.g. 100 keeps the <br>
CPU busy for 0.1 seconds). Specifying small values for S
lends to small time slices and smoother scheduling. Setting
--cpu-load as a relatively low value and <br>
--cpu-load-slice to be large will cycle the CPU between long
idle and busy cycles and exercise different CPU frequencies.
The thermal range of the CPU is also cycled, so <br>
this is a good mechanism to exercise the scheduler,
frequency scaling and passive/active thermal cooling
mechanisms.</p>

<p style="margin-top: 1em">--cpu-method method <br>
specify a cpu stress method. By default, all the stress
methods are exercised sequentially, however one can specify
just one method to be used if required. Available cpu <br>
stress methods are described as follows:</p>

<p style="margin-top: 1em">Method Description <br>
all iterate over all the below cpu stress methods <br>
ackermann Ackermann function: compute A(3, 10), where: <br>
A(m, n) = n + 1 if m = 0; <br>
A(m - 1, 1) if m &gt; 0 and n = 0; <br>
A(m - 1, A(m, n - 1)) if m &gt; 0 and n &gt; 0 <br>
bitops various bit operations from bithack, namely: reverse
bits, parity check, bit count, round to nearest power of 2
<br>
callfunc recursively call 8 argument C function to a depth
of 1024 calls and unwind</p>

<p style="margin-top: 1em">cfloat 1000 iterations of a mix
of floating point complex operations <br>
cdouble 1000 iterations of a mix of double floating point
complex operations <br>
clongdouble 1000 iterations of a mix of long double floating
point complex operations <br>
correlate perform a 16384 &Atilde; 1024 correlation of
random doubles <br>
crc16 compute 1024 rounds of CCITT CRC16 on random data <br>
decimal32 1000 iterations of a mix of 32 bit decimal
floating point operations (GCC only) <br>
decimal64 1000 iterations of a mix of 64 bit decimal
floating point operations (GCC only) <br>
decimal128 1000 iterations of a mix of 128 bit decimal
floating point operations (GCC only) <br>
dither Floyd&acirc;Steinberg dithering of a 1024 &Atilde;
768 random image from 8 bits down to 1 bit of depth. <br>
djb2a 128 rounds of hash DJB2a (Dan Bernstein hash using the
xor variant) on 128 to 1 bytes of random strings <br>
double 1000 iterations of a mix of double precision floating
point operations <br>
euler compute e using n = (1 + (1 &Atilde;&middot; n))
&acirc; n <br>
explog iterate on n = exp(log(n) &Atilde;&middot; 1.00002)
<br>
fibonacci compute Fibonacci sequence of 0, 1, 1, 2, 5, 8...
<br>
fft 4096 sample Fast Fourier Transform <br>
float 1000 iterations of a mix of floating point operations
<br>
fnv1a 128 rounds of hash FNV-1a (Fowler&acirc;Noll&acirc;Vo
hash using the xor then multiply variant) on 128 to 1 bytes
of random strings <br>
gamma calculate the Euler-Mascheroni constant &Icirc;&sup3;
using the limiting difference between the harmonic series (1
+ 1/2 + 1/3 + 1/4 + 1/5 ... + 1/n) and the natural <br>
logarithm ln(n), for n = 80000. <br>
gcd compute GCD of integers <br>
gray calculate binary to gray code and gray code back to
binary for integers from 0 to 65535 <br>
hamming compute Hamming H(8,4) codes on 262144 lots of 4 bit
data. This turns 4 bit data into 8 bit Hamming code
containing 4 parity bits. For data bits d1..d4, <br>
parity bits are computed as: <br>
p1 = d2 + d3 + d4 <br>
p2 = d1 + d3 + d4 <br>
p3 = d1 + d2 + d4 <br>
p4 = d1 + d2 + d3 <br>
hanoi solve a 21 disc Towers of Hanoi stack using the
recursive solution <br>
hyperbolic compute sinh(&Icirc;&cedil;) &Atilde;
cosh(&Icirc;&cedil;) + sinh(2&Icirc;&cedil;) +
cosh(3&Icirc;&cedil;) for float, double and long double
hyperbolic sine and cosine functions where &Icirc;&cedil; =
0 to 2&Iuml; in 1500 steps <br>
idct 8 &Atilde; 8 IDCT (Inverse Discrete Cosine Transform)
<br>
int8 1000 iterations of a mix of 8 bit integer operations
<br>
int16 1000 iterations of a mix of 16 bit integer operations
<br>
int32 1000 iterations of a mix of 32 bit integer operations
<br>
int64 1000 iterations of a mix of 64 bit integer operations
<br>
int128 1000 iterations of a mix of 128 bit integer
operations (GCC only) <br>
int32float 1000 iterations of a mix of 32 bit integer and
floating point operations <br>
int32double 1000 iterations of a mix of 32 bit integer and
double precision floating point operations <br>
int32longdouble 1000 iterations of a mix of 32 bit integer
and long double precision floating point operations <br>
int64float 1000 iterations of a mix of 64 bit integer and
floating point operations <br>
int64double 1000 iterations of a mix of 64 bit integer and
double precision floating point operations <br>
int64longdouble 1000 iterations of a mix of 64 bit integer
and long double precision floating point operations <br>
int128float 1000 iterations of a mix of 128 bit integer and
floating point operations (GCC only) <br>
int128double 1000 iterations of a mix of 128 bit integer and
double precision floating point operations (GCC only) <br>
int128longdouble 1000 iterations of a mix of 128 bit integer
and long double precision floating point operations (GCC
only) <br>
int128decimal32 1000 iterations of a mix of 128 bit integer
and 32 bit decimal floating point operations (GCC only) <br>
int128decimal64 1000 iterations of a mix of 128 bit integer
and 64 bit decimal floating point operations (GCC only) <br>
int128decimal128 1000 iterations of a mix of 128 bit integer
and 128 bit decimal floating point operations (GCC only)
<br>
jenkin Jenkin&rsquo;s integer hash on 128 rounds of 128..1
bytes of random data <br>
jmp Simple unoptimised compare &gt;, &lt;, == and jmp
branching <br>
ln2 compute ln(2) based on series: <br>
1 - 1/2 + 1/3 - 1/4 + 1/5 - 1/6 ... <br>
longdouble 1000 iterations of a mix of long double precision
floating point operations <br>
loop simple empty loop <br>
matrixprod matrix product of two 128 &Atilde; 128 matrices
of double floats. Testing on 64 bit x86 hardware shows that
this is provides a good mix of memory, cache and <br>
floating point operations and is probably the best CPU
method to use to make a CPU run hot. <br>
nsqrt compute sqrt() of long doubles using Newton-Raphson
<br>
omega compute the omega constant defined by
&Icirc;&copy;e&acirc;&Icirc;&copy; = 1 using efficient
iteration of &Icirc;&copy;n+1 = (1 + &Icirc;&copy;n) / (1 +
e&acirc;&Icirc;&copy;n) <br>
parity compute parity using various methods from the
Standford Bit Twiddling Hacks. Methods employed are: the
na&Atilde;&macr;ve way, the na&Atilde;&macr;ve way with the
Brian Kernigan <br>
bit counting optimisation, the multiply way, the parallel
way, and the lookup table ways (2 variations). <br>
phi compute the Golden Ratio &Iuml; using series <br>
pi compute &Iuml; using the Srinivasa Ramanujan fast
convergence algorithm <br>
pjw 128 rounds of hash pjw function on 128 to 1 bytes of
random strings <br>
prime find all the primes in the range 1..1000000 using a
slightly optimised brute force na&Atilde;&macr;ve trial
division search <br>
psi compute &Iuml; (the reciprocal Fibonacci constant) using
the sum of the reciprocals of the Fibonacci numbers <br>
queens compute all the solutions of the classic 8 queens
problem for board sizes 1..12</p>

<p style="margin-top: 1em">rand 16384 iterations of rand(),
where rand is the MWC pseudo random number generator. The
MWC random function concatenates two 16 bit
multiply-with-carry <br>
generators: <br>
x(n) = 36969 &Atilde; x(n - 1) + carry, <br>
y(n) = 18000 &Atilde; y(n - 1) + carry mod 2 &acirc; 16</p>

<p style="margin-top: 1em">and has period of around 2
&acirc; 60 <br>
rand48 16384 iterations of drand48(3) and lrand48(3) <br>
rgb convert RGB to YUV and back to RGB (CCIR 601) <br>
sdbm 128 rounds of hash sdbm (as used in the SDBM database
and GNU awk) on 128 to 1 bytes of random strings <br>
sieve find the primes in the range 1..10000000 using the
sieve of Eratosthenes <br>
sqrt compute sqrt(rand()), where rand is the MWC pseudo
random number generator <br>
trig compute sin(&Icirc;&cedil;) &Atilde;
cos(&Icirc;&cedil;) + sin(2&Icirc;&cedil;) +
cos(3&Icirc;&cedil;) for float, double and long double sine
and cosine functions where &Icirc;&cedil; = 0 to 2&Iuml; in
1500 steps <br>
union perform integer arithmetic on a mix of bit fields in a
C union. This exercises how well the compiler and CPU can
perform integer bit field loads and <br>
stores. <br>
zeta compute the Riemann Zeta function &Icirc;&para;(s) for
s = 2.0..10.0</p>

<p style="margin-top: 1em">Note that some of these methods
try to exercise the CPU with computations found in some real
world use cases. However, the code has not been optimised on
a per-architec&acirc; <br>
ture basis, so may be a sub-optimal compared to
hand-optimised code used in some applications. They do try
to represent the typical instruction mixes found in these
use <br>
cases.</p>

<p style="margin-top: 1em">--cpu-online N <br>
start N workers that put randomly selected CPUs offline and
online. This Linux only stressor requires root privilege to
perform this action.</p>

<p style="margin-top: 1em">--cpu-online-ops N <br>
stop after offline/online operations.</p>

<p style="margin-top: 1em">--crypt N <br>
start N workers that encrypt a 16 character random password
using crypt(3). The password is encrypted using MD5, SHA-256
and SHA-512 encryption methods.</p>

<p style="margin-top: 1em">--crypt-ops N <br>
stop after N bogo encryption operations.</p>

<p style="margin-top: 1em">--daemon N <br>
start N workers that each create a daemon that dies
immediately after creating another daemon and so on. This
effectively works through the process table with short lived
<br>
processes that do not have a parent and are waited for by
init. This puts pressure on init to do rapid child reaping.
The daemon processes perform the usual mix of calls <br>
to turn into typical UNIX daemons, so this artificially
mimics very heavy daemon system stress.</p>

<p style="margin-top: 1em">--daemon-ops N <br>
stop daemon workers after N daemons have been created.</p>

<p style="margin-top: 1em">-D N, --dentry N <br>
start N workers that create and remove directory entries.
This should create file system meta data activity. The
directory entry names are suffixed by a gray-code encoded
<br>
number to try to mix up the hashing of the namespace.</p>

<p style="margin-top: 1em">--dentry-ops N <br>
stop denty thrash workers after N bogo dentry
operations.</p>

<p style="margin-top: 1em">--dentry-order [ forward |
reverse | stride | random ] <br>
specify unlink order of dentries, can be one of forward,
reverse, stride or random. By default, dentries are unlinked
in random order. The forward order will unlink them <br>
from first to last, reverse order will unlink them from last
to first, stride order will unlink them by stepping around
order in a quasi-random pattern and random order <br>
will randomly select one of forward, reverse or stride
orders.</p>

<p style="margin-top: 1em">--dentries N <br>
create N dentries per dentry thrashing loop, default is
2048.</p>

<p style="margin-top: 1em">--dir N <br>
start N workers that create and remove directories using
mkdir and rmdir.</p>

<p style="margin-top: 1em">--dir-ops N <br>
stop directory thrash workers after N bogo directory
operations.</p>

<p style="margin-top: 1em">--dnotify N <br>
start N workers performing file system activities such as
making/deleting files/directories, renaming files, etc. to
stress exercise the various dnotify events (Linux <br>
only).</p>

<p style="margin-top: 1em">--dnotify-ops N <br>
stop inotify stress workers after N dnotify bogo
operations.</p>

<p style="margin-top: 1em">--dup N <br>
start N workers that perform dup(2) and then close(2)
operations on /dev/zero. The maximum opens at one time is
system defined, so the test will run up to this maximum,
<br>
or 65536 open file descriptors, which ever comes first.</p>

<p style="margin-top: 1em">--dup-ops N <br>
stop the dup stress workers after N bogo open
operations.</p>

<p style="margin-top: 1em">--epoll N <br>
start N workers that perform various related socket stress
activity using epoll_wait(2) to monitor and handle new
connections. This involves client/server processes
per&acirc; <br>
forming rapid connect, send/receives and disconnects on the
local host. Using epoll allows a large number of connections
to be efficiently handled, however, this can lead <br>
to the connection table filling up and blocking further
socket connections, hence impacting on the epoll bogo op
stats. For ipv4 and ipv6 domains, multiple servers are <br>
spawned on multiple ports. The epoll stressor is for Linux
only.</p>

<p style="margin-top: 1em">--epoll-domain D <br>
specify the domain to use, the default is unix (aka local).
Currently ipv4, ipv6 and unix are supported.</p>

<p style="margin-top: 1em">--epoll-port P <br>
start at socket port P. For N epoll worker processes, ports
P to (P * 4) - 1 are used for ipv4, ipv6 domains and ports P
to P - 1 are used for the unix domain.</p>

<p style="margin-top: 1em">--epoll-ops N <br>
stop epoll workers after N bogo operations.</p>

<p style="margin-top: 1em">--eventfd N <br>
start N parent and child worker processes that read and
write 8 byte event messages between them via the eventfd
mechanism (Linux only).</p>

<p style="margin-top: 1em">--eventfd-ops N <br>
stop eventfd workers after N bogo operations.</p>

<p style="margin-top: 1em">--exec N <br>
start N workers continually forking children that exec
stress-ng and then exit almost immediately.</p>

<p style="margin-top: 1em">--exec-ops N <br>
stop exec stress workers after N bogo operations.</p>

<p style="margin-top: 1em">--exec-max P <br>
create P child processes that exec stress-ng and then wait
for them to exit per iteration. The default is just 1;
higher values will create many temporary zombie processes
<br>
that are waiting to be reaped. One can potentially fill up
the process table using high values for --exec-max and
--exec.</p>

<p style="margin-top: 1em">-F N, --fallocate N <br>
start N workers continually fallocating (preallocating file
space) and ftuncating (file truncating) temporary files. If
the file is larger than the free space, fallocate <br>
will produce an ENOSPC error which is ignored by this
stressor.</p>

<p style="margin-top: 1em">--fallocate-bytes N <br>
allocated file size, the default is 1 GB. One can specify
the size in units of Bytes, KBytes, MBytes and GBytes using
the suffix b, k, m or g.</p>

<p style="margin-top: 1em">--fallocate-ops N <br>
stop fallocate stress workers after N bogo fallocate
operations.</p>

<p style="margin-top: 1em">--fault N <br>
start N workers that generates minor and major page
faults.</p>

<p style="margin-top: 1em">--fault-ops N <br>
stop the page fault workers after N bogo page fault
operations.</p>

<p style="margin-top: 1em">--fcntl N <br>
start N workers that perform fcntl(2) calls with various
commands. The exercised commands (if available) are:
F_DUPFD, F_DUPFD_CLOEXEC, F_GETFD, F_SETFD, F_GETFL, <br>
F_SETFL, F_GETOWN, F_SETOWN, F_GETOWN_EX, F_SETOWN_EX,
F_GETSIG, F_SETSIG, F_GETLK, F_SETLK, F_SETLKW, F_OFD_GETLK,
F_OFD_SETLK and F_OFD_SETLKW.</p>

<p style="margin-top: 1em">--fcntl-ops N <br>
stop the fcntl workers after N bogo fcntl operations.</p>

<p style="margin-top: 1em">--fiemap N <br>
start N workers that each create a file with many randomly
changing extents and has 4 child processes per worker that
gather the extent information using the FS_IOC_FIEMAP <br>
ioctl(2).</p>

<p style="margin-top: 1em">--fiemap-ops N <br>
stop after N fiemap bogo operations.</p>

<p style="margin-top: 1em">--fiemap-bytes N <br>
specify the size of the fiemap&rsquo;d file in bytes. One
can specify the size in units of Bytes, KBytes, MBytes and
GBytes using the suffix b, k, m or g. Larger files will <br>
contain more extents, causing more stress when gathering
extent information.</p>

<p style="margin-top: 1em">--fifo N <br>
start N workers that exercise a named pipe by transmitting
64 bit integers.</p>

<p style="margin-top: 1em">--fifo-ops N <br>
stop fifo workers after N bogo pipe write operations.</p>

<p style="margin-top: 1em">--fifo-readers N <br>
for each worker, create N fifo reader workers that read the
named pipe using simple blocking reads.</p>

<p style="margin-top: 1em">--filename N <br>
start N workers that exercise file creation using various
length filenames containing a range of allower filename
characters. This will try to see if it can exceed the <br>
file system allowed filename length was well as test various
filename lengths between 1 and the maximum allowed by the
file system.</p>

<p style="margin-top: 1em">--filename-ops N <br>
stop filename workers after N bogo filename tests.</p>

<p style="margin-top: 1em">--filename-opts opt <br>
use characters in the filename based on option
&rsquo;opt&rsquo;. Valid options are:</p>

<p style="margin-top: 1em">Option Description <br>
probe default option, probe the file system for valid
allowed characters in a file name and use these <br>
posix use characters as specifed by The Open Group Base
Specifications Issue 7, POSIX.1-2008, 3.278 Portable
Filename Character Set <br>
ext use characters allowed by the ext2, ext3, ext4 file
systems, namely any 8 bit character apart from NUL and /</p>

<p style="margin-top: 1em">--flock N <br>
start N workers locking on a single file.</p>

<p style="margin-top: 1em">--flock-ops N <br>
stop flock stress workers after N bogo flock operations.</p>

<p style="margin-top: 1em">-f N, --fork N <br>
start N workers continually forking children that
immediately exit.</p>

<p style="margin-top: 1em">--fork-ops N <br>
stop fork stress workers after N bogo operations.</p>

<p style="margin-top: 1em">--fork-max P <br>
create P child processes and then wait for them to exit per
iteration. The default is just 1; higher values will create
many temporary zombie processes that are waiting to <br>
be reaped. One can potentially fill up the the process table
using high values for --fork-max and --fork.</p>

<p style="margin-top: 1em">--fp-error N <br>
start N workers that generate floating point exceptions.
Computations are performed to force and check for the
FE_DIVBYZERO, FE_INEXACT, FE_INVALID, FE_OVERFLOW and <br>
FE_UNDERFLOW exceptions. EDOM and ERANGE errors are also
checked.</p>

<p style="margin-top: 1em">--fp-error-ops N <br>
stop after N bogo floating point exceptions.</p>

<p style="margin-top: 1em">--fstat N <br>
start N workers fstat&rsquo;ing files in a directory
(default is /dev).</p>

<p style="margin-top: 1em">--fstat-ops N <br>
stop fstat stress workers after N bogo fstat operations.</p>

<p style="margin-top: 1em">--fstat-dir directory <br>
specify the directory to fstat to override the default of
/dev. All the files in the directory will be fstat&rsquo;d
repeatedly.</p>

<p style="margin-top: 1em">--full N <br>
start N workers that exercise /dev/full. This attempts to
write to the device (which should always get error ENOSPC),
to read from the device (which should always return <br>
a buffer of zeros) and to seek randomly on the device (which
should always succeed). (Linux only).</p>

<p style="margin-top: 1em">--full-ops N <br>
stop the stress full workers after N bogo I/O
operations.</p>

<p style="margin-top: 1em">--futex N <br>
start N workers that rapidly exercise the futex system call.
Each worker has two processes, a futex waiter and a futex
waker. The waiter waits with a very small timeout to <br>
stress the timeout and rapid polled futex waiting. This is a
Linux specific stress option.</p>

<p style="margin-top: 1em">--futex-ops N <br>
stop futex workers after N bogo successful futex wait
operations.</p>

<p style="margin-top: 1em">--get N <br>
start N workers that call all the get*(2) system calls.</p>

<p style="margin-top: 1em">--get-ops N <br>
stop get workers after N bogo get operations.</p>

<p style="margin-top: 1em">--getdent N <br>
start N workers that recursively read directories /proc,
/dev/, /tmp, /sys and /run using getdents and getdents64
(Linux only).</p>

<p style="margin-top: 1em">--getdent-ops N <br>
stop getdent workers after N bogo getdent bogo
operations.</p>

<p style="margin-top: 1em">--getrandom N <br>
start N workers that get 8192 random bytes from the
/dev/urandom pool using the getrandom(2) system call (Linux
only).</p>

<p style="margin-top: 1em">--getrandom-ops N <br>
stop getrandom workers after N bogo get operations.</p>

<p style="margin-top: 1em">--handle N <br>
start N workers that exercise the name_to_handle_at(2) and
open_by_handle_at(2) system calls. (Linux only).</p>

<p style="margin-top: 1em">--handle-ops N <br>
stop after N handle bogo operations.</p>

<p style="margin-top: 1em">-d N, --hdd N <br>
start N workers continually writing, reading and removing
temporary files. The default mode is to stress test
sequential writes and reads. With the --ggressive option
<br>
enabled without any --hdd-opts options the hdd stressor will
work through all the --hdd-opt options one by one to cover a
range of I/O options.</p>

<p style="margin-top: 1em">--hdd-bytes N <br>
write N bytes for each hdd process, the default is 1 GB. One
can specify the size in units of Bytes, KBytes, MBytes and
GBytes using the suffix b, k, m or g.</p>

<p style="margin-top: 1em">--hdd-opts list <br>
specify various stress test options as a comma separated
list. Options are as follows:</p>

<p style="margin-top: 1em">Option Description <br>
direct try to minimize cache effects of the I/O. File I/O
writes are performed directly from user space buffers and
synchronous transfer is also attempted. To <br>
guarantee synchronous I/O, also use the sync option. <br>
dsync ensure output has been transferred to underlying
hardware and file metadata has been updated (using the
O_DSYNC open flag). This is equivalent to each <br>
write(2) being followed by a call to fdatasync(2). See also
the fdatasync option. <br>
fadv-dontneed advise kernel to expect the data will not be
accessed in the near future. <br>
fadv-noreuse advise kernel to expect the data to be accessed
only once. <br>
fadv-normal advise kernel there are no explicit access
pattern for the data. This is the default advice assumption.
<br>
fadv-rnd advise kernel to expect random access patterns for
the data. <br>
fadv-seq advise kernel to expect sequential access patterns
for the data. <br>
fadv-willneed advise kernel to expect the data to be
accessed in the near future. <br>
fsync flush all modified in-core data after each write to
the output device using an explicit fsync(2) call. <br>
fdatasync similar to fsync, but do not flush the modified
metadata unless metadata is required for later data reads to
be handled correctly. This uses an explicit <br>
fdatasync(2) call. <br>
iovec use readv/writev multiple buffer I/Os rather than
read/write. Instead of 1 read/write operation, the buffer is
broken into an iovec of 16 buffers. <br>
noatime do not update the file last access timestamp, this
can reduce metadata writes. <br>
sync ensure output has been transferred to underlying
hardware (using the O_SYNC open flag). This is equivalent to
a each write(2) being followed by a call to <br>
fsync(2). See also the fsync option. <br>
rd-rnd read data randomly. By default, written data is not
read back, however, this option will force it to be read
back randomly. <br>
rd-seq read data sequentially. By default, written data is
not read back, however, this option will force it to be read
back sequentially. <br>
syncfs write all buffered modifications of file metadata and
data on the filesystem that contains the hdd worker files.
<br>
utimes force update of file timestamp which may increase
metadata writes. <br>
wr-rnd write data randomly. The wr-seq option cannot be used
at the same time. <br>
wr-seq write data sequentially. This is the default if no
write modes are specified.</p>

<p style="margin-top: 1em">Note that some of these options
are mutually exclusive, for example, there can be only one
method of writing or reading. Also, fadvise flags may be
mutually exclusive, for exam&acirc; <br>
ple fadv-willneed cannot be used with fadv-dontneed.</p>

<p style="margin-top: 1em">--hdd-ops N <br>
stop hdd stress workers after N bogo operations.</p>

<p style="margin-top: 1em">--hdd-write-size N <br>
specify size of each write in bytes. Size can be from 1 byte
to 4MB.</p>

<p style="margin-top: 1em">--heapsort N <br>
start N workers that sort 32 bit integers using the BSD
heapsort.</p>

<p style="margin-top: 1em">--heapsort-ops N <br>
stop heapsort stress workers after N bogo heapsorts.</p>

<p style="margin-top: 1em">--heapsort-size N <br>
specify number of 32 bit integers to sort, default is 262144
(256 &Atilde; 1024).</p>

<p style="margin-top: 1em">--hsearch N <br>
start N workers that search a 80% full hash table using
hsearch(3). By default, there are 8192 elements inserted
into the hash table. This is a useful method to exercise
<br>
access of memory and processor cache.</p>

<p style="margin-top: 1em">--hsearch-ops N <br>
stop the hsearch workers after N bogo hsearch operations are
completed.</p>

<p style="margin-top: 1em">--hsearch-size N <br>
specify the number of hash entries to be inserted into the
hash table. Size can be from 1K to 4M.</p>

<p style="margin-top: 1em">--icache N <br>
start N workers that stress the instruction cache by forcing
instruction cache reloads. This is achieved by modifying an
instruction cache line, causing the processor to <br>
reload it when we call a function in inside it. Currently
only verified and enabled for Intel x86 CPUs.</p>

<p style="margin-top: 1em">--icache-ops N <br>
stop the icache workers after N bogo icache operations are
completed.</p>

<p style="margin-top: 1em">--icmp-flood N <br>
start N workers that flood localhost with randonly sized
ICMP ping packets. This option can only be run as root.</p>

<p style="margin-top: 1em">--icmp-flood-ops N <br>
stop icmp flood workers after N ICMP ping packets have been
sent.</p>

<p style="margin-top: 1em">--inotify N <br>
start N workers performing file system activities such as
making/deleting files/directories, moving files, etc. to
stress exercise the various inotify events (Linux only).</p>

<p style="margin-top: 1em">--inotify-ops N <br>
stop inotify stress workers after N inotify bogo
operations.</p>

<p style="margin-top: 1em">-i N, --io N <br>
start N workers continuously calling sync(2) to commit
buffer cache to disk. This can be used in conjunction with
the --hdd options.</p>

<p style="margin-top: 1em">--io-ops N <br>
stop io stress workers after N bogo operations.</p>

<p style="margin-top: 1em">--ioprio N <br>
start N workers that exercise the ioprio_get(2) and
ioprio_set(2) system calls (Linux only).</p>

<p style="margin-top: 1em">--ioprio-ops N <br>
stop after N io priority bogo operations.</p>

<p style="margin-top: 1em">--itimer N <br>
start N workers that exercise the system interval timers.
This sets up an ITIMER_PROF itimer that generates a SIGPROF
signal. The default frequency for the itimer is 1 <br>
MHz, however, the Linux kernel will set this to be no more
that the jiffy setting, hence high frequency SIGPROF signals
are not normally possible. A busy loop spins on <br>
getitimer(2) calls to consume CPU and hence decrement the
itimer based on amount of time spent in CPU and system
time.</p>

<p style="margin-top: 1em">--itimer-ops N <br>
stop itimer stress workers after N bogo itimer SIGPROF
signals.</p>

<p style="margin-top: 1em">--itimer-freq F <br>
run itimer at F Hz; range from 1 to 1000000 Hz. Normally the
highest frequency is limited by the number of jiffy ticks
per second, so running above 1000 Hz is difficult to <br>
attain in practice.</p>

<p style="margin-top: 1em">--kcmp N <br>
start N workers that use kcmp(2) to compare parent and child
processes to determine if they share kernel resources (Linux
only).</p>

<p style="margin-top: 1em">--kcmp-ops N <br>
stop kcmp workers after N bogo kcmp operations.</p>

<p style="margin-top: 1em">--key N <br>
start N workers that create and manipulate keys using
add_key(2) and ketctl(2). As many keys are created as the
per user limit allows and then the following keyctl
com&acirc; <br>
mands are exercised on each key: KEYCTL_SET_TIMEOUT,
KEYCTL_DESCRIBE, KEYCTL_UPDATE, KEYCTL_READ, KEYCTL_CLEAR
and KEYCTL_INVALIDATE.</p>

<p style="margin-top: 1em">--key-ops N <br>
stop key workers after N bogo key operations.</p>

<p style="margin-top: 1em">--kill N <br>
start N workers sending SIGUSR1 kill signals to a SIG_IGN
signal handler. Most of the process time will end up in
kernel space.</p>

<p style="margin-top: 1em">--kill-ops N <br>
stop kill workers after N bogo kill operations.</p>

<p style="margin-top: 1em">--klog N <br>
start N workers exercising the kernel syslog(2) system call.
This will attempt to read the kernel log with various sized
read buffers. Linux only.</p>

<p style="margin-top: 1em">--klog-ops N <br>
stop klog workers after N syslog operations.</p>

<p style="margin-top: 1em">--lease N <br>
start N workers locking, unlocking and breaking leases via
the fcntl(2) F_SETLEASE operation. The parent processes
continually lock and unlock a lease on a file while a <br>
user selectable number of child processes open the file with
a non-blocking open to generate SIGIO lease breaking
notifications to the parent. This stressor is only <br>
available if F_SETLEASE, F_WRLCK and F_UNLCK support is
provided by fcntl(2).</p>

<p style="margin-top: 1em">--lease-ops N <br>
stop lease workers after N bogo operations.</p>

<p style="margin-top: 1em">--lease-breakers N <br>
start N lease breaker child processes per lease worker.
Normally one child is plenty to force many SIGIO lease
breaking notification signals to the parent, however, this
<br>
option allows one to specify more child processes if
required.</p>

<p style="margin-top: 1em">--link N <br>
start N workers creating and removing hardlinks.</p>

<p style="margin-top: 1em">--link-ops N <br>
stop link stress workers after N bogo operations.</p>

<p style="margin-top: 1em">--lockbus N <br>
start N workers that rapidly lock and increment 64 bytes of
randomly chosen memory from a 16MB mmap&rsquo;d region
(Intel x86 CPUs only). This will cause cacheline misses and
<br>
stalling of CPUs.</p>

<p style="margin-top: 1em">--lockbus-ops N <br>
stop lockbus workers after N bogo operations.</p>

<p style="margin-top: 1em">--locka N <br>
start N workers that randomly lock and unlock regions of a
file using the POSIX advisory locking mechanism (see
fcntl(2), F_SETLK, F_GETLK). Each worker creates a 1024 KB
<br>
file and attempts to hold a maximum of 1024 concurrent locks
with a child process that also tries to hold 1024 concurrent
locks. Old locks are unlocked in a first-in, <br>
first-out basis.</p>

<p style="margin-top: 1em">--locka-ops N <br>
stop locka workers after N bogo locka operations.</p>

<p style="margin-top: 1em">--lockf N <br>
start N workers that randomly lock and unlock regions of a
file using the POSIX lockf(3) locking mechanism. Each worker
creates a 64 KB file and attempts to hold a maximum <br>
of 1024 concurrent locks with a child process that also
tries to hold 1024 concurrent locks. Old locks are unlocked
in a first-in, first-out basis.</p>

<p style="margin-top: 1em">--lockf-ops N <br>
stop lockf workers after N bogo lockf operations.</p>

<p style="margin-top: 1em">--lockf-nonblock <br>
instead of using blocking F_LOCK lockf(3) commands, use
non-blocking F_TLOCK commands and re-try if the lock failed.
This creates extra system call overhead and CPU util&acirc;
<br>
isation as the number of lockf workers increases and should
increase locking contention.</p>

<p style="margin-top: 1em">--lockofd N <br>
start N workers that randomly lock and unlock regions of a
file using the Linux open file description locks (see
fcntl(2), F_OFD_SETLK, F_OFD_GETLK). Each worker creates
<br>
a 1024 KB file and attempts to hold a maximum of 1024
concurrent locks with a child process that also tries to
hold 1024 concurrent locks. Old locks are unlocked in a <br>
first-in, first-out basis.</p>

<p style="margin-top: 1em">--lockofd-ops N <br>
stop lockofd workers after N bogo lockofd operations.</p>

<p style="margin-top: 1em">--longjmp N <br>
start N workers that exercise setjmp(3)/longjmp(3) by rapid
looping on longjmp calls.</p>

<p style="margin-top: 1em">--longjmp-ops N <br>
stop longjmp stress workers after N bogo longjmp operations
(1 bogo op is 1000 longjmp calls).</p>

<p style="margin-top: 1em">--lsearch N <br>
start N workers that linear search a unsorted array of 32
bit integers using lsearch(3). By default, there are 8192
elements in the array. This is a useful method to <br>
exercise sequential access of memory and processor
cache.</p>

<p style="margin-top: 1em">--lsearch-ops N <br>
stop the lsearch workers after N bogo lsearch operations are
completed.</p>

<p style="margin-top: 1em">--lsearch-size N <br>
specify the size (number of 32 bit integers) in the array to
lsearch. Size can be from 1K to 4M.</p>

<p style="margin-top: 1em">--madvise N <br>
start N workers that apply random madvise(2) advise settings
on pages of a 4MB file backed shared memory mapping.</p>

<p style="margin-top: 1em">--madvice-ops N <br>
stop madvise stressors after N bogo madvise operations.</p>

<p style="margin-top: 1em">--malloc N <br>
start N workers continuously calling malloc(3), calloc(3),
realloc(3) and free(3). By default, up to 65536 allocations
can be active at any point, but this can be altered <br>
with the --malloc-max option. Allocation, reallocation and
freeing are chosen at random; 50% of the time memory is
allocation (via malloc, calloc or realloc) and 50% of <br>
the time allocations are free&rsquo;d. Allocation sizes are
also random, with the maximum allocation size controlled by
the --malloc-bytes option, the default size being 64K. <br>
The worker is re-started if it is killed by the out of
mememory (OOM) killer.</p>

<p style="margin-top: 1em">--malloc-bytes N <br>
maximum per allocation/reallocation size. Allocations are
randomly selected from 1 to N bytes. One can specify the
size in units of Bytes, KBytes, MBytes and GBytes using <br>
the suffix b, k, m or g. Large allocation sizes cause the
memory allocator to use mmap(2) rather than expanding the
heap using brk(2).</p>

<p style="margin-top: 1em">--malloc-max N <br>
maximum number of active allocations allowed. Allocations
are chosen at ramdom and placed in an allocation slot.
Because about 50%/50% split between allocation and
free&acirc; <br>
ing, typically half of the allocation slots are in use at
any one time.</p>

<p style="margin-top: 1em">--malloc-ops N <br>
stop after N malloc bogo operations. One bogo operations
relates to a successful malloc(3), calloc(3) or
realloc(3).</p>

<p style="margin-top: 1em">--malloc-thresh N <br>
specify the threshold where malloc uses mmap(2) instead of
sbrk(2) to allocate more memory. This is only available on
systems that provide the GNU C mallopt(3) tuning <br>
function.</p>

<p style="margin-top: 1em">--matrix N <br>
start N workers that perform various matrix operations on
floating point values. By default, this will exercise all
the matrix stress methods one by one. One can specify <br>
a specific matrix stress method with the --matrix-method
option.</p>

<p style="margin-top: 1em">--matrix-ops N <br>
stop matrix stress workers after N bogo operations.</p>

<p style="margin-top: 1em">--matrix-method method <br>
specify a matrix stress method. Available matrix stress
methods are described as follows:</p>

<p style="margin-top: 1em">Method Description <br>
all iterate over all the below matrix stress methods <br>
add add two N &Atilde; N matrices <br>
copy copy one N &Atilde; N matrix to another <br>
div divide an N &Atilde; N matrix by a scalar <br>
hadamard Hadamard product of two N &Atilde; N matrices <br>
frobenius Frobenius product of two N &Atilde; N matrices
<br>
mean arithmetic mean of two N &Atilde; N matrices <br>
mult multiply an N &Atilde; N matrix by a scalar <br>
prod product of two N &Atilde; N matrices <br>
sub subtract one N &Atilde; N matrix from another N &Atilde;
N matrix <br>
trans transpose an N &Atilde; N matrix</p>

<p style="margin-top: 1em">--matrix-size N <br>
specify the N &Atilde; N size of the matrices. Smaller
values result in a floating point compute throughput bound
stressor, where as large values result in a cache and/or
memory <br>
bandwidth bound stressor.</p>

<p style="margin-top: 1em">--membarrier N <br>
start N workers that exercise the membarrier system call
(Linux only).</p>

<p style="margin-top: 1em">--membarrier-ops N <br>
stop membarrier stress workers after N bogo membarrier
operations.</p>

<p style="margin-top: 1em">--memcpy N <br>
start N workers that copy 2MB of data from a shared region
to a buffer using memcpy(3) and then move the data in the
buffer with memmove(3) with 3 different alignments. <br>
This will exercise processor cache and system memory.</p>

<p style="margin-top: 1em">--memcpy-ops N <br>
stop memcpy stress workers after N bogo memcpy
operations.</p>

<p style="margin-top: 1em">--memfd N <br>
start N workers that create 256 allocations of 1024 pages
using memfd_create(2) and ftruncate(2) for allocation and
mmap(2) to map the allocation into the process address <br>
space. (Linux only).</p>

<p style="margin-top: 1em">--memfd-bytes N <br>
allocate N bytes per memfd stress worker, the default is
256MB. One can specify the size in units of Bytes, KBytes,
MBytes and GBytes using the suffix b, k, m</p>

<p style="margin-top: 1em">--memfd-ops N <br>
stop after N memfd-create(2) bogo operations.</p>

<p style="margin-top: 1em">--mergesort N <br>
start N workers that sort 32 bit integers using the BSD
mergesort.</p>

<p style="margin-top: 1em">--mergesort-ops N <br>
stop mergesort stress workers after N bogo mergesorts.</p>

<p style="margin-top: 1em">--mergesort-size N <br>
specify number of 32 bit integers to sort, default is 262144
(256 &Atilde; 1024).</p>

<p style="margin-top: 1em">--mincore N <br>
start N workers that walk through all of memory 1 page at a
time checking of the page mapped and also is resident in
memory using mincore(2).</p>

<p style="margin-top: 1em">--mincore-ops N <br>
stop after N mincore bogo operations. One mincore bogo op is
equivalent to a 1000 mincore(2) calls.</p>

<p style="margin-top: 1em">--mincore-random <br>
instead of walking through pages sequentially, select pages
at random. The chosen address is iterated over by shifting
it right one place and checked by mincore until the <br>
address is less or equal to the page size.</p>

<p style="margin-top: 1em">--mknod N <br>
start N workers that create and remove fifos, empty files
and named sockets using mknod and unlink.</p>

<p style="margin-top: 1em">--mknod-ops N <br>
stop directory thrash workers after N bogo mknod
operations.</p>

<p style="margin-top: 1em">--mlock N <br>
start N workers that lock and unlock memory mapped pages
using mlock(2), munlock(2), mlockall(2) and munlockall(2).
This is achieved by the mapping of three contiguous <br>
pages and then locking the second page, hence ensuring
non-contiguous pages are locked . This is then repeated
until the maximum allowed mlocks or a maximum of 262144
map&acirc; <br>
pings are made. Next, all future mappings are mlocked and
the worker attempts to map 262144 pages, then all pages are
munlocked and the pages are unmapped.</p>

<p style="margin-top: 1em">--mlock-ops N <br>
stop after N mlock bogo operations.</p>

<p style="margin-top: 1em">--mmap N <br>
start N workers continuously calling mmap(2)/munmap(2). The
initial mapping is a large chunk (size specified by
--mmap-bytes) followed by pseudo-random 4K unmappings, <br>
then pseudo-random 4K mappings, and then linear 4K
unmappings. Note that this can cause systems to trip the
kernel OOM killer on Linux systems if not enough physical
mem&acirc; <br>
ory and swap is not available. The MAP_POPULATE option is
used to populate pages into memory on systems that support
this. By default, anonymous mappings are used, how&acirc;
<br>
ever, the --mmap-file and --mmap-async options allow one to
perform file based mappings if desired.</p>

<p style="margin-top: 1em">--mmap-ops N <br>
stop mmap stress workers after N bogo operations.</p>

<p style="margin-top: 1em">--mmap-async <br>
enable file based memory mapping and use asynchronous
msync&rsquo;ing on each page, see --mmap-file.</p>

<p style="margin-top: 1em">--mmap-bytes N <br>
allocate N bytes per mmap stress worker, the default is
256MB. One can specify the size in units of Bytes, KBytes,
MBytes and GBytes using the suffix b, k, m or g.</p>

<p style="margin-top: 1em">--mmap-file <br>
enable file based memory mapping and by default use
synchronous msync&rsquo;ing on each page.</p>

<p style="margin-top: 1em">--mmap-mprotect <br>
change protection settings on each page of memory. Each time
a page or a group of pages are mapped or remapped then this
option will make the pages read-only, write-only, <br>
exec-only, and read-write.</p>

<p style="margin-top: 1em">--mmapfork N <br>
start N workers that each fork off 32 child processes, each
of which tries to allocate some of the free memory left in
the system (and trying to avoid any swapping). The <br>
child processes then hint that the allocation will be needed
with madvise(2) and then memset it to zero and hint that it
is no longer needed with madvise before exiting. <br>
This produces significant amounts of VM activity, a lot of
cache misses and with minimal swapping.</p>

<p style="margin-top: 1em">--mmapfork-ops N <br>
stop after N mmapfork bogo operations.</p>

<p style="margin-top: 1em">--mmapmany N <br>
start N workers that attempt to create the maximum allowed
per-process memory mappings. This is achieved by mapping 3
contiguous pages and then unmapping the middle page <br>
hence splitting the mapping into two. This is then repeated
until the maximum allowed mappings or a maximum of 262144
mappings are made.</p>

<p style="margin-top: 1em">--mmapmany-ops N <br>
stop after N mmapmany bogo operations.</p>

<p style="margin-top: 1em">--mremap N <br>
start N workers continuously calling mmap(2), mremap(2) and
munmap(2). The initial anonymous mapping is a large chunk
(size specified by --mremap-bytes) and then itera&acirc;
<br>
tively halved in size by remapping all the way down to a
page size and then back up to the original size. This worker
is only available for Linux.</p>

<p style="margin-top: 1em">--mremap-ops N <br>
stop mremap stress workers after N bogo operations.</p>

<p style="margin-top: 1em">--mremap-bytes N <br>
initially allocate N bytes per remap stress worker, the
default is 256MB. One can specify the size in units of
Bytes, KBytes, MBytes and GBytes using the suffix b, k, m or
<br>
g.</p>

<p style="margin-top: 1em">--msg N <br>
start N sender and receiver processes that continually send
and receive messages using System V message IPC.</p>

<p style="margin-top: 1em">--msg-ops N <br>
stop after N bogo message send operations completed.</p>

<p style="margin-top: 1em">--msync N <br>
start N stressors that msync data from a file backed memory
mapping from memory back to the file and msync modified data
from the file back to the mapped memory. This <br>
exercises the msync(2) MS_SYNC and MS_INVALIDATE sync
operations.</p>

<p style="margin-top: 1em">--msync-ops N <br>
stop after N msync bogo operations completed.</p>

<p style="margin-top: 1em">--msync-bytes N <br>
allocate N bytes for the memory mapped file, the default is
256MB. One can specify the size in units of Bytes, KBytes,
MBytes and GBytes using the</p>

<p style="margin-top: 1em">--mq N start N sender and
receiver processes that continually send and receive
messages using POSIX message queues. (Linux only).</p>

<p style="margin-top: 1em">--mq-ops N <br>
stop after N bogo POSIX message send operations
completed.</p>

<p style="margin-top: 1em">--mq-size N <br>
specify size of POSIX message queue. The default size is 10
messages and most Linux systems this is the maximum allowed
size for normal users. If the given size is greater <br>
than the allowed message queue size then a warning is issued
and the maximum allowed size is used instead.</p>

<p style="margin-top: 1em">--nice N <br>
start N cpu consuming workers that exercise the available
nice levels. Each iteration forks off a child process that
runs through the all the nice levels running a busy <br>
loop for 0.1 seconds per level and then exits.</p>

<p style="margin-top: 1em">--nice-ops N <br>
stop after N nice bogo nice loops</p>

<p style="margin-top: 1em">--null N <br>
start N workers writing to /dev/null.</p>

<p style="margin-top: 1em">--null-ops N <br>
stop null stress workers after N /dev/null bogo write
operations.</p>

<p style="margin-top: 1em">--numa N <br>
start N workers that migrate stressors and a 4MB memory
mapped buffer around all the available NUMA nodes. This uses
migrate_pages(2) to move the stressors and mbind(2) <br>
and move_pages(2) to move the pages of the mapped buffer.
After each move, the buffer is written to force activity
over the bus which results cache misses. This test will <br>
only run on hardware with NUMA enabled and more than 1 NUMA
node.</p>

<p style="margin-top: 1em">--numa-ops N <br>
stop NUMA stress workers after N bogo NUMA operations.</p>

<p style="margin-top: 1em">--oom-pipe N <br>
start N workers that create as many pipes as allowed and
exercise expanding and shrinking the pipes from the largest
pipe size down to a page size. Data is written into <br>
the pipes and read out again to fill the pipe buffers. With
the --aggressive mode enabled the data is not read out when
the pipes are shrunk, causing the kernel to OOM <br>
processes aggressively. Running many instances of this
stressor will force kernel to OOM processes due to the many
large pipe buffer allocations.</p>

<p style="margin-top: 1em">--oom-pipe-ops N <br>
stop after N bogo pipe expand/shrink operations.</p>

<p style="margin-top: 1em">--opcode N <br>
start N workers that fork off children that execute randomly
generated executable code. This will generate issues such as
illegal instructions, bus errors, segmentation <br>
faults, traps, floating point errors that are handled
gracefully by the stressor.</p>

<p style="margin-top: 1em">--opcode-ops N <br>
stop after N attempts to executate illegal code.</p>

<p style="margin-top: 1em">-o N, --open N <br>
start N workers that perform open(2) and then close(2)
operations on /dev/zero. The maximum opens at one time is
system defined, so the test will run up to this maximum,
<br>
or 65536 open file descriptors, which ever comes first.</p>

<p style="margin-top: 1em">--open-ops N <br>
stop the open stress workers after N bogo open
operations.</p>

<p style="margin-top: 1em">--personality N <br>
start N workers that attempt to set personality and get all
the available personality types (process execution domain
types) via the personality(2) system call. (Linux <br>
only).</p>

<p style="margin-top: 1em">--personality-ops N <br>
stop personality stress workers after N bogo personality
operations.</p>

<p style="margin-top: 1em">-p N, --pipe N <br>
start N workers that perform large pipe writes and reads to
exercise pipe I/O. This exercises memory write and reads as
well as context switching. Each worker has two <br>
processes, a reader and a writer.</p>

<p style="margin-top: 1em">--pipe-ops N <br>
stop pipe stress workers after N bogo pipe write
operations.</p>

<p style="margin-top: 1em">--pipe-data-size N <br>
specifies the size in bytes of each write to the pipe (range
from 4 bytes to 4096 bytes). Setting a small data size will
cause more writes to be buffered in the pipe, <br>
hence reducing the context switch rate between the pipe
writer and pipe reader processes. Default size is the page
size.</p>

<p style="margin-top: 1em">--pipe-size N <br>
specifies the size of the pipe in bytes (for systems that
support the F_SETPIPE_SZ fcntl() command). Setting a small
pipe size will cause the pipe to fill and block more <br>
frequently, hence increasing the context switch rate between
the pipe writer and the pipe reader processes. Default size
is 512 bytes.</p>

<p style="margin-top: 1em">-P N, --poll N <br>
start N workers that perform zero timeout polling via the
poll(2), select(2) and sleep(3) calls. This wastes system
and user time doing nothing.</p>

<p style="margin-top: 1em">--poll-ops N <br>
stop poll stress workers after N bogo poll operations.</p>

<p style="margin-top: 1em">--procfs N <br>
start N workers that read files from /proc and recursively
read files from /proc/self (Linux only).</p>

<p style="margin-top: 1em">--procfs-ops N <br>
stop procfs reading after N bogo read operations. Note,
since the number of entries may vary between kernels, this
bogo ops metric is probably very misleading.</p>

<p style="margin-top: 1em">--pthread N <br>
start N workers that iteratively creates and terminates
multiple pthreads (the default is 1024 pthreads per worker).
In each iteration, each newly created pthread waits <br>
until the worker has created all the pthreads and then they
all terminate together.</p>

<p style="margin-top: 1em">--pthread-ops N <br>
stop pthread workers after N bogo pthread create
operations.</p>

<p style="margin-top: 1em">--pthread-max N <br>
create N pthreads per worker. If the product of the number
of pthreads by the number of workers is greater than the
soft limit of allowed pthreads then the maximum is re- <br>
adjusted down to the maximum allowed.</p>

<p style="margin-top: 1em">--ptrace N <br>
start N workers that fork and trace system calls of a child
process using ptrace(2).</p>

<p style="margin-top: 1em">--ptrace-ops N <br>
stop ptracer workers after N bogo system calls are
traced.</p>

<p style="margin-top: 1em">--pty N <br>
start N workers that repeatedly attempt to open 65536
pseudoterminals and perform various pty ioctls upon the ptys
before closing them.</p>

<p style="margin-top: 1em">--pty-ops N <br>
stop pty workers after N pty bogo operations.</p>

<p style="margin-top: 1em">-Q, --qsort N <br>
start N workers that sort 32 bit integers using qsort.</p>

<p style="margin-top: 1em">--qsort-ops N <br>
stop qsort stress workers after N bogo qsorts.</p>

<p style="margin-top: 1em">--qsort-size N <br>
specify number of 32 bit integers to sort, default is 262144
(256 &Atilde; 1024).</p>

<p style="margin-top: 1em">--quota N <br>
start N workers that exercise the Q_GETQUOTA, Q_GETFMT,
Q_GETINFO, Q_GETSTATS and Q_SYNC quotactl(2) commands on all
the available mounted block based file systems.</p>

<p style="margin-top: 1em">--quota-ops N <br>
stop quota stress workers after N bogo quotactl
operations.</p>

<p style="margin-top: 1em">--rdrand N <br>
start N workers that read the Intel hardware random number
generator (Intel Ivybridge processors upwards).</p>

<p style="margin-top: 1em">--rdrand-ops N <br>
stop rdrand stress workers after N bogo rdrand operations (1
bogo op = 2048 random bits successfully read).</p>

<p style="margin-top: 1em">--readahead N <br>
start N workers that randomly seeks and performs 512 byte
read/write I/O operations on a file with readahead. The
default file size is 1 GB. Readaheads and reads are <br>
batched into 16 readaheads and then 16 reads.</p>

<p style="margin-top: 1em">--readahead-bytes N <br>
set the size of readahead file, the default is 1 GB. One can
specify the size in units of Bytes, KBytes, MBytes and
GBytes using the suffix b, k, m or g.</p>

<p style="margin-top: 1em">--readahead-ops N <br>
stop readahead stress workers after N bogo read
operations.</p>

<p style="margin-top: 1em">--remap N <br>
start N workers that map 512 pages and re-order these pages
using the deprecated system call remap_file_pages(2).
Several page re-orderings are exercised: forward, <br>
reverse, random and many pages to 1 page.</p>

<p style="margin-top: 1em">--remap-ops N <br>
stop after N remapping bogo operations.</p>

<p style="margin-top: 1em">-R N, --rename N <br>
start N workers that each create a file and then repeatedly
rename it.</p>

<p style="margin-top: 1em">--rename-ops N <br>
stop rename stress workers after N bogo rename
operations.</p>

<p style="margin-top: 1em">--resources N <br>
start N workers that consume various system resources. Each
worker will spawn 1024 child processes that iterate 1024
times consuming shared memory, heap, stack, temporary <br>
files and various file descriptors (eventfds, memoryfds,
<br>
userfaultfds, pipes and sockets).</p>

<p style="margin-top: 1em">--resources-ops N <br>
stop after N resource child forks.</p>

<p style="margin-top: 1em">--rlimit N <br>
start N workers that exceed CPU and file size resource
imits, generating SIGXCPU and SIGXFSZ signals.</p>

<p style="margin-top: 1em">--rlimit-ops N <br>
stop after N bogo resource limited SIGXCPU and SIGXFSZ
signals have been caught.</p>

<p style="margin-top: 1em">--rmap N <br>
start N workers that exercise the VM reverse-mapping. This
creates 16 processes per worker that write/read multiple
file-backed memory mappings. There are 64 lots of 4 <br>
page mappings made onto the file, with each mapping
overlapping the previous by 3 pages and at least 1 page of
non-mapped memory between each of the mappings. Data is
syn&acirc; <br>
chronously msync&rsquo;d to the file 1 in every 256
iterations in a random manner.</p>

<p style="margin-top: 1em">--rmap-ops N <br>
stop after N bogo rmap memory writes/reads.</p>

<p style="margin-top: 1em">--rtc N <br>
start N workers that exercise the real time clock (RTC)
interfaces via /dev/rtc and /sys/class/rtc/rtc0. No
destructive writes (modifications) are performed on the RTC.
<br>
This is a Linux only stressor.</p>

<p style="margin-top: 1em">--rtc-ops N <br>
stop after N bogo RTC interface accesses.</p>

<p style="margin-top: 1em">--sctp N <br>
start N workers that perform network sctp stress activity
using the Stream Control Transmission Protocol (SCTP). This
involves client/server processes performing rapid <br>
connect, send/receives and disconnects on the local
host.</p>

<p style="margin-top: 1em">--sctp-domain D <br>
specify the domain to use, the default is ipv4. Currently
ipv4 and ipv6 are supported.</p>

<p style="margin-top: 1em">--sctp-ops N <br>
stop sctp workers after N bogo operations.</p>

<p style="margin-top: 1em">--sctp-port P <br>
start at sctp port P. For N sctp worker processes, ports P
to (P * 4) - 1 are used for ipv4, ipv6 domains and ports P
to P - 1 are used for the unix domain.</p>

<p style="margin-top: 1em">--seal N <br>
start N workers that exercise the fcntl(2) SEAL commands on
a small anonymous file created using memfd_create(2). After
each SEAL command is issued the stessor also san&acirc; <br>
ity checks if the seal operation has sealed the file
correctly. (Linux only).</p>

<p style="margin-top: 1em">--seal-ops N <br>
stop after N bogo seal operations.</p>

<p style="margin-top: 1em">--seccomp N <br>
start N workers that exercise Secure Computing system call
filtering. Each worker creates child processes that write a
short message to /dev/null and then exits. 2% of the <br>
child processes have a seccomp filter that disallows the
write system call and hence it is killed by seccomp with a
SIGSYS. Note that this stressor can generate many <br>
audit log messages each time the child is killed.</p>

<p style="margin-top: 1em">--seccomp-ops N <br>
stop seccomp stress workers after N seccomp filter
tests.</p>

<p style="margin-top: 1em">--seek N <br>
start N workers that randomly seeks and performs 512 byte
read/write I/O operations on a file. The default file size
is 16 GB.</p>

<p style="margin-top: 1em">--seek-ops N <br>
stop seek stress workers after N bogo seek operations.</p>

<p style="margin-top: 1em">--seek-punch <br>
punch randomly located 8K holes into the file to cause more
extents to force a more demanding seek stressor, (Linux
only).</p>

<p style="margin-top: 1em">--seek-size N <br>
specify the size of the file in bytes. Small file sizes
allow the I/O to occur in the cache, causing greater CPU
load. Large file sizes force more I/O operations to drive
<br>
causing more wait time and more I/O on the drive. One can
specify the size in units of Bytes, KBytes, MBytes and
GBytes using the suffix b, k, m or g.</p>

<p style="margin-top: 1em">--sem N <br>
start N workers that perform POSIX semaphore wait and post
operations. By default, a parent and 4 children are started
per worker to provide some contention on the sema&acirc;
<br>
phore. This stresses fast semaphore operations and produces
rapid context switching.</p>

<p style="margin-top: 1em">--sem-ops N <br>
stop semaphore stress workers after N bogo semaphore
operations.</p>

<p style="margin-top: 1em">--sem-procs N <br>
start N child workers per worker to provide contention on
the semaphore, the default is 4 and a maximum of 64 are
allowed.</p>

<p style="margin-top: 1em">--sem-sysv N <br>
start N workers that perform System V semaphore wait and
post operations. By default, a parent and 4 children are
started per worker to provide some contention on the
sem&acirc; <br>
aphore. This stresses fast semaphore operations and produces
rapid context switching.</p>

<p style="margin-top: 1em">--sem-sysv-ops N <br>
stop semaphore stress workers after N bogo System V
semaphore operations.</p>

<p style="margin-top: 1em">--sem-sysv-procs N <br>
start N child processes per worker to provide contention on
the System V semaphore, the default is 4 and a maximum of 64
are allowed.</p>

<p style="margin-top: 1em">--sendfile N <br>
start N workers that send an empty file to /dev/null. This
operation spends nearly all the time in the kernel. The
default sendfile size is 4MB. The sendfile options are <br>
for Linux only.</p>

<p style="margin-top: 1em">--sendfile-ops N <br>
stop sendfile workers after N sendfile bogo operations.</p>

<p style="margin-top: 1em">--sendfile-size S <br>
specify the size to be copied with each sendfile call. The
default size is 4MB. One can specify the size in units of
Bytes, KBytes, MBytes and GBytes using the suffix b, <br>
k, m or g.</p>

<p style="margin-top: 1em">--shm N <br>
start N workers that open and allocate shared memory objects
using the POSIX shared memory interfaces. By default, the
test will repeatedly create and destroy 32 shared <br>
memory objects, each of which is 8MB in size.</p>

<p style="margin-top: 1em">--shm-ops N <br>
stop after N POSIX shared memory create and destroy bogo
operations are complete.</p>

<p style="margin-top: 1em">--shm-bytes N <br>
specify the size of the POSIX shared memory objects to be
created. One can specify the size in units of Bytes, KBytes,
MBytes and GBytes using the suffix b, k, m or g.</p>

<p style="margin-top: 1em">--shm-objs N <br>
specify the number of shared memory objects to be
created.</p>

<p style="margin-top: 1em">--shm-sysv N <br>
start N workers that allocate shared memory using the System
V shared memory interface. By default, the test will
repeatedly create and destroy 8 shared memory segments, <br>
each of which is 8MB in size.</p>

<p style="margin-top: 1em">--shm-sysv-ops N <br>
stop after N shared memory create and destroy bogo
operations are complete.</p>

<p style="margin-top: 1em">--shm-sysv-bytes N <br>
specify the size of the shared memory segment to be created.
One can specify the size in units of Bytes, KBytes, MBytes
and GBytes using the suffix b, k, m or g.</p>

<p style="margin-top: 1em">--shm-sysv-segs N <br>
specify the number of shared memory segments to be
created.</p>

<p style="margin-top: 1em">--sigfd N <br>
start N workers that generate SIGRT signals and are handled
by reads by a child process using a file descriptor set up
using signalfd(2). (Linux only). This will generate <br>
a heavy context switch load when all CPUs are fully
loaded.</p>

<p style="margin-top: 1em">--sigfd-ops <br>
stop sigfd workers after N bogo SIGUSR1 signals are
sent.</p>

<p style="margin-top: 1em">--sigfpe N <br>
start N workers that rapidly cause division by zero SIGFPE
faults.</p>

<p style="margin-top: 1em">--sigfpe-ops N <br>
stop sigfpe stress workers after N bogo SIGFPE faults.</p>

<p style="margin-top: 1em">--sigpending N <br>
start N workers that check if SIGUSR1 signals are pending.
This stressor masks SIGUSR1, generates a SIGUSR1 signal and
uses sigpending(2) to see if the signal is pending. <br>
Then it unmasks the signal and checks if the signal is no
longer pending.</p>

<p style="margin-top: 1em">--signpending-ops N <br>
stop sigpending stress workers after N bogo sigpending
pending/unpending checks.</p>

<p style="margin-top: 1em">--sigsegv N <br>
start N workers that rapidly create and catch segmentation
faults.</p>

<p style="margin-top: 1em">--sigsegv-ops N <br>
stop sigsegv stress workers after N bogo segmentation
faults.</p>

<p style="margin-top: 1em">--sigsuspend N <br>
start N workers that each spawn off 4 child processes that
wait for a SIGUSR1 signal from the parent using
sigsuspend(2). The parent sends SIGUSR1 signals to each
child in <br>
rapid succession. Each sigsuspend wakeup is counted as one
bogo operation.</p>

<p style="margin-top: 1em">--sigsuspend-ops N <br>
stop sigsuspend stress workers after N bogo sigsuspend
wakeups.</p>

<p style="margin-top: 1em">--sigq N <br>
start N workers that rapidly send SIGUSR1 signals using
sigqueue(3) to child processes that wait for the signal via
sigwaitinfo(2).</p>

<p style="margin-top: 1em">--sigq-ops N <br>
stop sigq stress workers after N bogo signal send
operations.</p>

<p style="margin-top: 1em">--sleep N <br>
start N workers that spawn off multiple threads that each
perform multiple sleeps of ranges 1us to 0.1s. This creates
multiple context switches and timer interrupts.</p>

<p style="margin-top: 1em">--sleep-ops N <br>
stop after N sleep bogo operations.</p>

<p style="margin-top: 1em">--sleep-max P <br>
start P threads per worker. The default is 1024, the maximum
allowed is 30000.</p>

<p style="margin-top: 1em">-S N, --sock N <br>
start N workers that perform various socket stress activity.
This involves a pair of client/server processes performing
rapid connect, send and receives and disconnects on <br>
the local host.</p>

<p style="margin-top: 1em">--sock-domain D <br>
specify the domain to use, the default is ipv4. Currently
ipv4, ipv6 and unix are supported.</p>

<p style="margin-top: 1em">--sock-nodelay <br>
This disables the TCP Nagle algorithm, so data segments are
always sent as soon as possible. This stops data from being
buffered before being transmitted, hence resulting <br>
in poorer network utilisation and more context switches
between the sender and receiver.</p>

<p style="margin-top: 1em">--sock-port P <br>
start at socket port P. For N socket worker processes, ports
P to P - 1 are used.</p>

<p style="margin-top: 1em">--sock-ops N <br>
stop socket stress workers after N bogo operations.</p>

<p style="margin-top: 1em">--sock-opts [ send | sendmsg |
sendmmsg ] <br>
by default, messages are sent using send(2). This option
allows one to specify the sending method using send(2),
sendmsg(2) or sendmmsg(2). Note that sendmmsg is only <br>
available for Linux systems that support this system
call.</p>

<p style="margin-top: 1em">--sock-type [ stream | seqpacket
] <br>
specify the socket type to use. The default type is stream.
seqpacket currently only works for the unix socket
domain.</p>

<p style="margin-top: 1em">--sockfd N <br>
start N workers that pass file descriptors over a UNIX
domain socket using the CMSG(3) ancillary data mechanism.
For each worker, pair of client/server processes are
cre&acirc; <br>
ated, the server opens as many file descriptors on /dev/null
as possible and passing these over the socket to a client
that reads these from the CMSG data and immediately <br>
closes the files.</p>

<p style="margin-top: 1em">--sockfd-ops N <br>
stop sockfd stress workers after N bogo operations.</p>

<p style="margin-top: 1em">--sockfd-port P <br>
start at socket port P. For N socket worker processes, ports
P to P - 1 are used.</p>

<p style="margin-top: 1em">--sockpair N <br>
start N workers that perform socket pair I/O read/writes.
This involves a pair of client/server processes performing
randomly sized socket I/O operations.</p>

<p style="margin-top: 1em">--sockpair-ops N <br>
stop socket pair stress workers after N bogo operations.</p>

<p style="margin-top: 1em">--spawn N <br>
start N workers continually spawn children using
posix_spawn(3) that exec stress-ng and then exit almost
immediately. Currently Linux only.</p>

<p style="margin-top: 1em">--spawn-ops N <br>
stop spawn stress workers after N bogo spawns.</p>

<p style="margin-top: 1em">--splice N <br>
move data from /dev/zero to /dev/null through a pipe without
any copying between kernel address space and user address
space using splice(2). This is only available for <br>
Linux.</p>

<p style="margin-top: 1em">--splice-ops N <br>
stop after N bogo splice operations.</p>

<p style="margin-top: 1em">--splice-bytes N <br>
transfer N bytes per splice call, the default is 64K. One
can specify the size in units of Bytes, KBytes, MBytes and
GBytes using the suffix b, k, m or g.</p>

<p style="margin-top: 1em">--stack N <br>
start N workers that rapidly cause and catch stack overflows
by use of alloca(3).</p>

<p style="margin-top: 1em">--stack-fill <br>
the default action is to touch the lowest page on each stack
allocation. This option touches all the pages by filling the
new stack allocation with zeros which forces <br>
physical pages to be allocated and hence is more
aggressive.</p>

<p style="margin-top: 1em">--stack-ops N <br>
stop stack stress workers after N bogo stack overflows.</p>

<p style="margin-top: 1em">--stackmmap N <br>
start N workers that use a 2MB stack that is memory mapped
onto a temporary file. A recursive function works down the
stack and flushes dirty stack pages back to the mem&acirc;
<br>
ory mapped file using msync(2) until the end of the stack is
reached (stack overflow). This exercises dirty page and
stack exception handling.</p>

<p style="margin-top: 1em">--stackmmap-ops N <br>
stop workers after N stack overflows have occurred.</p>

<p style="margin-top: 1em">--str N <br>
start N workers that exercise various libc string functions
on random strings.</p>

<p style="margin-top: 1em">--str-method strfunc <br>
select a specific libc string function to stress. Available
string functions to stress are: all, index, rindex,
strcasecmp, strcat, strchr, strcoll, strcmp, strcpy, <br>
strlen, strncasecmp, strncat, strncmp, strrchr and strxfrm.
See string(3) for more information on these string
functions. The &rsquo;all&rsquo; method is the default and
will exer&acirc; <br>
cise all the string methods.</p>

<p style="margin-top: 1em">--str-ops N <br>
stop after N bogo string operations.</p>

<p style="margin-top: 1em">--stream N <br>
start N workers exercising a memory bandwidth stressor
loosely based on the STREAM &quot;Sustainable Memory
Bandwidth in High Performance Computers&quot; benchmarking
tool by John <br>
D. McCalpin, Ph.D. This stressor allocates buffers that are
at least 4 times the size of the CPU L2 cache and
continually performs rounds of following computations on
<br>
large arrays of double precision floating point numbers:</p>

<p style="margin-top: 1em">Operation Description <br>
copy c[i] = a[i] <br>
scale b[i] = scalar * c[i] <br>
add c[i] = a[i] + b[i] <br>
triad a[i] = b[i] + (c[i] * scalar)</p>

<p style="margin-top: 1em">Since this is loosely based on a
variant of the STREAM benchmark code, DO NOT submit results
based on this as it is intended to in stress-ng just to
stress memory and com&acirc; <br>
pute and NOT intended for STREAM accurate tuned or non-tuned
benchmarking whatsoever. Use the official STREAM
benchmarking tool if you desire accurate and standardised
<br>
STREAM benchmarks.</p>

<p style="margin-top: 1em">--stream-ops N <br>
stop after N stream bogo operations, where a bogo operation
is one round of copy, scale, add and triad operations.</p>

<p style="margin-top: 1em">--stream-l3-size N <br>
Specify the CPU Level 3 cache size in bytes. One can specify
the size in units of Bytes, KBytes, MBytes and GBytes using
the suffix b, k, m or g. If the L3 cache size is <br>
not provided, then stress-ng will attempt to determine the
cache size, and failing this, will default the size to
4MB.</p>

<p style="margin-top: 1em">-s N, --switch N <br>
start N workers that send messages via pipe to a child to
force context switching.</p>

<p style="margin-top: 1em">--switch-ops N <br>
stop context switching workers after N bogo operations.</p>

<p style="margin-top: 1em">--symlink N <br>
start N workers creating and removing symbolic links.</p>

<p style="margin-top: 1em">--symlink-ops N <br>
stop symlink stress workers after N bogo operations.</p>

<p style="margin-top: 1em">--sync-file N <br>
start N workers that perform a range of data syncs across a
file using sync_file_range(2). Three mixes of syncs are
performed, from start to the end of the file, from <br>
end of the file to the start, and a random mix. A random
selection of valid sync types are used, covering the
SYNC_FILE_RANGE_WAIT_BEFORE, SYNC_FILE_RANGE_WRITE and <br>
SYNC_FILE_RANGE_WAIT_AFTER flag bits.</p>

<p style="margin-top: 1em">--sync-file-ops N <br>
stop sync-file workers after N bogo sync operations.</p>

<p style="margin-top: 1em">--sync-file-bytes N <br>
specify the size of the file to be sync&rsquo;d. One can
specify the size in units of Bytes, KBytes, MBytes and
GBytes using the suffix b, k, m or g.</p>

<p style="margin-top: 1em">--sysinfo N <br>
start N workers that continually read system and process
specific information. This reads the process user and system
times using the times(2) system call. For Linux <br>
systems, it also reads overall system statistics using the
sysinfo(2) system call and also the file system statistics
for all mounted file systems using statfs(2).</p>

<p style="margin-top: 1em">--sysinfo-ops N <br>
stop the sysinfo workers after N bogo operations.</p>

<p style="margin-top: 1em">--sysfs N <br>
start N workers that recursively read files from /sys (Linux
only). This may cause specific kernel drivers to emit
messages into the kernel log.</p>

<p style="margin-top: 1em">--sys-ops N <br>
stop sysfs reading after N bogo read operations. Note, since
the number of entries may vary between kernels, this bogo
ops metric is probably very misleading.</p>

<p style="margin-top: 1em">--tee N <br>
move data from a writer process to a reader process through
pipes and to /dev/null without any copying between kernel
address space and user address space using tee(2). <br>
This is only available for Linux.</p>

<p style="margin-top: 1em">--tee-ops N <br>
stop after N bogo tee operations.</p>

<p style="margin-top: 1em">-T N, --timer N <br>
start N workers creating timer events at a default rate of 1
MHz (Linux only); this can create a many thousands of timer
clock interrupts. Each timer event is caught by a <br>
signal handler and counted as a bogo timer op.</p>

<p style="margin-top: 1em">--timer-ops N <br>
stop timer stress workers after N bogo timer events (Linux
only).</p>

<p style="margin-top: 1em">--timer-freq F <br>
run timers at F Hz; range from 1 to 1000000000 Hz (Linux
only). By selecting an appropriate frequency stress-ng can
generate hundreds of thousands of interrupts per sec&acirc;
<br>
ond.</p>

<p style="margin-top: 1em">--timer-rand <br>
select a timer frequency based around the timer frequency
+/- 12.5% random jitter. This tries to force more
variability in the timer interval to make the scheduling
less <br>
predictable.</p>

<p style="margin-top: 1em">--timerfd N <br>
start N workers creating timerfd events at a default rate of
1 MHz (Linux only); this can create a many thousands of
timer clock events. Timer events are waited for on the <br>
timer file descriptor using select(2) and then read and
counted as a bogo timerfd op.</p>

<p style="margin-top: 1em">--timerfd-ops N <br>
stop timerfd stress workers after N bogo timerfd events
(Linux only).</p>

<p style="margin-top: 1em">--timerfd-freq F <br>
run timers at F Hz; range from 1 to 1000000000 Hz (Linux
only). By selecting an appropriate frequency stress-ng can
generate hundreds of thousands of interrupts per sec&acirc;
<br>
ond.</p>

<p style="margin-top: 1em">--timerfd-rand <br>
select a timerfd frequency based around the timer frequency
+/- 12.5% random jitter. This tries to force more
variability in the timer interval to make the scheduling
less <br>
predictable.</p>

<p style="margin-top: 1em">--tlb-shootdown N <br>
start N workers that force Translation Lookaside Buffer
(TLB) shootdowns. This is achieved by creating upto 16 child
processes that all share a region of memory and these <br>
processes are shared amongst the available CPUs. The
processes adjust the page mapping settings causing TLBs to
be force flushed on the other processors, causing the TLB
<br>
shootdowns.</p>

<p style="margin-top: 1em">--tlb-shootdown-ops N <br>
stop after N bogo TLB shootdown operations are
completed.</p>

<p style="margin-top: 1em">--tsc N <br>
start N workers that read the Time Stamp Counter (TSC) 256
times per loop iteration (bogo operation). Available only on
Intel x86 platforms.</p>

<p style="margin-top: 1em">--tsc-ops N <br>
stop the tsc workers after N bogo operations are
completed.</p>

<p style="margin-top: 1em">--tsearch N <br>
start N workers that insert, search and delete 32 bit
integers on a binary tree using tsearch(3), tfind(3) and
tdelete(3). By default, there are 65536 randomized integers
<br>
used in the tree. This is a useful method to exercise random
access of memory and processor cache.</p>

<p style="margin-top: 1em">--tsearch-ops N <br>
stop the tsearch workers after N bogo tree operations are
completed.</p>

<p style="margin-top: 1em">--tsearch-size N <br>
specify the size (number of 32 bit integers) in the array to
tsearch. Size can be from 1K to 4M.</p>

<p style="margin-top: 1em">--udp N <br>
start N workers that transmit data using UDP. This involves
a pair of client/server processes performing rapid connect,
send and receives and disconnects on the local <br>
host.</p>

<p style="margin-top: 1em">--udp-domain D <br>
specify the domain to use, the default is ipv4. Currently
ipv4, ipv6 and unix are supported.</p>

<p style="margin-top: 1em">--udp-lite <br>
use the UDP-Lite (RFC 3828) protocol (only for ipv4 and ipv4
domains).</p>

<p style="margin-top: 1em">--udp-ops N <br>
stop udp stress workers after N bogo operations.</p>

<p style="margin-top: 1em">--udp-port P <br>
start at port P. For N udp worker processes, ports P to P -
1 are used. By default, ports 7000 upwards are used.</p>

<p style="margin-top: 1em">--udp-flood N <br>
start N workers that attempt to flood the host with UDP
packets to random ports. The IP address of the packets are
currently not spoofed. This is only available on systems
<br>
that support AF_PACKET.</p>

<p style="margin-top: 1em">--udp-flood-domain D <br>
specify the domain to use, the default is ipv4. Currently
ipv4 and ipv6 are supported.</p>

<p style="margin-top: 1em">--udp-flood-ops N <br>
stop udp-flood stress workers after N bogo operations.</p>

<p style="margin-top: 1em">--unshare N <br>
start N workers that each fork off 32 child processes, each
of which exercises the unshare(2) system call by
disassociating parts of the process execution context.
(Linux <br>
only).</p>

<p style="margin-top: 1em">--unshare-ops N <br>
stop after N bogo unshare operations.</p>

<p style="margin-top: 1em">-u N, --urandom N <br>
start N workers reading /dev/urandom (Linux only). This will
load the kernel random number source.</p>

<p style="margin-top: 1em">--urandom-ops N <br>
stop urandom stress workers after N urandom bogo read
operations (Linux only).</p>

<p style="margin-top: 1em">--userfaultfd N <br>
start N workers that generate write page faults on a small
anonymously mapped memory region and handle these faults
using the user space fault handling via the userfaultfd <br>
mechanism. This will generate a large quanity of major page
faults and also context switches during the handling of the
page faults. (Linux only).</p>

<p style="margin-top: 1em">--userfaultfd-ops N <br>
stop userfaultfd stress workers after N page faults.</p>

<p style="margin-top: 1em">--userfaultfd-bytes N <br>
mmap N bytes per userfaultfd worker to page fault on, the
default is 16MB One can specify the size in units of Bytes,
KBytes, MBytes and GBytes using the suffix b, k, m or <br>
g.</p>

<p style="margin-top: 1em">--utime N <br>
start N workers updating file timestamps. This is mainly CPU
bound when the default is used as the system flushes
metadata changes only periodically.</p>

<p style="margin-top: 1em">--utime-ops N <br>
stop utime stress workers after N utime bogo operations.</p>

<p style="margin-top: 1em">--utime-fsync <br>
force metadata changes on each file timestamp update to be
flushed to disk. This forces the test to become I/O bound
and will result in many dirty metadata writes.</p>

<p style="margin-top: 1em">--vecmath N <br>
start N workers that perform various unsigned integer math
operations on various 128 bit vectors. A mix of vector math
operations are performed on the following vectors: <br>
16 &Atilde; 8 bits, 8 &Atilde; 16 bits, 4 &Atilde; 32 bits,
2 &Atilde; 64 bits. The metrics produced by this mix depend
on the processor architecture and the vector math
optimisations produced by the <br>
compiler.</p>

<p style="margin-top: 1em">--vecmath-ops N <br>
stop after N bogo vector integer math operations.</p>

<p style="margin-top: 1em">--vfork N <br>
start N workers continually vforking children that
immediately exit.</p>

<p style="margin-top: 1em">--vfork-ops N <br>
stop vfork stress workers after N bogo operations.</p>

<p style="margin-top: 1em">--vfork-max P <br>
create P processes and then wait for them to exit per
iteration. The default is just 1; higher values will create
many temporary zombie processes that are waiting to be <br>
reaped. One can potentially fill up the the process table
using high values for --vfork-max and --vfork.</p>

<p style="margin-top: 1em">-m N, --vm N <br>
start N workers continuously calling mmap(2)/munmap(2) and
writing to the allocated memory. Note that this can cause
systems to trip the kernel OOM killer on Linux systems <br>
if not enough physical memory and swap is not available.</p>

<p style="margin-top: 1em">--vm-bytes N <br>
mmap N bytes per vm worker, the default is 256MB. One can
specify the size in units of Bytes, KBytes, MBytes and
GBytes using the suffix b, k, m or g.</p>

<p style="margin-top: 1em">--vm-ops N <br>
stop vm workers after N bogo operations.</p>

<p style="margin-top: 1em">--vm-hang N <br>
sleep N seconds before unmapping memory, the default is zero
seconds. Specifying 0 will do an infinite wait.</p>

<p style="margin-top: 1em">--vm-keep <br>
do not continually unmap and map memory, just keep on
re-writing to it.</p>

<p style="margin-top: 1em">--vm-locked <br>
Lock the pages of the mapped region into memory using mmap
MAP_LOCKED (since Linux 2.5.37). This is similar to locking
memory as described in mlock(2).</p>

<p style="margin-top: 1em">--vm-method m <br>
specify a vm stress method. By default, all the stress
methods are exercised sequentially, however one can specify
just one method to be used if required. Each of the vm <br>
workers have 3 phases:</p>

<p style="margin-top: 1em">1. Initialised. The anonymously
memory mapped region is set to a known pattern.</p>

<p style="margin-top: 1em">2. Exercised. Memory is modified
in a known predictable way. Some vm workers alter memory
sequentially, some use small or large strides to step along
memory.</p>

<p style="margin-top: 1em">3. Checked. The modified memory
is checked to see if it matches the expected result.</p>

<p style="margin-top: 1em">The vm methods containing
&rsquo;prime&rsquo; in their name have a stride of the
largest prime less than 2^64, allowing to them to thoroughly
step through memory and touch all loca&acirc; <br>
tions just once while also doing without touching memory
cells next to each other. This strategy exercises the cache
and page non-locality.</p>

<p style="margin-top: 1em">Since the memory being exercised
is virtually mapped then there is no guarantee of touching
page addresses in any particular physical order. These
workers should not be <br>
used to test that all the system&rsquo;s memory is working
correctly either, use tools such as memtest86 instead.</p>

<p style="margin-top: 1em">The vm stress methods are
intended to exercise memory in ways to possibly find memory
issues and to try to force thermal errors.</p>

<p style="margin-top: 1em">Available vm stress methods are
described as follows:</p>

<p style="margin-top: 1em">Method Description <br>
all iterate over all the vm stress methods as listed below.
<br>
flip sequentially work through memory 8 times, each time
just one bit in memory flipped (inverted). This will
effectively invert each byte in 8 passes. <br>
galpat-0 galloping pattern zeros. This sets all bits to 0
and flips just 1 in 4096 bits to 1. It then checks to see if
the 1s are pulled down to 0 by their <br>
neighbours or of the neighbours have been pulled up to 1.
<br>
galpat-1 galloping pattern ones. This sets all bits to 1 and
flips just 1 in 4096 bits to 0. It then checks to see if the
0s are pulled up to 1 by their <br>
neighbours or of the neighbours have been pulled down to 0.
<br>
gray fill the memory with sequential gray codes (these only
change 1 bit at a time between adjacent bytes) and then
check if they are set correctly. <br>
incdec work sequentially through memory twice, the first
pass increments each byte by a specific value and the second
pass decrements each byte back to <br>
the original start value. The increment/decrement value
changes on each invocation of the stressor. <br>
inc-nybble initialise memory to a set value (that changes on
each invocation of the stressor) and then sequentially work
through each byte incrementing the <br>
bottom 4 bits by 1 and the top 4 bits by 15. <br>
rand-set sequentially work through memory in 64 bit chunks
setting bytes in the chunk to the same 8 bit random value.
The random value changes on each <br>
chunk. Check that the values have not changed. <br>
rand-sum sequentially set all memory to random values and
then summate the number of bits that have changed from the
original set values. <br>
read64 sequentially read memory using 32 x 64 bit reads per
bogo loop. Each loop equates to one bogo operation. This
exercises raw memory reads. <br>
ror fill memory with a random pattern and then sequentially
rotate 64 bits of memory right by one bit, then check the
final load/rotate/stored values. <br>
swap fill memory in 64 byte chunks with random patterns.
Then swap each 64 chunk with a randomly chosen chunk.
Finally, reverse the swap to put the <br>
chunks back to their original place and check if the data is
correct. This exercises adjacent and random memory
load/stores. <br>
move-inv sequentially fill memory 64 bits of memory at a
time with random values, and then check if the memory is set
correctly. Next, sequentially invert <br>
each 64 bit pattern and again check if the memory is set as
expected. <br>
modulo-x fill memory over 23 iterations. Each iteration
starts one byte further along from the start of the memory
and steps along in 23 byte strides. In <br>
each stride, the first byte is set to a random pattern and
all other bytes are set to the inverse. Then it checks see
if the first byte contains <br>
the expected random pattern. This exercises cache
store/reads as well as seeing if neighbouring cells
influence each other. <br>
prime-0 iterate 8 times by stepping through memory in very
large prime strides clearing just on bit at a time in every
byte. Then check to see if all bits <br>
are set to zero. <br>
prime-1 iterate 8 times by stepping through memory in very
large prime strides setting just on bit at a time in every
byte. Then check to see if all bits <br>
are set to one. <br>
prime-gray-0 first step through memory in very large prime
strides clearing just on bit (based on a gray code) in every
byte. Next, repeat this but clear the <br>
other 7 bits. Then check to see if all bits are set to zero.
<br>
prime-gray-1 first step through memory in very large prime
strides setting just on bit (based on a gray code) in every
byte. Next, repeat this but set the other <br>
7 bits. Then check to see if all bits are set to one. <br>
rowhammer try to force memory corruption using the rowhammer
memory stressor. This fetches two 32 bit integers from
memory and forces a cache flush on the <br>
two addresses multiple times. This has been known to force
bit flipping on some hardware, especially with lower
frequency memory refresh cycles. <br>
walk-0d for each byte in memory, walk through each data line
setting them to low (and the others are set high) and check
that the written value is as <br>
expected. This checks if any data lines are stuck. <br>
walk-1d for each byte in memory, walk through each data line
setting them to high (and the others are set low) and check
that the written value is as <br>
expected. This checks if any data lines are stuck. <br>
walk-0a in the given memory mapping, work through a range of
specially chosen addresses working through address lines to
see if any address lines are stuck <br>
low. This works best with physical memory addressing,
however, exercising these virtual addresses has some value
too. <br>
walk-1a in the given memory mapping, work through a range of
specially chosen addresses working through address lines to
see if any address lines are stuck <br>
high. This works best with physical memory addressing,
however, exercising these virtual addresses has some value
too. <br>
write64 sequentially write memory using 32 x 64 bit writes
per bogo loop. Each loop equates to one bogo operation. This
exercises raw memory writes. Note <br>
that memory writes are not checked at the end of each test
iteration. <br>
zero-one set all memory bits to zero and then check if any
bits are not zero. Next, set all the memory bits to one and
check if any bits are not one.</p>

<p style="margin-top: 1em">--vm-populate <br>
populate (prefault) page tables for the memory mappings;
this can stress swapping. Only available on systems that
support MAP_POPULATE (since Linux 2.5.46).</p>

<p style="margin-top: 1em">--vm-rw N <br>
start N workers that transfer memory to/from a parent/child
using process_vm_writev(2) and process_vm_readv(2). This is
feature is only supported on Linux. Memory trans&acirc; <br>
fers are only verified if the --verify option is
enabled.</p>

<p style="margin-top: 1em">--vm-rw-ops N <br>
stop vm-rw workers after N memory read/writes.</p>

<p style="margin-top: 1em">--vm-rw-bytes N <br>
mmap N bytes per vm-rw worker, the default is 16MB. One can
specify the size in units of Bytes, KBytes, MBytes and
GBytes using the suffix b, k, m or g.</p>

<p style="margin-top: 1em">--vm-splice N <br>
move data from memory to /dev/null through a pipe without
any copying between kernel address space and user address
space using vmsplice(2) and splice(2). This is only <br>
available for Linux.</p>

<p style="margin-top: 1em">--vm-splice-ops N <br>
stop after N bogo vm-splice operations.</p>

<p style="margin-top: 1em">--vm-splice-bytes N <br>
transfer N bytes per vmsplice call, the default is 64K. One
can specify the size in units of Bytes, KBytes, MBytes and
GBytes using the suffix b, k, m or g.</p>

<p style="margin-top: 1em">--wait N <br>
start N workers that spawn off two children; one spins in a
pause(2) loop, the other continually stops and continues the
first. The controlling process waits on the first <br>
child to be resumed by the delivery of SIGCONT using
waitpid(2) and waitid(2).</p>

<p style="margin-top: 1em">--wait-ops N <br>
stop after N bogo wait operations.</p>

<p style="margin-top: 1em">--wcs N <br>
start N workers that exercise various libc wide character
string functions on random strings.</p>

<p style="margin-top: 1em">--wcs-method wcsfunc <br>
select a specific libc wide character string function to
stress. Available string functions to stress are: all,
wcscasecmp, wcscat, wcschr, wcscoll, wcscmp, wcscpy, <br>
wcslen, wcsncasecmp, wcsncat, wcsncmp, wcsrchr and wcsxfrm.
The &rsquo;all&rsquo; method is the default and will
exercise all the string methods.</p>

<p style="margin-top: 1em">--wcs-ops N <br>
stop after N bogo wide character string operations.</p>

<p style="margin-top: 1em">--xattr N <br>
start N workers that create, update and delete batches of
extended attributes on a file.</p>

<p style="margin-top: 1em">--xattr-ops N <br>
stop after N bogo extended attribute operations.</p>

<p style="margin-top: 1em">-y N, --yield N <br>
start N workers that call sched_yield(2). This stressor
ensures that at least 2 child processes per CPU exercice
shield_yield(2) no matter how many workers are specified,
<br>
thus always ensuring rapid context switching.</p>

<p style="margin-top: 1em">--yield-ops N <br>
stop yield stress workers after N sched_yield(2) bogo
operations.</p>

<p style="margin-top: 1em">--zero N <br>
start N workers reading /dev/zero.</p>

<p style="margin-top: 1em">--zero-ops N <br>
stop zero stress workers after N /dev/zero bogo read
operations.</p>

<p style="margin-top: 1em">--zlib N <br>
start N workers compressing and decompressing random data
using zlib. Each worker has two processes, one that
compresses random data and pipes it to another process that
<br>
decompresses the data. This stressor exercises CPU, cache
and memory.</p>

<p style="margin-top: 1em">--zlib-ops N <br>
stop after N bogo compression operations, each bogo
compression operation is a compression of 64K of random data
at the highest compression level.</p>

<p style="margin-top: 1em">--zombie N <br>
start N workers that create zombie processes. This will
rapidly try to create a default of 8192 child processes that
immediately die and wait in a zombie state until they <br>
are reaped. Once the maximum number of processes is reached
(or fork fails because one has reached the maximum allowed
number of children) the oldest child is reaped and <br>
a new process is then created in a first-in first-out
manner, and then repeated.</p>

<p style="margin-top: 1em">--zombie-ops N <br>
stop zombie stress workers after N bogo zombie
operations.</p>

<p style="margin-top: 1em">--zombie-max N <br>
try to create as many as N zombie processes. This may not be
reached if the system limit is less than N.</p>

<p style="margin-top: 1em">EXAMPLES <br>
stress-ng --cpu 4 --io 2 --vm 1 --vm-bytes 1G --timeout
60s</p>

<p style="margin-top: 1em">runs for 60 seconds with 4 cpu
stressors, 2 io stressors and 1 vm stressor using 1GB of
virtual memory.</p>

<p style="margin-top: 1em">stress-ng --cpu 8 --cpu-ops
800000</p>

<p style="margin-top: 1em">runs 8 cpu stressors and stops
after 800000 bogo operations.</p>

<p style="margin-top: 1em">stress-ng --sequential 2
--timeout 2m --metrics</p>

<p style="margin-top: 1em">run 2 simultaneous instances of
all the stressors sequentially one by one, each for 2
minutes and summarise with performance metrics at the
end.</p>

<p style="margin-top: 1em">stress-ng --cpu 4 --cpu-method
fft --cpu-ops 10000 --metrics-brief</p>

<p style="margin-top: 1em">run 4 FFT cpu stressors, stop
after 10000 bogo operations and produce a summary just for
the FFT results.</p>

<p style="margin-top: 1em">stress-ng --cpu 0 --cpu-method
all -t 1h</p>

<p style="margin-top: 1em">run cpu stressors on all online
CPUs working through all the available CPU stressors for 1
hour.</p>

<p style="margin-top: 1em">stress-ng --all 4 --timeout
5m</p>

<p style="margin-top: 1em">run 4 instances of all the
stressors for 5 minutes.</p>

<p style="margin-top: 1em">stress-ng --random 64</p>

<p style="margin-top: 1em">run 64 stressors that are
randomly chosen from all the available stressors.</p>

<p style="margin-top: 1em">stress-ng --cpu 64 --cpu-method
all --verify -t 10m --metrics-brief</p>

<p style="margin-top: 1em">run 64 instances of all the
different cpu stressors and verify that the computations are
correct for 10 minutes with a bogo operations summary at the
end.</p>

<p style="margin-top: 1em">stress-ng --sequential 0 -t
10m</p>

<p style="margin-top: 1em">run all the stressors one by one
for 10 minutes, with the number of instances of each
stressor matching the number of online CPUs.</p>

<p style="margin-top: 1em">stress-ng --sequential 8 --class
io -t 5m --times</p>

<p style="margin-top: 1em">run all the stressors in the io
class one by one for 5 minutes each, with 8 instances of
each stressor running concurrently and show overall time
utilisation statistics at <br>
the end of the run.</p>

<p style="margin-top: 1em">stress-ng --all 0 --maximize
--aggressive</p>

<p style="margin-top: 1em">run all the stressors (1
instance of each per CPU) simultaneously, maximize the
settings (memory sizes, file allocations, etc.) and select
the most demanding/aggressive <br>
options.</p>

<p style="margin-top: 1em">stress-ng --random 32 -x
numa,hdd,key</p>

<p style="margin-top: 1em">run 32 randomly selected
stressors and exclude the numa, hdd and key stressors</p>

<p style="margin-top: 1em">stress-ng --sequential 4 --class
vm --exclude bigheap,brk,stack</p>

<p style="margin-top: 1em">run 4 instances of the VM
stressors one after each other, excluding the bigheap, brk
and stack stressors</p>

<p style="margin-top: 1em">stress-ng --taskset 0,2-3 --cpu
3</p>

<p style="margin-top: 1em">run 3 instances of the CPU
stressor and pin them to CPUs 0, 2 and 3.</p>

<p style="margin-top: 1em">EXIT STATUS <br>
Status Description <br>
0 Success. <br>
1 Error; incorrect user options or a fatal resource issue in
the stress-ng stressor harness (for example, out of memory).
<br>
2 One or more stressors failed. <br>
3 One or more stressors failed to initialise because of lack
of resources, for example ENOMEM (no memory), ENOSPC (no
space on file system) or a missing or unimple&acirc; <br>
mented system call. <br>
4 One or more stressors were not implemented on a specific
architecture or operating system.</p>

<p style="margin-top: 1em">BUGS <br>
File bug reports at: <br>
https://launchpad.net/ubuntu/+source/stress-ng/+filebug</p>

<p style="margin-top: 1em">SEE ALSO <br>
cpuburn(1), perf(1), stress(1), taskset(1)</p>

<p style="margin-top: 1em">AUTHOR <br>
stress-ng was written by Colin King
&lt;colin.king@canonical.com&gt; and is a clean room
re-implementation and extension of the original stress tool
by Amos Waterland <br>
&lt;apw@rossby.metr.ou.edu&gt;. Thanks also for
contributions from Christian Ehrhardt, James Hunt, Jim
Rowan, Tim Gardner, Luca Pizzamiglio and Zhiyi Sun.</p>

<p style="margin-top: 1em">NOTES <br>
Sending a SIGALRM, SIGINT or SIGHUP to stress-ng causes it
to terminate all the stressor processes and ensures
temporary files and shared memory segments are removed
cleanly.</p>

<p style="margin-top: 1em">Sending a SIGUSR2 to stress-ng
will dump out the current load average and memory
statistics.</p>

<p style="margin-top: 1em">Note that the stress-ng cpu, io,
vm and hdd tests are different implementations of the
original stress tests and hence may produce different stress
characteristics. stress-ng <br>
does not support any GPU stress tests.</p>

<p style="margin-top: 1em">The bogo operations metrics may
change with each release because of bug fixes to the code,
new features, compiler optimisations or changes in system
call performance.</p>

<p style="margin-top: 1em">COPYRIGHT <br>
Copyright &Acirc;&copy; 2013-2016 Canonical Ltd. <br>
This is free software; see the source for copying
conditions. There is NO warranty; not even for
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.</p>

<p style="margin-top: 1em">November 11, 2016
STRESS-NG(1)</p>
<hr>
</body>
</html>
