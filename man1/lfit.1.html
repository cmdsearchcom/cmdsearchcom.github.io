<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 16:21:44 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>LFIT(1) User Commands LFIT(1)</p>

<p style="margin-top: 1em">NAME <br>
lfit - general purpose evaluation and regression analysis
tool</p>

<p style="margin-top: 1em">SYNOPSIS <br>
lfit [method of analysis] [options] &lt;input&gt; [-o,
--output &lt;output&gt;]</p>

<p style="margin-top: 1em">DESCRIPTION <br>
The program &lsquo;lfit&lsquo; is a standalone command line
driven tool designed for both interactive and batch
processed data analysis and regression. In principle, the
program may run in <br>
two modes. First, &lsquo;lfit&lsquo; supports numerous
regression analysis methods that can be used to search for
&quot;best fit&quot; parameters of model functions in order
to model the input data <br>
(which are read from one or more input files in tabulated
form). Second, &lsquo;lfit&lsquo; is capable to read input
data and performs various arithmetic operations as it is
specified by the <br>
user. Basically this second mode is used to evaluate the
model functions with the parameters presumably derived by
the actual regression methods (and in order to complete this
<br>
evaluation, only slight changes are needed in the command
line invocation arguments).</p>

<p style="margin-top: 1em">OPTIONS <br>
General options: <br>
-h, --help <br>
Gives general summary about the command line options.</p>

<p style="margin-top: 1em">--long-help, --help-long <br>
Gives a detailed list of command line options.</p>

<p style="margin-top: 1em">--wiki-help, --help-wiki,
--mediawiki-help, --help-mediawiki <br>
Gives a detailed list of command line options in Mediawiki
format.</p>

<p style="margin-top: 1em">--version, --version-short,
--short-version <br>
Gives some version information about the program.</p>

<p style="margin-top: 1em">--functions, --list-functions,
--function-list <br>
Lists the available arithmetic operations and built-in
functions supported by the program.</p>

<p style="margin-top: 1em">--examples <br>
Prints some very basic examples for the program
invocation.</p>

<p style="margin-top: 1em">Common options for regression
analysis: <br>
-v, --variable, --variables &lt;list-of-variables&gt; <br>
Comma-separated list of regression variables. In case of
non-linear regression analysis, all of these fit variables
are expected to have some initial values (specified as <br>
&lt;name&gt;=&lt;value&gt;), otherwise the initial values
are set to be zero. Note that in the case of some of the
regression/analysis methods, additional parameters should be
<br>
assigned to these fit/regression variables. See the section
&quot;Regression analysis methods&quot; for additional
details.</p>

<p style="margin-top: 1em">-c, --column, --columns
&lt;independent&gt;[:&lt;column index&gt;],... <br>
Comma-separated list of independet variable names as read
from the subsequent columns of the primary input data file.
If the independent variables are not in sequential <br>
order in the input file, the optional column indices should
be defined for each variable, by separating the column index
with a colon after the name of the variable. In <br>
the case of multiple input files and data blocks, the user
should assign the individual independent variables and the
respective column names and definitions for each file <br>
(see later, Sec. &quot;Multiple data blocks&quot;).</p>

<p style="margin-top: 1em">-f, --function &lt;model
function&gt; <br>
Model function of the analysis in a symbolic form. This
expression for the model function should contain built-in
arithmetic operators, built-in functions, user-defined <br>
macros (see -x, --define) or functions provided by the
dynamically loaded external modules (see -d, --dynamic). The
model function can depend on both the fit/regression <br>
variables (see -v, --variables) and the independent
variables read from the input file (see -c, --columns). In
the case of multiple input files and data blocks, the user
<br>
should assign the respective model functions for each data
block (see later). Note that some of the analysis methods
expects the model function to be either differentiable <br>
or linear in the fit/regression variables. See
&quot;Regression analysis methods&quot; later on about more
details.</p>

<p style="margin-top: 1em">-y, --dependent &lt;dependent
expression&gt; <br>
The dependent variable of the regression analysis, in a form
of an arithmetic expression. This expression for the
dependent variable can depend only on the variables read
<br>
from the input file (see -c, --columns). In the case of
multiple input files and data blocks, the user should assign
the respective dependent expressions for each data <br>
block (see later).</p>

<p style="margin-top: 1em">-o, --output &lt;output file&gt;
<br>
Name of the output file into which the fit results (the
values for the fit/regression variables) are written.</p>

<p style="margin-top: 1em">Common options for function
evaluation: <br>
-f, --function &lt;function to evaluate&gt;[...] <br>
List of functions to be evaluated. More expressions can be
specified by either separating the subsequent expressions by
a comma or by specifying more -f, --function <br>
options in the command line.</p>

<p style="margin-top: 1em">Note that the two basic modes of
&lsquo;lfit&lsquo; are distinguished only by the presence or
the absence of the -y, --dependent command line argument. In
other words, there isn&rsquo;t any <br>
explicit command line argument which specify the mode of
&lsquo;lfit&lsquo;. If the -y, --dependent command line
argument is omitted, &lsquo;lfit&lsquo; runs in function
evaluation mode, otherwise the <br>
program runs in regression analysis mode.</p>

<p style="margin-top: 1em">-o, --output &lt;output file&gt;
<br>
Name of the output file in which the results of the function
evaluation are written.</p>

<p style="margin-top: 1em">Regression analysis methods:
<br>
-L, --clls, --linear <br>
The default mode of &lsquo;lfit&lsquo;, the classical linear
least squares (CLLS) method. The model functions specified
after -f, --function are expected to be both differentiable
and <br>
linear with respect to the fit/regression variables.
Otherwise, &lsquo;lfit&lsquo; detects the non-differentiable
and non-linear property of the model function(s) and refuses
the <br>
analysis. In this case, other types of regression analysis
methods can be applied depending our needs, for instance the
Levenberg-Marquardtalgorithm (NLLM, see -N, --nllm) <br>
or the downhill simplex minimization (DHSX, see -D,
--dhsx).</p>

<p style="margin-top: 1em">-N, --nllm, --nonlinear <br>
This option implies a regression involving the nonlinear
Levenberg-Marquardt (NLLM) minimization algorithm. The model
function(s) specified after -f, --function are <br>
expected to be differentiable with respect to the
fit/regression variables. Otherwise, &lsquo;lfit&lsquo;
detects the non-differentiable property and refuses the
analysis. There some <br>
fine-tune parameters of the Levenberg-Marquardt algorithm,
see also the secion &quot;Fine-tuning of regression analysis
methods&quot; for more details how these additional
regres&acirc; <br>
sion parameters can be set. Note that all of the
fit/regression variables should have a proper initial value,
defined in the command line argument -v, --variable (see
also <br>
there).</p>

<p style="margin-top: 1em">-U, --lmnd <br>
Levenberg-Marquardt minimization with numerical partial
derivatives (LMND). Same as the NLLM method, with the
exception of that the partial derivatives of the model
func&acirc; <br>
tion(s) are calculated numerically. Therefore, the model
function(s) may contain functions of which partial
derivatives are not known in an analytic form. The
differences <br>
used in the computations of the partial derivatives should
be declared by the user, see also the command line option
-q, --differences.</p>

<p style="margin-top: 1em">-D, --dhsx, --downhill <br>
This option implies a regression involving the nonlinear
downhill simplex (DHSX) minimization algorithm. The user
should specify the proper inital values and their
uncer&acirc; <br>
tainties as
&lt;name&gt;=&lt;initial&gt;:&lt;uncertainty&gt;, unless the
&quot;fisher&quot; option is passed to the -P, --parameters
command line argument (see later in the section
&quot;Fine-tuning of <br>
regression analysis methods&quot;). In the first case, the
initial size of the simplex is based on the uncertainties
provided by the user while in the second case, the initial
<br>
simplex is derived from the eigenvalues and eigenvectors of
the Fisher covariance matrix. Note that the model functions
must be differentiable in the latter case.</p>

<p style="margin-top: 1em">-M, --mcmc <br>
This option implies the method of Markov Chain Monte-Carlo
(MCMC). The model function(s) can be arbitrary in the point
of differentiability. However, each of the <br>
fit/regression variables must have an initial assumption for
their uncertainties which must be specified via the command
line argument -v, --variable. The user should <br>
specify the proper inital values and uncertainties of these
as &lt;name&gt;=&lt;initial&gt;:&lt;uncertainty&gt;. In the
actual implementation of &lsquo;lfit&lsquo;, each variable
has an uncorrelated <br>
Gaussian a priori distribution with the specified
uncertainty. The MCMC algorithm has some fine-tune
parameters, see the section &quot;Fine-tuning of regression
analysis meth&acirc; <br>
ods&quot; for more details.</p>

<p style="margin-top: 1em">-K, --mchi, --chi2 <br>
With this option one can perform a &quot;brute force&quot;
Chi^2 minimization by evaluating the value of the merit
function of Chi^2 on a grid of the fit/regression variables.
In <br>
this case the grid size and resolution must be specified in
a specific form after the -v, --variable command line
argument. Namely each of the fit/regression variables <br>
intended to be varied on a grid must have a format of
&lt;name&gt;=[&lt;min&gt;:&lt;step&gt;:&lt;max&gt;] while
the other ones specified as &lt;name&gt;=&lt;value&gt; are
kept fixed. The output of this anal&acirc; <br>
ysis will be a series of lines with N+1 columns, where the
values of fit/regression variables are followed by the value
of the merit function. Note that all of the <br>
declared fit/regression variables are written to the output,
including the ones which are fixed (therefore the output is
somewhat redundant).</p>

<p style="margin-top: 1em">-E, --emce <br>
This option implies the method of &quot;refitting to
synthetic data sets&quot;, or &quot;error Monte-Carlo
estimation&quot; (EMCE). This method must have a primarily
assigned minimization <br>
algorithm (that can be any of the CLLS, NLLM or DHSX
methods). First, the program searches the best fit values
for the fit/regression variables involving the assigned
pri&acirc; <br>
mary minimization algorithm and reports these best fit
variables. Then, additional synthetic data sets are
generated around this set of best fit variables and the
mini&acirc; <br>
mization is repeated involving the same primary method. The
synthetic data sets are generated independently for each
input data block, taking into account the fit residu&acirc;
<br>
als. The noise added to the best fit data is generated from
the power spectrum of the residuals.</p>

<p style="margin-top: 1em">-X, --xmmc <br>
This option implies an improved/extended version of the
Markov Chain Monte-Carlo analysis (XMMC). The major
differences between the classic MCMC and XMMC methods are
the <br>
following. 1/ The transition distribution is derived from
the Fisher covariance matrix. 2/ The program performs an
initial minimization of the merit function involving the
<br>
method of downhill simplex. 3/ Various sanity checks are
performed in order to verify the convergence of the Markov
chains (including the comparison of the actual and
the&acirc; <br>
oretical transition probabilities, the computation of the
autocorrelation lengths of each fit/regression variable
series and the comparison of the statistical and Fisher <br>
covariance).</p>

<p style="margin-top: 1em">-A, --fima <br>
Fisher information matrix analysis (FIMA). With this
analysis method one can estimate the uncertainties and
correlations of the fit/regression variables involving the
<br>
method of Fisher matrix analysis. This method does not
minimize the merit functions by adjusting the fit/regression
variables, instead, the initial values (specified after <br>
the -v, --variables option) are expected to be the
&quot;best fit&quot; ones.</p>

<p style="margin-top: 1em">Fine-tuning of regression
analysis methods: <br>
-e, --error &lt;error expression&gt; <br>
Expression for the uncertainties. Note that zero or negative
uncertainty is equivalent to zero weight, i.e. input lines
with zero or negative errors are discarded from the <br>
fit.</p>

<p style="margin-top: 1em">-w, --weight &lt;weight
expression&gt; <br>
Expression for the weights. The weight is simply the
reciprocal of the uncertainty. The default error/uncertainty
(and therefore the weight) is unity. Note that most of <br>
the analysis/regression methods are rather sensitive to the
uncertainties since the merit function also depends on
these.</p>

<p style="margin-top: 1em">-P, --parameters &lt;regression
parameters&gt; <br>
This option is followed by a set of optional fine-tune
parameters, that is different for each primary regression
analysis method:</p>

<p style="margin-top: 1em">default, defaults <br>
Use the default fine-tune parameters for the given
regression method.</p>

<p style="margin-top: 1em">clls, linear <br>
Use the classic linear least squares method as the primary
minimization algorithm of the EMCE method. Like in the case
of the CLLS regression analysis (see -L, --clls), <br>
the model function(s) must be both differentiable and linear
with respect to the fit/regression variables.</p>

<p style="margin-top: 1em">nllm, nonlinear <br>
Use the non-linear Levenberg-Marquardt minimization
algorithm as the primary minimization algorithm of the EMCE
method. Like in the case of the NLLM regression analysis
<br>
(see -N, --nllm), the model function(s) must be
differentiable with respect to the fit/regression
variables.</p>

<p style="margin-top: 1em">lmnd Use the non-linear
Levenberg-Marquardt minimization algorithm as the primary
minimization algorithm of the EMCE method. Like in the case
of -U, --lmnd regression method, <br>
the parametric derivatives of the model function(s) are
calculated by a numerical approximation (see also -U, --lmnd
and -q, --differences for additional details).</p>

<p style="margin-top: 1em">dhsx, downhill <br>
Use the downhill simplex (DHSX) minimization as the primary
minimization algorithm of the EMCE method. Unless the
additional &rsquo;fisher&rsquo; option is specified
directly, like in <br>
the default case of the DHSX regression method, the user
should specify the uncertainties of the fit/regression
variables that are used as an initial size of the
simplex.</p>

<p style="margin-top: 1em">mc, montecarlo <br>
Use a primitive Monte-Carlo diffusion minimization technique
as the primary minimization algorithm of the EMCE method.
The user should specify the uncertainties of the <br>
fit/regression variables which are then used to generate the
Monte-Carlo transitions. This primary minimization technique
is rather nasty (very slow), so its usage is not <br>
recommended.</p>

<p style="margin-top: 1em">fisher In the case of the DHSX
regression method or in the case of the EMCE method when the
primary minimization is the downhill simplex algorithm, the
initial size of the sim&acirc; <br>
plex is derived from the Fisher covariance approximation
evaluated at the point represented by the initial values of
the fit/regression variables. Since the derivation of <br>
the Fisher covariance requires the knowledge of the partial
derivatives of the model function(s) with respect to the
fit/regression variables, the(se) model function(s) <br>
must be differentiable. On the other hand, the user do not
have to specify the initial uncertainties after the -v,
--variables option since these uncertainties derived <br>
automatically from the Fisher covariance.</p>

<p style="margin-top: 1em">skip In the case of EMCE and
XMMC method, the initial minimization is skipped.</p>

<p style="margin-top: 1em">lambda=&lt;value&gt; <br>
Initial value for the &quot;lambda&quot; parameter of the
Levenberg-Marquardt algorithm.</p>

<p style="margin-top: 1em">multiply=&lt;value&gt; <br>
Value of the &quot;lambda multiplicator&quot; parameter of
the Levenberg-Marquardt algorithm.</p>


<p style="margin-top: 1em">iterations=&lt;max.iterations&gt;
<br>
Number of iterations during the Levenberg-Marquardt
algorithm.</p>

<p style="margin-top: 1em">accepted <br>
Count the accepted transitions in the MCMC and XMMC methods
(default).</p>

<p style="margin-top: 1em">nonaccepted <br>
Count the total (accepted plus non-accepted) transitions in
the MCMC and XMMC methods.</p>

<p style="margin-top: 1em">gibbs Use the Gibbs sampler in
the MCMC method.</p>

<p style="margin-top: 1em">adaptive <br>
Use the adaptive XMMC algorithm (i.e. the Fisher covariance
is re-computed after each accepted transition).</p>

<p style="margin-top: 1em">window=&lt;window size&gt; <br>
Window size for calculating the autocorrelation lengths for
the Markov chains (these autocorrelation lengths are
reported only in the case of XMMC method). The default <br>
value is 20, which is fine in the most cases since the
typical autocorrelation lengths are between 1 and 2 for nice
convergent chains.</p>

<p style="margin-top: 1em">-q, --difference
&lt;variablename&gt;=&lt;difference&gt;[,...] <br>
The analysis method of LMND (Levenberg-Marquardt
minimization using numerical derivatives, see -U, --lmnd)
requires the differences that are used during the
computations <br>
of the partial derivatives of the model function(s). With
this option, one can specify these differences.</p>

<p style="margin-top: 1em">-k, --separate
&lt;variablename&gt;[,...] <br>
In the case of non-linear regression methods (for instance,
DHSX or XMMC) the fit/regression variables in which the
model functions are linear can be separated from the <br>
nonlinear part and therefore make the minimization process
more robust and reliable. Since the set of variables in
which the model functions are linear is ambiguous, the <br>
user should explicitly specify this supposedly linear subset
of regression variables. (For instance, the model function
&quot;a*b*x+a*cos(x)+b*sin(x)+c*x^2&quot; is linear in both
<br>
&quot;(a,c)&quot; and &quot;(b,c)&quot; parameter vectors
but it is non-linear in &quot;(a,b,c)&quot;.) The program
checks whether the specified subset of regression variables
is a linear subset and <br>
reports a warning if not. Note that the subset of separated
linear variables (defined here) and the subset of the
fit/regression variables affected by linear constraints <br>
(see also section &quot;Constraints&quot;) must be
disjoint.</p>

<p style="margin-top: 1em">--perturbations &lt;noise
level&gt;, --perturbations &lt;key&gt;=&lt;noise
level&gt;[,...] <br>
Additional white noise to be added to each EMCE synthetic
data sets. Each data block (referred here by the approprate
data block keys, see also section &quot;Multiple data <br>
blocks&quot;) may have different white noise levels. If
there is only one data block, this command line argument is
followed only by a single number specifying the white noise
<br>
level.</p>

<p style="margin-top: 1em">Additional parameters for
Monte-Carlo analysis: <br>
-s, --seed &lt;random seed&gt; <br>
Seed for the random number generator. By default this seed
is 0, thus all of the Monte-Carlo regression analyses (EMCE,
MCMC, XMMC and the optional generator for the FIMA <br>
method) generate reproducible parameter distributions. A
positive value after this option yields alternative random
seeds while all negative values result in an automatic <br>
random seed (derived from various available sources, such as
/dev/[u]random, system time, hardware MAC address and so),
therefore distributions generated involving this <br>
kind of automatic random seed are not reproducible.</p>

<p style="margin-top: 1em">-i,
--[mcmc,emce,xmmc,fima]-iterations &lt;iterations&gt; <br>
The actual number of Monte-Carlo iterations for the MCMC,
EMCE, XMMC methods. Additionally, the FIMA method is capable
to generate a mock Gaussian distribution of the <br>
parameter with the same covariance as derived by the Fisher
analysis. The number of points in this mock distribution is
also specified by this command line option.</p>

<p style="margin-top: 1em">Clipping outlier data points:
<br>
-r, --sigma, --rejection-level &lt;level&gt; <br>
Rejection level in the units of standard deviations.</p>

<p style="margin-top: 1em">-n, --iterations &lt;number of
iterations&gt; <br>
Maximum number of iterations in the outlier clipping cycles.
The actual number of outlier points can be traced by
increasing the verbosity of the program (see -V,
--ver&acirc; <br>
bose).</p>

<p style="margin-top: 1em">--[no-]weighted-sigma <br>
During the derivation of the standard deviation, the
contribution of the data points data points can be weighted
by the respective weights/error bars (see also -w, <br>
--weight or -e, --error in the section &quot;Fine-tuning of
regression analysis methods&quot;). If no weights/error bars
are associated to the data points (i.e. both -w, --weight or
<br>
-e, --error options are omitted), this option will have no
practical effect.</p>

<p style="margin-top: 1em">Note that in the actual version
of &lsquo;lfit&lsquo;, only the CLLS, NLLM and LMND
regression methods support the above discussed way of
outlier clipping.</p>

<p style="margin-top: 1em">Multiple data blocks: <br>
-i&lt;key&gt; &lt;input file name&gt; <br>
Input file name for the data block named as &lt;key&gt;.</p>

<p style="margin-top: 1em">-c&lt;key&gt;
&lt;independent&gt;[:&lt;column index&gt;],... <br>
Column definitions (see also -c, --columns) for the given
data block named as &lt;key&gt;.</p>

<p style="margin-top: 1em">-f&lt;key&gt; &lt;model
function&gt; <br>
Expression for the model function assigned to the data block
named as &lt;key&gt;.</p>

<p style="margin-top: 1em">-y&lt;key&gt; &lt;dependent
expression&gt; <br>
Expression of the dependent variable for the data block
named as &lt;key&gt;.</p>

<p style="margin-top: 1em">-e&lt;key&gt; &lt;errors&gt;
<br>
Expression of the uncertainties for the data block named as
&lt;key&gt;.</p>

<p style="margin-top: 1em">-w&lt;key&gt; &lt;weights&gt;
<br>
Expression of the weights for the data block named as
&lt;key&gt;. Note that like in the case of -e, --errors and
-w, --weights, only one of the -e&lt;key&gt;, -w&lt;key&gt;
arguments <br>
should be specified.</p>

<p style="margin-top: 1em">Constraints: <br>
-t, --constraint, --constraints
&lt;expression&gt;{=&lt;&gt;}&lt;expression&gt;[,...] <br>
List of fit and domain constraints between the regression
variables. Each fit constraint expression must be linear in
the fit/regression variables. The program checks the <br>
linearity of the fit constraints and reports an error if any
of the constraints are non-linear. A domain constraint can
be any expression involving arbitrary binary <br>
arithmetic relation (such as strict greater than:
&rsquo;&gt;&rsquo;, strict less than: &rsquo;&lt;&rsquo;,
greater or equal to: &rsquo;&gt;=&rsquo; and less or requal
to: &rsquo;&lt;=&rsquo;). Constraints can be specified
either <br>
by a comma-separated list after a single command line
argument of -t, --constraints or by multiple of these
command line arguments.</p>

<p style="margin-top: 1em">-v, --variable
&lt;name&gt;:=&lt;value&gt; <br>
Another form of specifying constraints. The variable
specifications after -v, --variable can also be used to
define constraints by writing &quot;:=&quot; instead of
&quot;=&quot; between the <br>
variable name and initial value. Thus, -v
&lt;name&gt;:=&lt;value&gt; is equivalent to -v
&lt;name&gt;=&lt;value&gt; -t
&lt;name&gt;=&lt;value&gt;.</p>

<p style="margin-top: 1em">User-defined functions: <br>
-x, --define, --macro
&lt;name&gt;(&lt;parameters&gt;)=&lt;definition
expression&gt; <br>
With this option, the user can define additional functions
(also called macros) on the top of the built-in functions
and operators, dynamically loadaded functions and pre&acirc;
<br>
viously defined macros. Note that each such user-defined
function must be stand-alone, i.e. external variables (such
as fit/regression variables and independent variables) <br>
cannot be part of the definition expression, only the
parameters of these functions.</p>

<p style="margin-top: 1em">Dynamically loaded extensions
and functions: <br>
-d, --dynamic &lt;library&gt;:&lt;array&gt;[,...] <br>
Load the dynamically linked library (shared object) named
&lt;library&gt; and import the global
&lsquo;lfit&lsquo;-compatible set of functions defined in
the arrays specified after the name <br>
of the library. The arrays must have to be declared with the
type of &rsquo;lfitfunction&rsquo;, as it is defined in the
file &quot;lfit.h&quot;. Each record in this array contains
information <br>
about a certain imported function, namely the actual name of
this function, flags specifying whether the function is
differentiable and/or linear in its regression parame&acirc;
<br>
ters, the number of regression variables and independent
variables and the actual C subroutine that implements the
evaulation of the function (and the optional computation
<br>
of the partial derivatives). The module
&rsquo;linear.c&rsquo; and &rsquo;linear.so&rsquo; provides
a simple example that implements the
&quot;line(a,b,x)=a*x+b&quot; function. This example
function has <br>
two regression variables (&quot;a&quot; and &quot;b&quot;)
and one independent variable (&quot;x&quot;) and the
function itself is linear in the regression variables.</p>

<p style="margin-top: 1em">More on outputs: <br>
-z, --columns-output &lt;column indices&gt; <br>
Column indices where the results are written in evaluation
mode. If this option is omitted, the results of the function
evaluation are written sequentally. Otherwise, the <br>
input file is written to the output and the appropriate
columns (specified here) are replaced by the respective
results of the function evaluation. Thus, although the <br>
default column order is sequential, there is a significant
difference between omitting this option and specifying
&quot;-z 1,2,...,N&quot;. In the first case, the output file
con&acirc; <br>
tains only the results of the function evaluations, while in
the latter case, the first N columns of the original file
are replaced with the results.</p>

<p style="margin-top: 1em">--errors, --error-line,
--error-columns <br>
Print the uncertainties of the fit/regression variables.</p>

<p style="margin-top: 1em">-F, --format &lt;variable
name&gt;=&lt;format&gt;[,...] <br>
Format of the output in printf-style for each fit/regression
variable(see printf(3)). The default format is %12.6g (6
signifiant figures).</p>

<p style="margin-top: 1em">-F, --format
&lt;format&gt;[,...] <br>
Format of the output in evaluation mode. The default format
is %12.6g (6 signifiant figures).</p>

<p style="margin-top: 1em">-C, --correlation-format
&lt;format&gt; <br>
Format of the correlation matrix elements. The default
format is %6.3f (3 significant figures).</p>

<p style="margin-top: 1em">-g, --derived-variable[s]
&lt;variable name&gt;=&lt;expression&gt;[,...] <br>
Some of the regression and analysis methods are capable to
compute the uncertainties and correlations for derived
regression variables. These additional (and therefore not
<br>
independent) variables can be defined with this command line
option. In the definition expression one should use only the
fit/regression variables (as defined by the -v, <br>
--variables command line argument). The output format of
these variables can also be specified by the -F, --format
command line argument.</p>

<p style="margin-top: 1em">-u, --output-fitted
&lt;filename&gt; <br>
Neme of an output file into which those lines of the input
are written that were involved in the final regression. This
option is useful in the case of outlier clipping in <br>
order to see what was the actual subset of input data that
was used in the fit (see also the -n, --iterations and -r,
--sigma options).</p>

<p style="margin-top: 1em">-j, --output-rejected
&lt;filename&gt; <br>
Neme of an output file into which those lines of the input
are written that were rejected from the final regression.
This option is useful in the case of outlier clipping <br>
in order to see what was the actual subset of input data
where the dependent variable represented outlier points (see
also the -n, --iterations and -r, --sigma options).</p>

<p style="margin-top: 1em">-a, --output-all
&lt;filename&gt; <br>
File containing the lines of the input file that were
involved in the complete regression analysis. This file is
simply the original file, only the commented and empty <br>
lines are omitted.</p>

<p style="margin-top: 1em">-p, --output-expression
&lt;filename&gt; <br>
In this file the model function is written in which the
fit/regression variables are replaced by their best-fit
values.</p>

<p style="margin-top: 1em">-l, --output-variables
&lt;filename&gt; <br>
List of the names and values of the fit/regression variables
in the same format as used after the -v, --variables command
line argument. The content of this file can <br>
therefore be passed to subsequent invocations of
&lsquo;lfit&lsquo;.</p>

<p style="margin-top: 1em">--delta <br>
Write the individual differences between the independent
variables and the evaluated best fit model function values
for each line in the output files specified by the -u, <br>
--output-fitted, -j, --output-rejected and -a, --output-all
command line options.</p>

<p style="margin-top: 1em">--delta-comment <br>
Same as --delta, but the differences are written as a
comment (i.e. separated by a &rsquo;##&rsquo; from the
original input lines).</p>

<p style="margin-top: 1em">--residual <br>
Write the final fit residual to the output file (after the
list of the best-fit values for the fit/regression
variables).</p>

<p style="margin-top: 1em">REPORTING BUGS <br>
Report bugs to &lt;apal@szofi.net&gt;, see also
http://fitsh.net/.</p>

<p style="margin-top: 1em">COPYRIGHT <br>
Copyright &Acirc;&copy; 1996, 2002, 2004-2008, 2009; Pal,
Andras &lt;apal@szofi.net&gt;</p>

<p style="margin-top: 1em">lfit 7.9 (2009.05.25) September
2016 LFIT(1)</p>
<hr>
</body>
</html>
