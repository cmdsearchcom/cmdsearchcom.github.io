<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 16:25:47 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>mlpack_nca(1) General Commands Manual mlpack_nca(1)</p>

<p style="margin-top: 1em">NAME <br>
mlpack_nca - neighborhood components analysis (nca)</p>

<p style="margin-top: 1em">SYNOPSIS <br>
mlpack_nca [-h] [-v] -i string -o string [-A double] [-l
string] [-L] [-n int] [-T int] [-M double] [-m double] [-N]
[-B int] [-O string] [-s int] [-a double] [-t double] [-V]
[-w double]</p>

<p style="margin-top: 1em">DESCRIPTION <br>
This program implements Neighborhood Components Analysis,
both a linear dimensionality reduction technique and a
distance learning technique. The method seeks to improve
k-near&acirc; <br>
est-neighbor classification on a dataset by scaling the
dimensions. The method is nonparametric, and does not
require a value of k. It works by using stochastic
(&quot;soft&quot;) neighbor <br>
assignments and using optimization techniques over the
gradient of the accuracy of the neighbor assignments.</p>

<p style="margin-top: 1em">To work, this algorithm needs
labeled data. It can be given as the last row of the input
dataset (--input_file), or alternatively in a separate file
(--labels_file).</p>

<p style="margin-top: 1em">This implementation of NCA uses
either stochastic gradient descent or the L_BFGS optimizer.
Both of these optimizers do not guarantee global convergence
for a nonconvex objective <br>
function (NCA&rsquo;s objective function is nonconvex), so
the final results could depend on the random seed or other
optimizer parameters.</p>

<p style="margin-top: 1em">Stochastic gradient descent,
specified by --optimizer &quot;sgd&quot;, depends primarily
on two parameters: the step size (--step_size) and the
maximum number of iterations (--max_itera&acirc; <br>
tions). In addition, a normalized starting point can be used
(--normalize), which is necessary if many warnings of the
form &acirc;Denominator of p_i is 0!&rsquo; are given.
Tuning the step <br>
size can be a tedious affair. In general, the step size is
too large if the objective is not mostly uniformly
decreasing, or if zero-valued denominator warnings are being
issued. <br>
The step size is too small if the objective is changing very
slowly. Setting the termination condition can be done easily
once a good step size parameter is found; either <br>
increase the maximum iterations to a large number and allow
SGD to find a minimum, or set the maximum iterations to 0
(allowing infinite iterations) and set the tolerance
(--tol&acirc; <br>
erance) to define the maximum allowed difference between
objectives for SGD to terminate. Be careful -- setting the
tolerance instead of the maximum iterations can take a very
<br>
long time and may actually never converge due to the
properties of the SGD optimizer.</p>

<p style="margin-top: 1em">The L-BFGS optimizer, specified
by --optimizer &quot;lbfgs&quot;, uses a back-tracking line
search algorithm to minimize a function. The following
parameters are used by L-BFGS: <br>
--num_basis (specifies the number of memory points used by
L-BFGS), --max_iterations, --armijo_constant, --wolfe,
--tolerance (the optimization is terminated when the
gradient <br>
norm is below this value), --max_line_search_trials,
--min_step and --max_step (which both refer to the line
search routine). For more details on the L-BFGS optimizer,
consult <br>
either the mlpack L-BFGS documentation (in lbfgs.hpp) or the
vast set of published literature on L-BFGS.</p>

<p style="margin-top: 1em">By default, the SGD optimizer is
used.</p>

<p style="margin-top: 1em">REQUIRED OPTIONS <br>
--input_file (-i) [string] <br>
Input dataset to run NCA on.</p>

<p style="margin-top: 1em">--output_file (-o) [string] <br>
Output file for learned distance matrix.</p>

<p style="margin-top: 1em">OPTIONS <br>
--armijo_constant (-A) [double] <br>
Armijo constant for L-BFGS. Default value 0.0001.</p>

<p style="margin-top: 1em">--help (-h) <br>
Default help info.</p>

<p style="margin-top: 1em">--info [string] <br>
Get help on a specific module or option. Default value
&rsquo;&rsquo;.</p>

<p style="margin-top: 1em">--labels_file (-l) [string] <br>
File of labels for input dataset. Default value
&rsquo;&rsquo;.</p>

<p style="margin-top: 1em">--linear_scan (-L) <br>
Don&rsquo;t shuffle the order in which data points are
visited for SGD.</p>

<p style="margin-top: 1em">--max_iterations (-n) [int] <br>
Maximum number of iterations for SGD or L-BFGS (0 indicates
no limit). Default value 500000.</p>

<p style="margin-top: 1em">--max_line_search_trials (-T)
[int] <br>
Maximum number of line search trials for L-BFGS. Default
value 50.</p>

<p style="margin-top: 1em">--max_step (-M) [double] <br>
Maximum step of line search for L-BFGS. Default value
1e+20.</p>

<p style="margin-top: 1em">--min_step (-m) [double] <br>
Minimum step of line search for L-BFGS. Default value
1e-20.</p>

<p style="margin-top: 1em">--normalize (-N) <br>
Use a normalized starting point for optimization. This is
useful for when points are far apart, or when SGD is
returning NaN.</p>

<p style="margin-top: 1em">--num_basis (-B) [int] <br>
Number of memory points to be stored for L-BFGS. Default
value 5.</p>

<p style="margin-top: 1em">--optimizer (-O) [string] <br>
Optimizer to use; &quot;sgd&quot; or &quot;lbfgs&quot;.
Default value &rsquo;sgd&rsquo;.</p>

<p style="margin-top: 1em">--seed (-s) [int] <br>
Random seed. If 0, &rsquo;std::time(NULL)&rsquo; is used.
Default value 0.</p>

<p style="margin-top: 1em">--step_size (-a) [double] <br>
Step size for stochastic gradient descent (alpha). Default
value 0.01.</p>

<p style="margin-top: 1em">--tolerance (-t) [double] <br>
Maximum tolerance for termination of SGD or L-BFGS. Default
value 1e-07.</p>

<p style="margin-top: 1em">--verbose (-v) <br>
Display informational messages and the full list of
parameters and timers at the end of execution.</p>

<p style="margin-top: 1em">--version (-V) <br>
Display the version of mlpack.</p>

<p style="margin-top: 1em">--wolfe (-w) [double] <br>
Wolfe condition parameter for L-BFGS. Default value 0.9.</p>

<p style="margin-top: 1em">ADDITIONAL INFORMATION <br>
For further information, including relevant papers,
citations, and theory, consult the documentation found at
http://www.mlpack.org or included with your DISTRIBUTION OF
MLPACK.</p>
 
<p style="margin-top: 1em">mlpack_nca(1)</p>
<hr>
</body>
</html>
