<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 16:44:09 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>xl(1) Xen xl(1)</p>

<p style="margin-top: 1em">NAME <br>
XL - Xen management tool, based on LibXenlight</p>

<p style="margin-top: 1em">SYNOPSIS <br>
xl subcommand [args]</p>

<p style="margin-top: 1em">DESCRIPTION <br>
The xl program is the new tool for managing Xen guest
domains. The program can be used to create, pause, and
shutdown domains. It can also be used to list current
domains, enable <br>
or pin VCPUs, and attach or detach virtual block
devices.</p>

<p style="margin-top: 1em">The basic structure of every xl
command is almost always:</p>

<p style="margin-top: 1em">xl subcommand [OPTIONS]
domain-id</p>

<p style="margin-top: 1em">Where subcommand is one of the
subcommands listed below, domain-id is the numeric domain
id, or the domain name (which will be internally translated
to domain id), and OPTIONS <br>
are subcommand specific options. There are a few exceptions
to this rule in the cases where the subcommand in question
acts on all domains, the entire machine, or directly on <br>
the Xen hypervisor. Those exceptions will be clear for each
of those subcommands.</p>

<p style="margin-top: 1em">NOTES <br>
start the script /etc/init.d/xencommons at boot time <br>
Most xl operations rely upon xenstored and xenconsoled: make
sure you start the script /etc/init.d/xencommons at boot
time to initialize all the daemons needed by xl.</p>

<p style="margin-top: 1em">setup a xenbr0 bridge in dom0
<br>
In the most common network configuration, you need to setup
a bridge in dom0 named xenbr0 in order to have a working
network in the guest domains. Please refer to the <br>
documentation of your Linux distribution to know how to
setup the bridge.</p>

<p style="margin-top: 1em">autoballoon <br>
If you specify the amount of memory dom0 has, passing
dom0_mem to Xen, it is highly recommended to disable
autoballoon. Edit /etc/xen/xl.conf and set it to 0.</p>

<p style="margin-top: 1em">run xl as root <br>
Most xl commands require root privileges to run due to the
communications channels used to talk to the hypervisor.
Running as non root will return an error.</p>

<p style="margin-top: 1em">GLOBAL OPTIONS <br>
Some global options are always available:</p>

<p style="margin-top: 1em">-v Verbose.</p>

<p style="margin-top: 1em">-N Dry run: do not actually
execute the command.</p>

<p style="margin-top: 1em">-f Force execution: xl will
refuse to run some commands if it detects that xend is also
running, this option will force the execution of those
commands, even though it is <br>
unsafe.</p>

<p style="margin-top: 1em">-t Always use
carriage-return-based overwriting for printing progress
messages without scrolling the screen. Without -t, this is
done only if stderr is a tty.</p>

<p style="margin-top: 1em">DOMAIN SUBCOMMANDS <br>
The following subcommands manipulate domains directly. As
stated previously, most commands take domain-id as the first
parameter.</p>

<p style="margin-top: 1em">button-press domain-id button
<br>
This command is deprecated. Please use &quot;xl
trigger&quot; in preference</p>

<p style="margin-top: 1em">Indicate an ACPI button press to
the domain. button is may be &rsquo;power&rsquo; or
&rsquo;sleep&rsquo;. This command is only available for HVM
domains.</p>

<p style="margin-top: 1em">create [configfile] [OPTIONS]
<br>
The create subcommand takes a config file as first argument:
see xl.cfg for full details of that file format and possible
options. If configfile is missing XL creates the <br>
domain starting from the default value for every option.</p>

<p style="margin-top: 1em">configfile has to be an absolute
path to a file.</p>

<p style="margin-top: 1em">Create will return as soon as
the domain is started. This does not mean the guest OS in
the domain has actually booted, or is available for
input.</p>

<p style="margin-top: 1em">If the -F option is specified,
create will start the domain and not return until its
death.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-q, --quiet <br>
No console output.</p>

<p style="margin-top: 1em">-f=FILE, --defconfig=FILE <br>
Use the given configuration file.</p>

<p style="margin-top: 1em">-p Leave the domain paused after
it is created.</p>

<p style="margin-top: 1em">-F Run in foreground until death
of the domain.</p>

<p style="margin-top: 1em">-V, --vncviewer <br>
Attach to domain&rsquo;s VNC server, forking a vncviewer
process.</p>

<p style="margin-top: 1em">-A, --vncviewer-autopass <br>
Pass VNC password to vncviewer via stdin.</p>

<p style="margin-top: 1em">-c Attach console to the domain
as soon as it has started. This is useful for determining
issues with crashing domains and just as a general
convenience since you often <br>
want to watch the domain boot.</p>

<p style="margin-top: 1em">key=value <br>
It is possible to pass key=value pairs on the command line
to provide options as if they were written in the
configuration file; these override whatever is in the <br>
configfile.</p>

<p style="margin-top: 1em">NB: Many config options require
characters such as quotes or brackets which are interpreted
by the shell (and often discarded) before being passed to
xl, resulting in xl <br>
being unable to parse the value correctly. A simple
work-around is to put all extra options within a single set
of quotes, separated by semicolons. (See below for an <br>
example.)</p>

<p style="margin-top: 1em">EXAMPLES</p>

<p style="margin-top: 1em">with config file <br>
xl create DebianLenny</p>

<p style="margin-top: 1em">This creates a domain with the
file /etc/xen/DebianLenny, and returns as soon as it is
run.</p>

<p style="margin-top: 1em">with extra parameters <br>
xl create hvm.cfg &rsquo;cpus=&quot;0-3&quot;;
pci=[&quot;01:05.1&quot;,&quot;01:05.2&quot;]&rsquo;</p>

<p style="margin-top: 1em">This creates a domain with the
file hvm.cfg, but additionally pins it to cpus 0-3, and
passes through two PCI devices.</p>

<p style="margin-top: 1em">config-update domid [configfile]
[OPTIONS] <br>
Update the saved configuration for a running domain. This
has no immediate effect but will be applied when the guest
is next restarted. This command is useful to ensure that
<br>
runtime modifications made to the guest will be preserved
when the guest is restarted.</p>

<p style="margin-top: 1em">Since Xen 4.5 xl has improved
capabilities to handle dynamic domain configuration changes
and will preserve any changes made a runtime when necessary.
Therefore it should not <br>
normally be necessary to use this command any more.</p>

<p style="margin-top: 1em">configfile has to be an absolute
path to a file.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-f=FILE, --defconfig=FILE <br>
Use the given configuration file.</p>

<p style="margin-top: 1em">key=value <br>
It is possible to pass key=value pairs on the command line
to provide options as if they were written in the
configuration file; these override whatever is in the <br>
configfile. Please see the note under create on handling
special characters when passing key=value pairs on the
command line.</p>

<p style="margin-top: 1em">console [OPTIONS] domain-id <br>
Attach to domain domain-id&rsquo;s console. If you&rsquo;ve
set up your domains to have a traditional log in console
this will look much like a normal text log in screen.</p>

<p style="margin-top: 1em">Use the key combination Ctrl+]
to detach the domain console.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-t [pv|serial] <br>
Connect to a PV console or connect to an emulated serial
console. PV consoles are the only consoles available for PV
domains while HVM domains can have both. If this <br>
option is not specified it defaults to emulated serial for
HVM guests and PV console for PV guests.</p>

<p style="margin-top: 1em">-n NUM <br>
Connect to console number NUM. Console numbers start from
0.</p>

<p style="margin-top: 1em">destroy [OPTIONS] domain-id <br>
Immediately terminate the domain domain-id. This
doesn&rsquo;t give the domain OS any chance to react, and is
the equivalent of ripping the power cord out on a physical
machine. <br>
In most cases you will want to use the shutdown command
instead.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-f Allow domain 0 to be
destroyed. Because domain cannot destroy itself, this is
only possible when using a disaggregated toolstack, and is
most useful when using a <br>
hardware domain separated from domain 0.</p>

<p style="margin-top: 1em">domid domain-name <br>
Converts a domain name to a domain id.</p>

<p style="margin-top: 1em">domname domain-id <br>
Converts a domain id to a domain name.</p>

<p style="margin-top: 1em">rename domain-id new-name <br>
Change the domain name of domain-id to new-name.</p>

<p style="margin-top: 1em">dump-core domain-id [filename]
<br>
Dumps the virtual machine&rsquo;s memory for the specified
domain to the filename specified, without pausing the
domain. The dump file will be written to a distribution
specific <br>
directory for dump files. Such as:
/var/lib/xen/dump/dump.</p>

<p style="margin-top: 1em">help [--long] <br>
Displays the short help message (i.e. common commands).</p>

<p style="margin-top: 1em">The --long option prints out the
complete set of xl subcommands, grouped by function.</p>

<p style="margin-top: 1em">list [OPTIONS] [domain-id ...]
<br>
Prints information about one or more domains. If no domains
are specified it prints out information about all
domains.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-l, --long <br>
The output for xl list is not the table view shown below,
but instead presents the data in as a JSON data
structure.</p>

<p style="margin-top: 1em">-Z, --context Also prints the
security labels. <br>
-v, --verbose <br>
Also prints the domain UUIDs, the shutdown reason and
security labels.</p>

<p style="margin-top: 1em">-c, &lt;--cpupool&gt; <br>
Also prints the cpupool the domain belong to.</p>

<p style="margin-top: 1em">-n, &lt;--numa&gt; <br>
Also prints the domain NUMA node affinity.</p>

<p style="margin-top: 1em">EXAMPLE</p>

<p style="margin-top: 1em">An example format for the list
is as follows:</p>

<p style="margin-top: 1em">Name ID Mem VCPUs State Time(s)
<br>
Domain-0 0 750 4 r----- 11794.3 <br>
win 1 1019 1 r----- 0.3 <br>
linux 2 2048 2 r----- 5624.2</p>

<p style="margin-top: 1em">Name is the name of the domain.
ID the numeric domain id. Mem is the desired amount of
memory to allocate to the domain (although it may not be the
currently allocated <br>
amount). VCPUs is the number of virtual CPUs allocated to
the domain. State is the run state (see below). Time is the
total run time of the domain as accounted for by Xen.</p>

<p style="margin-top: 1em">STATES</p>

<p style="margin-top: 1em">The State field lists 6 states
for a Xen domain, and which ones the current domain is
in.</p>

<p style="margin-top: 1em">r - running <br>
The domain is currently running on a CPU.</p>

<p style="margin-top: 1em">b - blocked <br>
The domain is blocked, and not running or runnable. This can
be caused because the domain is waiting on IO (a traditional
wait state) or has gone to sleep because there <br>
was nothing else for it to do.</p>

<p style="margin-top: 1em">p - paused <br>
The domain has been paused, usually occurring through the
administrator running xl pause. When in a paused state the
domain will still consume allocated resources like <br>
memory, but will not be eligible for scheduling by the Xen
hypervisor.</p>

<p style="margin-top: 1em">s - shutdown <br>
The guest OS has shut down (SCHEDOP_shutdown has been
called) but the domain is not dying yet.</p>

<p style="margin-top: 1em">c - crashed <br>
The domain has crashed, which is always a violent ending.
Usually this state can only occur if the domain has been
configured not to restart on crash. See xl.cfg(5) for <br>
more info.</p>

<p style="margin-top: 1em">d - dying <br>
The domain is in process of dying, but hasn&rsquo;t
completely shutdown or crashed.</p>

<p style="margin-top: 1em">NOTES</p>

<p style="margin-top: 1em">The Time column is deceptive.
Virtual IO (network and block devices) used by domains
requires coordination by Domain0, which means that Domain0
is actually charged for <br>
much of the time that a DomainU is doing IO. Use of this
time value to determine relative utilizations by domains is
thus very suspect, as a high IO workload may show as <br>
less utilized than a high CPU workload. Consider yourself
warned.</p>

<p style="margin-top: 1em">mem-max domain-id mem <br>
Specify the maximum amount of memory the domain is able to
use, appending &rsquo;t&rsquo; for terabytes,
&rsquo;g&rsquo; for gigabytes, &rsquo;m&rsquo; for
megabytes, &rsquo;k&rsquo; for kilobytes and &rsquo;b&rsquo;
for bytes.</p>

<p style="margin-top: 1em">The mem-max value may not
correspond to the actual memory used in the domain, as it
may balloon down its memory to give more back to the OS.</p>

<p style="margin-top: 1em">mem-set domain-id mem <br>
Set the domain&rsquo;s used memory using the balloon driver;
append &rsquo;t&rsquo; for terabytes, &rsquo;g&rsquo; for
gigabytes, &rsquo;m&rsquo; for megabytes, &rsquo;k&rsquo;
for kilobytes and &rsquo;b&rsquo; for bytes.</p>

<p style="margin-top: 1em">Because this operation requires
cooperation from the domain operating system, there is no
guarantee that it will succeed. This command will definitely
not work unless the <br>
domain has the required paravirt driver.</p>

<p style="margin-top: 1em">Warning: There is no good way to
know in advance how small of a mem-set will make a domain
unstable and cause it to crash. Be very careful when using
this command on running <br>
domains.</p>

<p style="margin-top: 1em">migrate [OPTIONS] domain-id host
<br>
Migrate a domain to another host machine. By default xl
relies on ssh as a transport mechanism between the two
hosts.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-s sshcommand <br>
Use &lt;sshcommand&gt; instead of ssh. String will be passed
to sh. If empty, run &lt;host&gt; instead of ssh
&lt;host&gt; xl migrate-receive [-d -e].</p>

<p style="margin-top: 1em">-e On the new host, do not wait
in the background (on &lt;host&gt;) for the death of the
domain. See the corresponding option of the create
subcommand.</p>

<p style="margin-top: 1em">-C config <br>
Send &lt;config&gt; instead of config file from
creation.</p>

<p style="margin-top: 1em">--debug <br>
Print huge (!) amount of debug during the migration
process.</p>

<p style="margin-top: 1em">-p Leave the domain on the
receive side paused after migration.</p>

<p style="margin-top: 1em">remus [OPTIONS] domain-id host
<br>
Enable Remus HA or COLO HA for domain. By default xl relies
on ssh as a transport mechanism between the two hosts.</p>

<p style="margin-top: 1em">NOTES</p>

<p style="margin-top: 1em">Remus support in xl is still in
experimental (proof-of-concept) phase. Disk replication
support is limited to DRBD disks.</p>

<p style="margin-top: 1em">COLO support in xl is still in
experimental (proof-of-concept) phase. All options are
subject to change in the future.</p>

<p style="margin-top: 1em">COLO disk configuration looks
like:</p>

<p style="margin-top: 1em">disk =
[&rsquo;...,colo,colo-host=xxx,colo-port=xxx,colo-export=xxx,active-disk=xxx,hidden-disk=xxx...&rsquo;]</p>

<p style="margin-top: 1em">The supported options are:</p>

<p style="margin-top: 1em">colo-host :Secondary
host&rsquo;s ip address. <br>
colo-port :Secondary host&rsquo;s port, we will run a nbd
server on secondary host, and the nbd server will listen
this port. <br>
colo-export :Nbd server&rsquo;s disk export name of
secondary host. <br>
active-disk :Secondary&rsquo;s guest write will be buffered
in this disk, and it&rsquo;s used by secondary. <br>
hidden-disk :Primary&rsquo;s modified contents will be
buffered in this disk, and it&rsquo;s used by secondary.</p>

<p style="margin-top: 1em">COLO network configuration looks
like:</p>

<p style="margin-top: 1em">vif = [
&rsquo;...,forwarddev=xxx,...&rsquo;]</p>

<p style="margin-top: 1em">The supported options are:</p>

<p style="margin-top: 1em">forwarddev :Forward devices for
primary and secondary, they are directly connected.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-i MS <br>
Checkpoint domain memory every MS milliseconds (default
200ms).</p>

<p style="margin-top: 1em">-u Disable memory checkpoint
compression.</p>

<p style="margin-top: 1em">-s sshcommand <br>
Use &lt;sshcommand&gt; instead of ssh. String will be passed
to sh. If empty, run &lt;host&gt; instead of ssh
&lt;host&gt; xl migrate-receive -r [-e].</p>

<p style="margin-top: 1em">-e On the new host, do not wait
in the background (on &lt;host&gt;) for the death of the
domain. See the corresponding option of the create
subcommand.</p>

<p style="margin-top: 1em">-N netbufscript <br>
Use &lt;netbufscript&gt; to setup network buffering instead
of the default script
(/etc/xen/scripts/remus-netbuf-setup).</p>

<p style="margin-top: 1em">-F Run Remus in unsafe mode. Use
this option with caution as failover may not work as
intended.</p>

<p style="margin-top: 1em">-b Replicate memory checkpoints
to /dev/null (blackhole). Generally useful for debugging.
Requires enabling unsafe mode.</p>

<p style="margin-top: 1em">-n Disable network output
buffering. Requires enabling unsafe mode.</p>

<p style="margin-top: 1em">-d Disable disk replication.
Requires enabling unsafe mode.</p>

<p style="margin-top: 1em">-c Enable COLO HA. This
conflicts with -i and -b, and memory checkpoint compression
must be disabled.</p>

<p style="margin-top: 1em">pause domain-id <br>
Pause a domain. When in a paused state the domain will still
consume allocated resources such as memory, but will not be
eligible for scheduling by the Xen hypervisor.</p>

<p style="margin-top: 1em">reboot [OPTIONS] domain-id <br>
Reboot a domain. This acts just as if the domain had the
reboot command run from the console. The command returns as
soon as it has executed the reboot action, which may be <br>
significantly before the domain actually reboots.</p>

<p style="margin-top: 1em">For HVM domains this requires PV
drivers to be installed in your guest OS. If PV drivers are
not present but you have configured the guest OS to behave
appropriately you may <br>
be able to use the -F option trigger a reset button
press.</p>

<p style="margin-top: 1em">The behavior of what happens to
a domain when it reboots is set by the on_reboot parameter
of the domain configuration file when the domain was
created.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-F If the guest does not support
PV reboot control then fallback to sending an ACPI power
event (equivalent to the reset option to trigger.</p>

<p style="margin-top: 1em">You should ensure that the guest
is configured to behave as expected in response to this
event.</p>

<p style="margin-top: 1em">restore [OPTIONS] [ConfigFile]
CheckpointFile <br>
Build a domain from an xl save state file. See save for more
info.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-p Do not unpause domain after
restoring it.</p>

<p style="margin-top: 1em">-e Do not wait in the background
for the death of the domain on the new host. See the
corresponding option of the create subcommand.</p>

<p style="margin-top: 1em">-d Enable debug messages.</p>

<p style="margin-top: 1em">-V, --vncviewer <br>
Attach to domain&rsquo;s VNC server, forking a vncviewer
process.</p>

<p style="margin-top: 1em">-A, --vncviewer-autopass <br>
Pass VNC password to vncviewer via stdin.</p>

<p style="margin-top: 1em">save [OPTIONS] domain-id
CheckpointFile [ConfigFile] <br>
Saves a running domain to a state file so that it can be
restored later. Once saved, the domain will no longer be
running on the system, unless the -c or -p options are <br>
used. xl restore restores from this checkpoint file. Passing
a config file argument allows the user to manually select
the VM config file used to create the domain.</p>

<p style="margin-top: 1em">-c Leave domain running after
creating the snapshot.</p>

<p style="margin-top: 1em">-p Leave domain paused after
creating the snapshot.</p>

<p style="margin-top: 1em">sharing [domain-id] <br>
List count of shared pages.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">domain_id <br>
List specifically for that domain. Otherwise, list for all
domains.</p>

<p style="margin-top: 1em">shutdown [OPTIONS] -a|domain-id
<br>
Gracefully shuts down a domain. This coordinates with the
domain OS to perform graceful shutdown, so there is no
guarantee that it will succeed, and may take a variable <br>
length of time depending on what services must be shutdown
in the domain.</p>

<p style="margin-top: 1em">For HVM domains this requires PV
drivers to be installed in your guest OS. If PV drivers are
not present but you have configured the guest OS to behave
appropriately you may <br>
be able to use the -F option trigger a power button
press.</p>

<p style="margin-top: 1em">The command returns immediately
after signally the domain unless that -w flag is used.</p>

<p style="margin-top: 1em">The behavior of what happens to
a domain when it reboots is set by the on_shutdown parameter
of the domain configuration file when the domain was
created.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-a, --all <br>
Shutdown all guest domains. Often used when doing a complete
shutdown of a Xen system.</p>

<p style="margin-top: 1em">-w, --wait <br>
Wait for the domain to complete shutdown before
returning.</p>

<p style="margin-top: 1em">-F If the guest does not support
PV shutdown control then fallback to sending an ACPI power
event (equivalent to the power option to trigger.</p>

<p style="margin-top: 1em">You should ensure that the guest
is configured to behave as expected in response to this
event.</p>

<p style="margin-top: 1em">sysrq domain-id letter <br>
Send a &lt;Magic System Request&gt; to the domain, each type
of request is represented by a different letter. It can be
used to send SysRq requests to Linux guests, see sysrq.txt
<br>
in your Linux Kernel sources for more information. It
requires PV drivers to be installed in your guest OS.</p>

<p style="margin-top: 1em">trigger domain-id
nmi|reset|init|power|sleep|s3resume [VCPU] <br>
Send a trigger to a domain, where the trigger can be: nmi,
reset, init, power or sleep. Optionally a specific vcpu
number can be passed as an argument. This command is only
<br>
available for HVM domains.</p>

<p style="margin-top: 1em">unpause domain-id <br>
Moves a domain out of the paused state. This will allow a
previously paused domain to now be eligible for scheduling
by the Xen hypervisor.</p>

<p style="margin-top: 1em">vcpu-set domain-id vcpu-count
<br>
Enables the vcpu-count virtual CPUs for the domain in
question. Like mem-set, this command can only allocate up to
the maximum virtual CPU count configured at boot for the
<br>
domain.</p>

<p style="margin-top: 1em">If the vcpu-count is smaller
than the current number of active VCPUs, the highest number
VCPUs will be hotplug removed. This may be important for
pinning purposes.</p>

<p style="margin-top: 1em">Attempting to set the VCPUs to a
number larger than the initially configured VCPU count is an
error. Trying to set VCPUs to &lt; 1 will be quietly
ignored.</p>

<p style="margin-top: 1em">Some guests may need to actually
bring the newly added CPU online after vcpu-set, go to SEE
ALSO section for information.</p>

<p style="margin-top: 1em">vcpu-list [domain-id] <br>
Lists VCPU information for a specific domain. If no domain
is specified, VCPU information for all domains will be
provided.</p>

<p style="margin-top: 1em">vcpu-pin [-f|--force] domain-id
vcpu cpus hard cpus soft <br>
Set hard and soft affinity for a vcpu of &lt;domain-id&gt;.
Normally VCPUs can float between available CPUs whenever Xen
deems a different run state is appropriate.</p>

<p style="margin-top: 1em">Hard affinity can be used to
restrict this, by ensuring certain VCPUs can only run on
certain physical CPUs. Soft affinity specifies a preferred
set of CPUs. Soft affinity <br>
needs special support in the scheduler, which is only
provided in credit1.</p>

<p style="margin-top: 1em">The keyword all can be used to
apply the hard and soft affinity masks to all the VCPUs in
the domain. The symbol &rsquo;-&rsquo; can be used to leave
either hard or soft affinity alone.</p>

<p style="margin-top: 1em">For example:</p>

<p style="margin-top: 1em">xl vcpu-pin 0 3 - 6-9</p>

<p style="margin-top: 1em">will set soft affinity for vCPU
3 of domain 0 to pCPUs 6,7,8 and 9, leaving its hard
affinity untouched. On the othe hand:</p>

<p style="margin-top: 1em">xl vcpu-pin 0 3 3,4 6-9</p>

<p style="margin-top: 1em">will set both hard and soft
affinity, the former to pCPUs 3 and 4, the latter to pCPUs
6,7,8, and 9.</p>

<p style="margin-top: 1em">Specifying -f or --force will
remove a temporary pinning done by the operating system
(normally this should be done by the operating system). In
case a temporary pinning is <br>
active for a vcpu the affinity of this vcpu can&rsquo;t be
changed without this option.</p>

<p style="margin-top: 1em">vm-list <br>
Prints information about guests. This list excludes
information about service or auxiliary domains such as dom0
and stubdoms.</p>

<p style="margin-top: 1em">EXAMPLE</p>

<p style="margin-top: 1em">An example format for the list
is as follows:</p>

<p style="margin-top: 1em">UUID ID name <br>
59e1cf6c-6ab9-4879-90e7-adc8d1c63bf5 2 win <br>
50bc8f75-81d0-4d53-b2e6-95cb44e2682e 3 linux</p>

<p style="margin-top: 1em">vncviewer [OPTIONS] domain-id
<br>
Attach to domain&rsquo;s VNC server, forking a vncviewer
process.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">--autopass <br>
Pass VNC password to vncviewer via stdin.</p>

<p style="margin-top: 1em">XEN HOST SUBCOMMANDS <br>
debug-keys keys <br>
Send debug keys to Xen. It is the same as pressing the Xen
&quot;conswitch&quot; (Ctrl-A by default) three times and
then pressing &quot;keys&quot;.</p>

<p style="margin-top: 1em">dmesg [-c] <br>
Reads the Xen message buffer, similar to dmesg on a Linux
system. The buffer contains informational, warning, and
error messages created during Xen&rsquo;s boot process. If
you <br>
are having problems with Xen, this is one of the first
places to look as part of problem determination.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-c, --clear <br>
Clears Xen&rsquo;s message buffer.</p>

<p style="margin-top: 1em">info [-n, --numa] <br>
Print information about the Xen host in name : value format.
When reporting a Xen bug, please provide this information as
part of the bug report. See <br>
http://wiki.xen.org/xenwiki/ReportingBugs on how to report
Xen bugs.</p>

<p style="margin-top: 1em">Sample output looks as
follows:</p>

<p style="margin-top: 1em">host : scarlett <br>
release : 3.1.0-rc4+ <br>
version : #1001 SMP Wed Oct 19 11:09:54 UTC 2011 <br>
machine : x86_64 <br>
nr_cpus : 4 <br>
nr_nodes : 1 <br>
cores_per_socket : 4 <br>
threads_per_core : 1 <br>
cpu_mhz : 2266 <br>
hw_caps :
bfebfbff:28100800:00000000:00003b40:009ce3bd:00000000:00000001:00000000
<br>
virt_caps : hvm hvm_directio <br>
total_memory : 6141 <br>
free_memory : 4274 <br>
free_cpus : 0 <br>
outstanding_claims : 0 <br>
xen_major : 4 <br>
xen_minor : 2 <br>
xen_extra : -unstable <br>
xen_caps : xen-3.0-x86_64 xen-3.0-x86_32p hvm-3.0-x86_32
hvm-3.0-x86_32p hvm-3.0-x86_64 <br>
xen_scheduler : credit <br>
xen_pagesize : 4096 <br>
platform_params : virt_start=0xffff800000000000 <br>
xen_changeset : Wed Nov 02 17:09:09 2011 +0000
24066:54a5e994a241 <br>
xen_commandline : com1=115200,8n1 guest_loglvl=all
dom0_mem=750M console=com1 <br>
cc_compiler : gcc version 4.4.5 (Debian 4.4.5-8) <br>
cc_compile_by : sstabellini <br>
cc_compile_domain : uk.xensource.com <br>
cc_compile_date : Tue Nov 8 12:03:05 UTC 2011 <br>
xend_config_format : 4</p>

<p style="margin-top: 1em">FIELDS</p>

<p style="margin-top: 1em">Not all fields will be explained
here, but some of the less obvious ones deserve
explanation:</p>

<p style="margin-top: 1em">hw_caps <br>
A vector showing what hardware capabilities are supported by
your processor. This is equivalent to, though more cryptic,
the flags field in /proc/cpuinfo on a normal <br>
Linux machine: they both derive from the feature bits
returned by the cpuid command on x86 platforms.</p>

<p style="margin-top: 1em">free_memory <br>
Available memory (in MB) not allocated to Xen, or any other
domains, or claimed for domains.</p>

<p style="margin-top: 1em">outstanding_claims <br>
When a claim call is done (see xl.conf) a reservation for a
specific amount of pages is set and also a global value is
incremented. This global value (outstanding_claims) <br>
is then reduced as the domain&rsquo;s memory is populated
and eventually reaches zero. Most of the time the value will
be zero, but if you are launching multiple guests, and <br>
claim_mode is enabled, this value can increase/decrease.
Note that the value also affects the free_memory - as it
will reflect the free memory in the hypervisor minus <br>
the outstanding pages claimed for guests. See xl info claims
parameter for detailed listing.</p>

<p style="margin-top: 1em">xen_caps <br>
The Xen version and architecture. Architecture values can be
one of: x86_32, x86_32p (i.e. PAE enabled), x86_64,
ia64.</p>

<p style="margin-top: 1em">xen_changeset <br>
The Xen mercurial changeset id. Very useful for determining
exactly what version of code your Xen system was built
from.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-n, --numa <br>
List host NUMA topology information</p>

<p style="margin-top: 1em">top Executes the xentop command,
which provides real time monitoring of domains. Xentop is a
curses interface, and reasonably self explanatory.</p>

<p style="margin-top: 1em">uptime <br>
Prints the current uptime of the domains running.</p>

<p style="margin-top: 1em">claims <br>
Prints information about outstanding claims by the guests.
This provides the outstanding claims and currently populated
memory count for the guests. These values added up <br>
reflect the global outstanding claim value, which is
provided via the info argument, outstanding_claims value.
The Mem column has the cumulative value of outstanding
claims <br>
and the total amount of memory that has been right now
allocated to the guest.</p>

<p style="margin-top: 1em">EXAMPLE</p>

<p style="margin-top: 1em">An example format for the list
is as follows:</p>

<p style="margin-top: 1em">Name ID Mem VCPUs State Time(s)
Claimed <br>
Domain-0 0 2047 4 r----- 19.7 0 <br>
OL5 2 2048 1 --p--- 0.0 847 <br>
OL6 3 1024 4 r----- 5.9 0 <br>
Windows_XP 4 2047 1 --p--- 0.0 1989</p>

<p style="margin-top: 1em">In which it can be seen that the
OL5 guest still has 847MB of claimed memory (out of the
total 2048MB where 1191MB has been allocated to the
guest).</p>

<p style="margin-top: 1em">SCHEDULER SUBCOMMANDS <br>
Xen ships with a number of domain schedulers, which can be
set at boot time with the sched= parameter on the Xen
command line. By default credit is used for scheduling.</p>

<p style="margin-top: 1em">sched-credit [OPTIONS] <br>
Set or get credit scheduler parameters. The credit scheduler
is a proportional fair share CPU scheduler built from the
ground up to be work conserving on SMP hosts.</p>

<p style="margin-top: 1em">Each domain (including Domain0)
is assigned a weight and a cap.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-d DOMAIN, --domain=DOMAIN <br>
Specify domain for which scheduler parameters are to be
modified or retrieved. Mandatory for modifying scheduler
parameters.</p>

<p style="margin-top: 1em">-w WEIGHT, --weight=WEIGHT <br>
A domain with a weight of 512 will get twice as much CPU as
a domain with a weight of 256 on a contended host. Legal
weights range from 1 to 65535 and the default is 256.</p>

<p style="margin-top: 1em">-c CAP, --cap=CAP <br>
The cap optionally fixes the maximum amount of CPU a domain
will be able to consume, even if the host system has idle
CPU cycles. The cap is expressed in percentage of <br>
one physical CPU: 100 is 1 physical CPU, 50 is half a CPU,
400 is 4 CPUs, etc. The default, 0, means there is no upper
cap.</p>

<p style="margin-top: 1em">NB: Many systems have features
that will scale down the computing power of a cpu that is
not 100% utilized. This can be in the operating system, but
can also sometimes <br>
be below the operating system in the BIOS. If you set a cap
such that individual cores are running at less than 100%,
this may have an impact on the performance of your <br>
workload over and above the impact of the cap. For example,
if your processor runs at 2GHz, and you cap a vm at 50%, the
power management system may also reduce the clock <br>
speed to 1GHz; the effect will be that your VM gets 25% of
the available power (50% of 1GHz) rather than 50% (50% of
2GHz). If you are not getting the performance you <br>
expect, look at performance and cpufreq options in your
operating system and your BIOS.</p>

<p style="margin-top: 1em">-p CPUPOOL, --cpupool=CPUPOOL
<br>
Restrict output to domains in the specified cpupool.</p>

<p style="margin-top: 1em">-s, --schedparam <br>
Specify to list or set pool-wide scheduler parameters.</p>

<p style="margin-top: 1em">-t TSLICE, --tslice_ms=TSLICE
<br>
Timeslice tells the scheduler how long to allow VMs to run
before pre-empting. The default is 30ms. Valid ranges are
1ms to 1000ms. The length of the timeslice (in ms) <br>
must be higher than the length of the ratelimit (see
below).</p>

<p style="margin-top: 1em">-r RLIMIT, --ratelimit_us=RLIMIT
<br>
Ratelimit attempts to limit the number of schedules per
second. It sets a minimum amount of time (in microseconds) a
VM must run before we will allow a higher-priority <br>
VM to pre-empt it. The default value is 1000 microseconds
(1ms). Valid range is 100 to 500000 (500ms). The ratelimit
length must be lower than the timeslice length.</p>

<p style="margin-top: 1em">COMBINATION</p>

<p style="margin-top: 1em">The following is the effect of
combining the above options:</p>

<p style="margin-top: 1em">&lt;nothing&gt; : List all
domain params and sched params from all pools <br>
-d [domid] : List domain params for domain [domid] <br>
-d [domid] [params] : Set domain params for domain [domid]
<br>
-p [pool] : list all domains and sched params for [pool]
<br>
-s : List sched params for poolid 0 <br>
-s [params] : Set sched params for poolid 0 <br>
-p [pool] -s : List sched params for [pool] <br>
-p [pool] -s [params] : Set sched params for [pool] <br>
-p [pool] -d... : Illegal <br>
sched-credit2 [OPTIONS] <br>
Set or get credit2 scheduler parameters. The credit2
scheduler is a proportional fair share CPU scheduler built
from the ground up to be work conserving on SMP hosts.</p>

<p style="margin-top: 1em">Each domain (including Domain0)
is assigned a weight.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-d DOMAIN, --domain=DOMAIN <br>
Specify domain for which scheduler parameters are to be
modified or retrieved. Mandatory for modifying scheduler
parameters.</p>

<p style="margin-top: 1em">-w WEIGHT, --weight=WEIGHT <br>
A domain with a weight of 512 will get twice as much CPU as
a domain with a weight of 256 on a contended host. Legal
weights range from 1 to 65535 and the default is 256.</p>

<p style="margin-top: 1em">-p CPUPOOL, --cpupool=CPUPOOL
<br>
Restrict output to domains in the specified cpupool.</p>

<p style="margin-top: 1em">-s, --schedparam <br>
Specify to list or set pool-wide scheduler parameters.</p>

<p style="margin-top: 1em">-r RLIMIT, --ratelimit_us=RLIMIT
<br>
Attempts to limit the rate of context switching. It is
basically the same as --ratelimit_us in sched-credit</p>

<p style="margin-top: 1em">sched-rtds [OPTIONS] <br>
Set or get rtds (Real Time Deferrable Server) scheduler
parameters. This rt scheduler applies Preemptive Global
Earliest Deadline First real-time scheduling algorithm to
<br>
schedule VCPUs in the system. Each VCPU has a dedicated
period and budget. VCPUs in the same domain have the same
period and budget. While scheduled, a VCPU burns its <br>
budget. A VCPU has its budget replenished at the beginning
of each period; Unused budget is discarded at the end of
each period.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-d DOMAIN, --domain=DOMAIN <br>
Specify domain for which scheduler parameters are to be
modified or retrieved. Mandatory for modifying scheduler
parameters.</p>

<p style="margin-top: 1em">-v VCPUID/all,
--vcpuid=VCPUID/all <br>
Specify vcpu for which scheduler parameters are to be
modified or retrieved.</p>

<p style="margin-top: 1em">-p PERIOD, --period=PERIOD <br>
Period of time, in microseconds, over which to replenish the
budget.</p>

<p style="margin-top: 1em">-b BUDGET, --budget=BUDGET <br>
Amount of time, in microseconds, that the VCPU will be
allowed to run every period.</p>

<p style="margin-top: 1em">-c CPUPOOL, --cpupool=CPUPOOL
<br>
Restrict output to domains in the specified cpupool.</p>

<p style="margin-top: 1em">EXAMPLE</p>

<p style="margin-top: 1em">1) Use -v all to see the budget
and period of all the VCPUs of all the domains:</p>

<p style="margin-top: 1em">xl sched-rtds -v all <br>
Cpupool Pool-0: sched=RTDS <br>
Name ID VCPU Period Budget <br>
Domain-0 0 0 10000 4000 <br>
vm1 1 0 300 150 <br>
vm1 1 1 400 200 <br>
vm1 1 2 10000 4000 <br>
vm1 1 3 1000 500 <br>
vm2 2 0 10000 4000 <br>
vm2 2 1 10000 4000</p>

<p style="margin-top: 1em">Without any arguments, it will
output the default scheduing parameters for each domain:</p>

<p style="margin-top: 1em">xl sched-rtds <br>
Cpupool Pool-0: sched=RTDS <br>
Name ID Period Budget <br>
Domain-0 0 10000 4000 <br>
vm1 1 10000 4000 <br>
vm2 2 10000 4000</p>

<p style="margin-top: 1em">2) Use, for instance -d vm1, -v
all to see the budget and period of all VCPUs of a specific
domain (vm1):</p>

<p style="margin-top: 1em">xl sched-rtds -d vm1 -v all <br>
Name ID VCPU Period Budget <br>
vm1 1 0 300 150 <br>
vm1 1 1 400 200 <br>
vm1 1 2 10000 4000 <br>
vm1 1 3 1000 500</p>

<p style="margin-top: 1em">To see the parameters of a
subset of the VCPUs of a domain, use:</p>

<p style="margin-top: 1em">xl sched-rtds -d vm1 -v 0 -v 3
<br>
Name ID VCPU Period Budget <br>
vm1 1 0 300 150 <br>
vm1 1 3 1000 500</p>

<p style="margin-top: 1em">If no -v is speficified, the
default scheduling parameter for the domain are shown:</p>

<p style="margin-top: 1em">xl sched-rtds -d vm1 <br>
Name ID Period Budget <br>
vm1 1 10000 4000</p>

<p style="margin-top: 1em">3) Users can set the budget and
period of multiple VCPUs of a specific domain with only one
command, e.g., &quot;xl sched-rtds -d vm1 -v 0 -p 100 -b 50
-v 3 -p 300 -b 150&quot;.</p>

<p style="margin-top: 1em">To change the parameters of all
the VCPUs of a domain, use -v all, e.g., &quot;xl sched-rtds
-d vm1 -v all -p 500 -b 250&quot;.</p>

<p style="margin-top: 1em">CPUPOOLS COMMANDS <br>
Xen can group the physical cpus of a server in cpu-pools.
Each physical CPU is assigned at most to one cpu-pool.
Domains are each restricted to a single cpu-pool. Scheduling
does <br>
not cross cpu-pool boundaries, so each cpu-pool has an own
scheduler. Physical cpus and domains can be moved from one
cpu-pool to another only by an explicit command. Cpu-pools
<br>
can be specified either by name or by id.</p>

<p style="margin-top: 1em">cpupool-create [OPTIONS]
[ConfigFile] [Variable=Value ...] <br>
Create a cpu pool based an config from a ConfigFile or
command-line parameters. Variable settings from the
ConfigFile may be altered by specifying new or additional
<br>
assignments on the command line.</p>

<p style="margin-top: 1em">See the xlcpupool.cfg(5) manpage
for more information.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-f=FILE, --defconfig=FILE <br>
Use the given configuration file.</p>

<p style="margin-top: 1em">cpupool-list [-c|--cpus]
[cpu-pool] <br>
List CPU pools on the host. If -c is specified, xl prints a
list of CPUs used by cpu-pool.</p>

<p style="margin-top: 1em">cpupool-destroy cpu-pool <br>
Deactivates a cpu pool. This is possible only if no domain
is active in the cpu-pool.</p>

<p style="margin-top: 1em">cpupool-rename cpu-pool
&lt;newname&gt; <br>
Renames a cpu-pool to newname.</p>

<p style="margin-top: 1em">cpupool-cpu-add cpu-pool
cpus|node:nodes <br>
Adds one or more CPUs or NUMA nodes to cpu-pool. CPUs and
NUMA nodes can be specified as single CPU/node IDs or as
ranges.</p>

<p style="margin-top: 1em">For example:</p>

<p style="margin-top: 1em">(a) xl cpupool-cpu-add mypool 4
<br>
(b) xl cpupool-cpu-add mypool 1,5,10-16,^13 <br>
(c) xl cpupool-cpu-add mypool node:0,nodes:2-3,^10-12,8</p>

<p style="margin-top: 1em">means adding CPU 4 to mypool, in
(a); adding CPUs 1,5,10,11,12,14,15 and 16, in (b); and
adding all the CPUs of NUMA nodes 0, 2 and 3, plus CPU 8,
but keeping out CPUs <br>
10,11,12, in (c).</p>

<p style="margin-top: 1em">All the specified CPUs that can
be added to the cpupool will be added to it. If some CPU
can&rsquo;t (e.g., because they&rsquo;re already part of
another cpupool), an error is reported <br>
about each one of them.</p>

<p style="margin-top: 1em">cpupool-cpu-remove
cpus|node:nodes <br>
Removes one or more CPUs or NUMA nodes from cpu-pool. CPUs
and NUMA nodes can be specified as single CPU/node IDs or as
ranges, using the exact same syntax as in cpupool-cpu- <br>
add above.</p>

<p style="margin-top: 1em">cpupool-migrate domain cpu-pool
<br>
Moves a domain specified by domain-id or domain-name into a
cpu-pool. Domain-0 can&rsquo;t be moved to another
cpu-pool.</p>

<p style="margin-top: 1em">cpupool-numa-split <br>
Splits up the machine into one cpu-pool per numa node.</p>

<p style="margin-top: 1em">VIRTUAL DEVICE COMMANDS <br>
Most virtual devices can be added and removed while guests
are running, assuming that the necessary support exists in
the guest. The effect to the guest OS is much the same as
<br>
any hotplug event.</p>

<p style="margin-top: 1em">BLOCK DEVICES <br>
block-attach domain-id disc-spec-component(s) ... <br>
Create a new virtual block device. This will trigger a
hotplug event for the guest.</p>

<p style="margin-top: 1em">Note that only PV block devices
are supported by block-attach. Requests to attach emulated
devices (eg, vdev=hdc) will result in only the PV view being
available to the <br>
guest.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">domain-id <br>
The domain id of the guest domain that the device will be
attached to.</p>

<p style="margin-top: 1em">disc-spec-component <br>
A disc specification in the same format used for the disk
variable in the domain config file. See
&lt;http://xenbits.xen.org/docs/unstable/misc/xl-disk-configuration.txt&gt;.</p>

<p style="margin-top: 1em">block-detach domain-id devid
[--force] <br>
Detach a domain&rsquo;s virtual block device. devid may be
the symbolic name or the numeric device id given to the
device by domain 0. You will need to run xl block-list to
<br>
determine that number.</p>

<p style="margin-top: 1em">Detaching the device requires
the cooperation of the domain. If the domain fails to
release the device (perhaps because the domain is hung or is
still using the device), the <br>
detach will fail. The --force parameter will forcefully
detach the device, but may cause IO errors in the
domain.</p>

<p style="margin-top: 1em">block-list domain-id <br>
List virtual block devices for a domain.</p>

<p style="margin-top: 1em">cd-insert domain-id
VirtualDevice target <br>
Insert a cdrom into a guest domain&rsquo;s existing virtial
cd drive. The virtual drive must already exist but can be
current empty.</p>

<p style="margin-top: 1em">Only works with HVM domains.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">VirtualDevice <br>
How the device should be presented to the guest domain; for
example &quot;hdc&quot;.</p>

<p style="margin-top: 1em">target <br>
the target path in the backend domain (usually domain 0) to
be exported; Can be a block device or a file etc. See target
in docs/misc/xl-disk-configuration.txt.</p>

<p style="margin-top: 1em">cd-eject domain-id VirtualDevice
<br>
Eject a cdrom from a guest&rsquo;s virtual cd drive. Only
works with HVM domains.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">VirtualDevice <br>
How the device should be presented to the guest domain; for
example &quot;hdc&quot;.</p>

<p style="margin-top: 1em">NETWORK DEVICES <br>
network-attach domain-id network-device <br>
Creates a new network device in the domain specified by
domain-id. network-device describes the device to attach,
using the same format as the vif string in the domain <br>
config file. See xl.cfg and
&lt;http://xenbits.xen.org/docs/unstable/misc/xl-network-configuration.html&gt;
for more informations.</p>

<p style="margin-top: 1em">Note that only attaching PV
network interface is supported.</p>

<p style="margin-top: 1em">network-detach domain-id
devid|mac <br>
Removes the network device from the domain specified by
domain-id. devid is the virtual interface device number
within the domain (i.e. the 3 in vif22.3). Alternatively the
<br>
mac address can be used to select the virtual interface to
detach.</p>

<p style="margin-top: 1em">network-list domain-id <br>
List virtual network interfaces for a domain.</p>

<p style="margin-top: 1em">CHANNEL DEVICES <br>
channel-list domain-id <br>
List virtual channel interfaces for a domain.</p>

<p style="margin-top: 1em">VTPM DEVICES <br>
vtpm-attach domain-id vtpm-device <br>
Creates a new vtpm device in the domain specified by
domain-id. vtpm-device describes the device to attach, using
the same format as the vtpm string in the domain config <br>
file. See xl.cfg for more information.</p>

<p style="margin-top: 1em">vtpm-detach domain-id devid|uuid
<br>
Removes the vtpm device from the domain specified by
domain-id. devid is the numeric device id given to the
virtual trusted platform module device. You will need to run
xl <br>
vtpm-list to determine that number. Alternatively the uuid
of the vtpm can be used to select the virtual device to
detach.</p>

<p style="margin-top: 1em">vtpm-list domain-id <br>
List virtual trusted platform modules for a domain.</p>

<p style="margin-top: 1em">PCI PASS-THROUGH <br>
pci-assignable-list <br>
List all the assignable PCI devices. These are devices in
the system which are configured to be available for
passthrough and are bound to a suitable PCI backend driver
in <br>
domain 0 rather than a real driver.</p>

<p style="margin-top: 1em">pci-assignable-add BDF <br>
Make the device at PCI Bus/Device/Function BDF assignable to
guests. This will bind the device to the pciback driver. If
it is already bound to a driver, it will first be <br>
unbound, and the original driver stored so that it can be
re-bound to the same driver later if desired. If the device
is already bound, it will return success.</p>

<p style="margin-top: 1em">CAUTION: This will make the
device unusable by Domain 0 until it is returned with
pci-assignable-remove. Care should therefore be taken not to
do this on a device critical <br>
to domain 0&rsquo;s operation, such as storage controllers,
network interfaces, or GPUs that are currently being
used.</p>

<p style="margin-top: 1em">pci-assignable-remove [-r] BDF
<br>
Make the device at PCI Bus/Device/Function BDF assignable to
guests. This will at least unbind the device from pciback.
If the -r option is specified, it will also attempt <br>
to re-bind the device to its original driver, making it
usable by Domain 0 again. If the device is not bound to
pciback, it will return success.</p>

<p style="margin-top: 1em">pci-attach domain-id BDF <br>
Hot-plug a new pass-through pci device to the specified
domain. BDF is the PCI Bus/Device/Function of the physical
device to pass-through.</p>

<p style="margin-top: 1em">pci-detach [-f] domain-id BDF
<br>
Hot-unplug a previously assigned pci device from a domain.
BDF is the PCI Bus/Device/Function of the physical device to
be removed from the guest domain.</p>

<p style="margin-top: 1em">If -f is specified, xl is going
to forcefully remove the device even without guest&rsquo;s
collaboration.</p>

<p style="margin-top: 1em">pci-list domain-id <br>
List pass-through pci devices for a domain.</p>

<p style="margin-top: 1em">USB PASS-THROUGH <br>
usbctrl-attach domain-id usbctrl-device <br>
Create a new USB controller in the domain specified by
domain-id, usbctrl-device describes the device to attach,
using form &quot;KEY=VALUE KEY=VALUE ...&quot; where
KEY=VALUE has the <br>
same meaning as the usbctrl description in the domain config
file. See xl.cfg for more information.</p>

<p style="margin-top: 1em">usbctrl-detach domain-id devid
<br>
Destroy a USB controller from the specified domain. devid is
devid of the USB controller.</p>

<p style="margin-top: 1em">usbdev-attach domain-id
usbdev-device <br>
Hot-plug a new pass-through USB device to the domain
specified by domain-id, usbdev-device describes the device
to attach, using form &quot;KEY=VALUE KEY=VALUE ...&quot;
where <br>
KEY=VALUE has the same meaning as the usbdev description in
the domain config file. See xl.cfg for more information.</p>

<p style="margin-top: 1em">usbdev-detach domain-id
controller=devid port=number <br>
Hot-unplug a previously assigned USB device from a domain.
controller=devid and port=number is USB controller:port in
guest where the USB device is attached to.</p>

<p style="margin-top: 1em">usb-list domain-id <br>
List pass-through usb devices for a domain.</p>

<p style="margin-top: 1em">DEVICE-MODEL CONTROL <br>
qemu-monitor-command domain-id command <br>
Issue a monitor command to the device model of the domain
specified by domain-id. command can be any valid command
qemu understands. This can be e.g. used to add non-standard
<br>
devices or devices with non-standard parameters to a domain.
The output of the command is printed to stdout.</p>

<p style="margin-top: 1em">Warning: This qemu monitor
access is provided for convenience when debugging,
troubleshooting, and experimenting. Its use is not supported
by the Xen Project.</p>

<p style="margin-top: 1em">Specifically, not all
information printed by the qemu monitor will necessarily be
accurate or complete, because in a Xen system qemu does not
have a complete view of the <br>
guest.</p>

<p style="margin-top: 1em">Furthermore, modifying the
guest&rsquo;s setup via the qemu monitor may conflict with
the Xen toolstack&rsquo;s assumptions. Resulting problems
may include, but are not limited to: <br>
guest crashes; toolstack error messages; inability to
migrate the guest; and security vulnerabilities which are
not covered by the Xen Project security response policy.</p>

<p style="margin-top: 1em">EXAMPLE</p>

<p style="margin-top: 1em">Obtain information of USB
devices connected as such via the device model (only!) to a
domain:</p>

<p style="margin-top: 1em">xl qemu-monitor-command vm1
&rsquo;info usb&rsquo; <br>
Device 0.2, Port 5, Speed 480 Mb/s, Product Mass Storage</p>

<p style="margin-top: 1em">TMEM <br>
tmem-list I[&lt;-l&gt;] domain-id <br>
List tmem pools. If -l is specified, also list tmem
stats.</p>

<p style="margin-top: 1em">tmem-freeze domain-id <br>
Freeze tmem pools.</p>

<p style="margin-top: 1em">tmem-thaw domain-id <br>
Thaw tmem pools.</p>

<p style="margin-top: 1em">tmem-set domain-id [OPTIONS]
<br>
Change tmem settings.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-w WEIGHT <br>
Weight (int)</p>

<p style="margin-top: 1em">-p COMPRESS <br>
Compress (int)</p>

<p style="margin-top: 1em">tmem-shared-auth domain-id
[OPTIONS] <br>
De/authenticate shared tmem pool.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-u UUID <br>
Specify uuid (abcdef01-2345-6789-1234-567890abcdef)</p>

<p style="margin-top: 1em">-a AUTH <br>
0=auth,1=deauth</p>

<p style="margin-top: 1em">tmem-freeable <br>
Get information about how much freeable memory (MB) is
in-use by tmem.</p>

<p style="margin-top: 1em">FLASK <br>
FLASK is a security framework that defines a mandatory
access control policy providing fine-grained controls over
Xen domains, allowing the policy writer to define what <br>
interactions between domains, devices, and the hypervisor
are permitted. Some example of what you can do using
XSM/FLASK: <br>
- Prevent two domains from communicating via event channels
or grants <br>
- Control which domains can use device passthrough (and
which devices) <br>
- Restrict or audit operations performed by privileged
domains <br>
- Prevent a privileged domain from arbitrarily mapping pages
from other <br>
domains.</p>

<p style="margin-top: 1em">You can find more details on how
to use FLASK and an example security policy here:
&lt;http://xenbits.xen.org/docs/unstable/misc/xsm-flask.txt&gt;</p>

<p style="margin-top: 1em">getenforce <br>
Determine if the FLASK security module is loaded and
enforcing its policy.</p>

<p style="margin-top: 1em">setenforce
1|0|Enforcing|Permissive <br>
Enable or disable enforcing of the FLASK access controls.
The default is permissive, but this can be changed to
enforcing by specifying &quot;flask=enforcing&quot; or
&quot;flask=late&quot; on <br>
the hypervisor&rsquo;s command line.</p>

<p style="margin-top: 1em">loadpolicy policy-file <br>
Load FLASK policy from the given policy file. The initial
policy is provided to the hypervisor as a multiboot module;
this command allows runtime updates to the policy. <br>
Loading new security policy will reset runtime changes to
device labels.</p>

<p style="margin-top: 1em">PLATFORM SHARED RESOURCE
MONITORING/CONTROL <br>
Intel Haswell and later server platforms offer shared
resource monitoring and control technologies. The
availability of these technologies and the hardware
capabilities can be <br>
shown with psr-hwinfo.</p>

<p style="margin-top: 1em">See
&lt;http://xenbits.xen.org/docs/unstable/misc/xl-psr.html&gt;
for more information.</p>

<p style="margin-top: 1em">psr-hwinfo [OPTIONS] <br>
Show Platform Shared Resource (PSR) hardware
information.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-m, --cmt <br>
Show Cache Monitoring Technology (CMT) hardware
information.</p>

<p style="margin-top: 1em">-a, --cat <br>
Show Cache Allocation Technology (CAT) hardware
information.</p>

<p style="margin-top: 1em">CACHE MONITORING TECHNOLOGY <br>
Intel Haswell and later server platforms offer monitoring
capability in each logical processor to measure specific
platform shared resource metric, for example, L3 cache <br>
occupancy. In the Xen implementation, the monitoring
granularity is domain level. To monitor a specific domain,
just attach the domain id with the monitoring service. When
the <br>
domain doesn&rsquo;t need to be monitored any more, detach
the domain id from the monitoring service.</p>

<p style="margin-top: 1em">Intel Broadwell and later server
platforms also offer total/local memory bandwidth
monitoring. Xen supports per-domain monitoring for these two
additional monitoring types. Both <br>
memory bandwidth monitoring and L3 cache occupancy
monitoring share the same set of underlying monitoring
service. Once a domain is attached to the monitoring
service, monitoring <br>
data can be shown for any of these monitoring types.</p>

<p style="margin-top: 1em">psr-cmt-attach [domain-id] <br>
attach: Attach the platform shared resource monitoring
service to a domain.</p>

<p style="margin-top: 1em">psr-cmt-detach [domain-id] <br>
detach: Detach the platform shared resource monitoring
service from a domain.</p>

<p style="margin-top: 1em">psr-cmt-show [psr-monitor-type]
[domain-id] <br>
Show monitoring data for a certain domain or all domains.
Current supported monitor types are: <br>
- &quot;cache-occupancy&quot;: showing the L3 cache
occupancy(KB). <br>
- &quot;total-mem-bandwidth&quot;: showing the total memory
bandwidth(KB/s). <br>
- &quot;local-mem-bandwidth&quot;: showing the local memory
bandwidth(KB/s).</p>

<p style="margin-top: 1em">CACHE ALLOCATION TECHNOLOGY <br>
Intel Broadwell and later server platforms offer
capabilities to configure and make use of the Cache
Allocation Technology (CAT) mechanisms, which enable more
cache resources <br>
(i.e. L3 cache) to be made available for high priority
applications. In the Xen implementation, CAT is used to
control cache allocation on VM basis. To enforce cache on a
<br>
specific domain, just set capacity bitmasks (CBM) for the
domain.</p>

<p style="margin-top: 1em">Intel Broadwell and later server
platforms also offer Code/Data Prioritization (CDP) for
cache allocations, which support specifying code or data
cache for applications. CDP is <br>
used on a per VM basis in the Xen implementation. To specify
code or data CBM for the domain, CDP feature must be enabled
and CBM type options need to be specified when setting <br>
CBM, and the type options (code and data) are mutually
exclusive.</p>

<p style="margin-top: 1em">psr-cat-cbm-set [OPTIONS]
domain-id cbm <br>
Set cache capacity bitmasks(CBM) for a domain. For how to
specify cbm please refer to
&lt;http://xenbits.xen.org/docs/unstable/misc/xl-psr.html&gt;.</p>

<p style="margin-top: 1em">OPTIONS</p>

<p style="margin-top: 1em">-s SOCKET, --socket=SOCKET <br>
Specify the socket to process, otherwise all sockets are
processed.</p>

<p style="margin-top: 1em">-c, --code <br>
Set code CBM when CDP is enabled.</p>

<p style="margin-top: 1em">-d, --data <br>
Set data CBM when CDP is enabled.</p>

<p style="margin-top: 1em">psr-cat-show [domain-id] <br>
Show CAT settings for a certain domain or all domains.</p>

<p style="margin-top: 1em">IGNORED FOR COMPATIBILITY WITH
XM <br>
xl is mostly command-line compatible with the old xm utility
used with the old Python xend. For compatibility, the
following options are ignored:</p>

<p style="margin-top: 1em">xl migrate --live</p>

<p style="margin-top: 1em">TO BE DOCUMENTED <br>
We need better documentation for:</p>

<p style="margin-top: 1em">tmem <br>
Transcendent Memory.</p>

<p style="margin-top: 1em">SEE ALSO <br>
The following man pages:</p>

<p style="margin-top: 1em">xl.cfg(5), xlcpupool.cfg(5),
xentop(1)</p>

<p style="margin-top: 1em">And the following documents on
the xen.org website:</p>


<p style="margin-top: 1em">&lt;http://xenbits.xen.org/docs/unstable/misc/xl-network-configuration.html&gt;
&lt;http://xenbits.xen.org/docs/unstable/misc/xl-disk-configuration.txt&gt;
<br>

&lt;http://xenbits.xen.org/docs/unstable/misc/xsm-flask.txt&gt;
&lt;http://xenbits.xen.org/docs/unstable/misc/xl-psr.html&gt;</p>

<p style="margin-top: 1em">For systems that don&rsquo;t
automatically bring CPU online:</p>


<p style="margin-top: 1em">&lt;http://wiki.xen.org/wiki/Paravirt_Linux_CPU_Hotplug&gt;</p>

<p style="margin-top: 1em">BUGS <br>
Send bugs to xen-devel@lists.xen.org, see
http://wiki.xen.org/xenwiki/ReportingBugs on how to send bug
reports.</p>

<p style="margin-top: 1em">4.8.1-6.fc26 2017-08-15
xl(1)</p>
<hr>
</body>
</html>
