<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 16:21:55 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>LIKWID-MPIRUN(1) General Commands Manual
LIKWID-MPIRUN(1)</p>

<p style="margin-top: 1em">NAME <br>
likwid-mpirun - A tool to start and monitor MPI applications
with LIKWID</p>

<p style="margin-top: 1em">SYNOPSIS <br>
likwid-memsweeper [-hvdOm] [-n number_of_processes]
[-hostfile filename] [-nperdomain
number_of_processes_in_domain] [-pin expression] [-omp
omptype] [-mpi mpitype] [-g eventset] <br>
[--]</p>

<p style="margin-top: 1em">DESCRIPTION <br>
likwid-mpirun is a command line application that wraps the
vendor-specific mpirun tool and adds calls to
likwid-perfctr(1) to the execution string. The user-given
application is <br>
ran, measured and the results returned to the staring
node.</p>

<p style="margin-top: 1em">OPTIONS <br>
-h,--help <br>
prints a help message to standard output, then exits</p>

<p style="margin-top: 1em">-v,--version <br>
prints version information to standard output, then
exits</p>

<p style="margin-top: 1em">-d,--debug <br>
prints debug messages to standard output</p>

<p style="margin-top: 1em">-n,-np,--n,--np
&lt;number_of_processes&gt; <br>
specifies how many MPI processes should be started</p>

<p style="margin-top: 1em">-hostfile &lt;filename&gt; <br>
specifies the nodes to schedule the MPI processes on. If not
given, the environment variables PBS_NODEFILE,
LOADL_HOSTFILE and SLURM_HOSTFILE are checked.</p>

<p style="margin-top: 1em">-nperdomain
&lt;number_of_processes_in_domain&gt; <br>
specifies the processes per affinity domain (see likwid-pin
for info about affinity domains)</p>

<p style="margin-top: 1em">-pin &lt;expression&gt; <br>
specifies the pinning for hybrid execution (see likwid-pin
for info about affinity domains)</p>

<p style="margin-top: 1em">-s, --skip &lt;mask&gt; <br>
Specify skip mask as HEX number. For each set bit the
corresponding thread is skipped.</p>

<p style="margin-top: 1em">-omp &lt;omptype&gt; <br>
enables hybrid setup. Likwid tries to determine OpenMP type
automatically. The only possible value are intel and gnu</p>

<p style="margin-top: 1em">-mpi &lt;mpitype&gt; <br>
specifies the MPI implementation that should be used by the
wrapper. Possible values are intelmpi, openmpi and
mvapich2</p>

<p style="margin-top: 1em">-m,--marker <br>
activates the Marker API for the executed MPI processes</p>

<p style="margin-top: 1em">-O prints output in CSV not
ASCII tables</p>

<p style="margin-top: 1em">-- stops parsing arguments for
likwid-mpirun, in order to set options for underlying MPI
implementation after --.</p>

<p style="margin-top: 1em">EXAMPLE <br>
1. For standard application:</p>

<p style="margin-top: 1em">likwid-mpirun -np 32 ./myApp</p>

<p style="margin-top: 1em">Will run 32 MPI processes, each
host is filled with as much processes as written in ppn</p>

<p style="margin-top: 1em">2. With pinning:</p>

<p style="margin-top: 1em">likwid-mpirun -np 32 -nperdomain
S:2 ./myApp</p>

<p style="margin-top: 1em">Will start 32 MPI processes with
2 processes per socket.</p>

<p style="margin-top: 1em">3. For hybrid runs:</p>

<p style="margin-top: 1em">likwid-mpirun -np 32 -pin
M0:0-3_M1:0-3 ./myApp</p>

<p style="margin-top: 1em">Will start 32 MPI processes with
2 processes per node. Threads of the first process are
pinned to the cores 0-3 in NUMA domain 0 (M0). The OpenMP
threads of the second process <br>
are pinned to the first four cores in NUMA domain 1 (M1)</p>

<p style="margin-top: 1em">BUGS <br>
When measuring Uncore events it is not possible to select a
cpu pin expression that covers multiple sockets, e.g.
S0:0-1_S0:2@S1:2. This runs two processes, each running on
two <br>
CPUs. But since the first CPU of the second expression is on
socket 0, which is already handled by S0:0-1, the second MPI
process gets a event set that does not contain Uncore <br>
counters although the second part of the second expression
would measure the Uncore counters on socket 1.</p>

<p style="margin-top: 1em">AUTHOR <br>
Written by Thomas Roehl
&lt;thomas.roehl@googlemail.com&gt;.</p>

<p style="margin-top: 1em">BUGS <br>
Report Bugs on
&lt;https://github.com/RRZE-HPC/likwid/issues&gt;.</p>

<p style="margin-top: 1em">SEE ALSO <br>
likwid-pin(1), likwid-perfctr(1), likwid-powermeter(1)</p>

<p style="margin-top: 1em">likwid-4 22.12.2016
LIKWID-MPIRUN(1)</p>
<hr>
</body>
</html>
