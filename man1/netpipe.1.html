<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8"/>
  <style>
    table.head, table.foot { width: 100%; }
    td.head-rtitle, td.foot-os { text-align: right; }
    td.head-vol { text-align: center; }
    div.Pp { margin: 1ex 0ex; }
  </style>
  <title>netpipe(1)</title>
</head>
<body>
<table class="head">
  <tr>
    <td class="head-ltitle">netpipe(1)</td>
    <td class="head-vol">netpipe</td>
    <td class="head-rtitle">netpipe(1)</td>
  </tr>
</table>
<div class="manual-text">
<h1 class="Sh" title="Sh" id="NAME"><a class="selflink" href="#NAME">NAME</a></h1>
NetPIPE - <i>Net</i><b>work</b> <i>P</i><b>rotocol</b> <i>I</i><b>ndependent</b>
  <i>P</i><b>erformance</b> <i>E</i><b>valuator</b>
<div style="height: 1.00em;">&#x00A0;</div>
<h1 class="Sh" title="Sh" id="SYNOPSIS"><a class="selflink" href="#SYNOPSIS">SYNOPSIS</a></h1>
<b>NPtcp</b> [<b>-h</b><i>&#x00A0;receiver_hostname</i>]
  [<b>-b</b><i>&#x00A0;TCP_buffer_sizes</i>] [options]
<div style="height: 1.00em;">&#x00A0;</div>
<div class="Pp"></div>
mpirun [<b>-machinefile</b><i>&#x00A0;hostlist</i>] -np 2 <b>NPmpi</b> [-a] [-S]
  [-z] [options]
<div style="height: 1.00em;">&#x00A0;</div>
<div class="Pp"></div>
mpirun [<b>-machinefile</b><i>&#x00A0;hostlist</i>] -np 2 <b>NPmpi2</b> [-f]
  [-g] [options]
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<div class="Pp"></div>
<b>NPpvm</b> [options]
<div style="height: 1.00em;">&#x00A0;</div>
See the TESTING sections below for a more complete description of how to run
  NetPIPE in each environment. The OPTIONS section describes the general options
  available for all modules. See the README file from the tar-ball at
  http://www.scl.ameslab.gov/Projects/NetPIPE/ for documentation on the
  InfiniBand, GM, SHMEM, LAPI, and memcpy modules.
<div style="height: 1.00em;">&#x00A0;</div>
<h1 class="Sh" title="Sh" id="DESCRIPTION"><a class="selflink" href="#DESCRIPTION">DESCRIPTION</a></h1>
<b>NetPIPE</b> uses a simple series of ping-pong tests over a range of message
  sizes to provide a complete measure of the performance of a network. It
  bounces messages of increasing size between two processes, whether across a
  network or within an SMP system. Message sizes are chosen at regular
  intervals, and with slight perturbations, to provide a complete evaluation of
  the communication system. Each data point involves many ping-pong tests to
  provide an accurate timing. Latencies are calculated by dividing the round
  trip time in half for small messages ( less than 64 Bytes ).
<div class="Pp"></div>
The communication time for small messages is dominated by the overhead in the
  communication layers, meaning that the transmission is latency bound. For
  larger messages, the communication rate becomes bandwidth limited by some
  component in the communication subsystem (PCI bus, network card link, network
  switch).
<div class="Pp"></div>
These measurements can be done at the message-passing layer (MPI, MPI-2, and
  PVM) or at the native communications layers that that run upon (TCP/IP, GM for
  Myrinet cards, InfiniBand, SHMEM for the Cray T3E systems, and LAPI for IBM SP
  systems). Recent work is being aimed at measuring some internal system
  properties such as the memcpy module that measures the internal memory copy
  rates, or a disk module under development that measures the performance to
  various I/O devices.
<div class="Pp"></div>
Some uses for NetPIPE include:
<div style="margin-left: 5.00ex;">
<div class="Pp"></div>
Comparing the latency and maximum throughput of various network cards.
<div class="Pp"></div>
Comparing the performance between different types of networks.
<div class="Pp"></div>
Looking for inefficiencies in the message-passing layer by comparing it to the
  native communication layer.
<div class="Pp"></div>
Optimizing the message-passing layer and tune OS and driver parameters for
  optimal performance of the communication subsystem.
<div style="height: 1.00em;">&#x00A0;</div>
</div>
<div class="Pp"></div>
<b>NetPIPE</b> is provided with many modules allowing it to interface with a
  wide variety of communication layers. It is fairly easy to write new
  interfaces for other reliable protocols by using the existing modules as
  examples.
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h1 class="Sh" title="Sh" id="TESTING_TCP"><a class="selflink" href="#TESTING_TCP">TESTING
  TCP</a></h1>
NPtcp can now be launched in two ways, by manually starting NPtcp on both
  systems or by using a nplaunch script. To manually start NPtcp, the NetPIPE
  receiver must be started first on the remote system using the command:
<div class="Pp"></div>
NPtcp [options]
<div class="Pp"></div>
then the primary transmitter is started on the local system with the command
<div class="Pp"></div>
NPtcp -h <i>receiver_hostname</i> [options]
<div class="Pp"></div>
Any options used must be the same on both sides. The -P parameter can be used to
  override the default port number. This is helpful when running several streams
  through a router to a single endpoint.
<div style="height: 1.00em;">&#x00A0;</div>
The nplaunch script uses ssh to launch the remote receiver before starting the
  local transmitter. To use rsh, simply change the nplaunch script.
<div class="Pp"></div>
nplaunch NPtcp -h <i>receiver_hostname</i> [options]
<div class="Pp"></div>
The <b>-b</b><i>&#x00A0;TCP_buffer_sizes</i> option sets the TCP socket buffer
  size, which can greatly influence the maximum throughput on some systems. A
  throughput graph that flattens out suddenly may be a sign of the performance
  being limited by the socket buffer sizes.
<div class="Pp"></div>
Several other protocols are testable in the same way as TCP. These include TCP6
  (TCP over IPv6), SCTP and IPX. They are started in the same way but the
  program names are NPtcp6, NPsctp, and NPipx respectively.
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h1 class="Sh" title="Sh" id="TESTING_MPI_and_MPI-2"><a class="selflink" href="#TESTING_MPI_and_MPI-2">TESTING
  MPI and MPI-2</a></h1>
Use of the MPI interface for NetPIPE depends on the MPI implementation being
  used. All will require the number of processes to be specified, usually with a
  <i>-np 2</i> argument. Clusters environments may require a list of the hosts
  being used, either during initialization of MPI (during lamboot for LAM-MPI)
  or when each job is run (using a -machinefile argument for MPICH). For
  LAM-MPI, for example, put the list of hosts in hostlist then boot LAM and run
  NetPIPE using:
<div class="Pp"></div>
lamboot -v -b <i>hostlist</i>
<div class="Pp"></div>
mpirun -np 2 NPmpi [NetPIPE options]
<div class="Pp"></div>
For MPICH use a command like:
<div class="Pp"></div>
mpirun -machinefile <i>hostlist</i> -np 2 NPmpi [NetPIPE options]
<div class="Pp"></div>
To test the 1-sided communications of the MPI-2 standard, compile using:
<div class="Pp"></div>
<b>make mpi2</b>
<div class="Pp"></div>
Running as described above and MPI will use 1-sided MPI_Put() calls in both
  directions, with each receiver blocking until the last byte has been
  overwritten before bouncing the message back. Use the <i>-f</i> option to
  force usage of a fence to block rather than an overwrite of the last byte. The
  <i>-g</i> option will use MP_Get() functions to transfer the data rather than
  MP_Put().
<div style="height: 1.00em;">&#x00A0;</div>
<div style="height: 1.00em;">&#x00A0;</div>
<h1 class="Sh" title="Sh" id="TESTING_PVM"><a class="selflink" href="#TESTING_PVM">TESTING
  PVM</a></h1>
Start the pvm system using:
<div class="Pp"></div>
pvm
<div class="Pp"></div>
and adding a second machine with the PVM command
<div class="Pp"></div>
add <i>receiver_hostname</i>
<div class="Pp"></div>
Exit the PVM command line interface using quit, then run the PVM NetPIPE
  receiver on one system with the command:
<div class="Pp"></div>
NPpvm [options]
<div class="Pp"></div>
and run the TCP NetPIPE transmitter on the other system with the command:
<div class="Pp"></div>
NPpvm -h <i>receiver hostname</i> [options]
<div class="Pp"></div>
Any options used must be the same on both sides. The nplaunch script may also be
  used with NPpvm as described above for NPtcp.
<div style="height: 1.00em;">&#x00A0;</div>
<h1 class="Sh" title="Sh" id="TESTING_METHODOLOGY"><a class="selflink" href="#TESTING_METHODOLOGY">TESTING
  METHODOLOGY</a></h1>
<b>NetPIPE</b> tests network performance by sending a number of messages at each
  block size, starting from the lower bound on the message sizes.
<div style="height: 1.00em;">&#x00A0;</div>
The message size is incremented until the upper bound on the message size is
  reached or the time to transmit a block exceeds one second, which ever occurs
  first. Message sizes are chosen at regular intervals, and for slight
  perturbations from them to provide a more complete evaluation of the
  communication subsystem.
<div class="Pp"></div>
The <b>NetPIPE</b> output file may be graphed using a program such as
  <b>gnuplot(1).</b> The output file contains three columns: the number of bytes
  in the block, the transfer rate in bits per second, and the time to transfer
  the block (half the round-trip time). The first two columns are normally used
  to graph the throughput vs block size, while the third column provides the
  latency. For example, the <b>throughput versus block size</b> graph can be
  created by graphing bytes versus bits per second. Sample <b>gnuplot(1)</b>
  commands for such a graph would be
<div class="Pp"></div>
set logscale x
<div class="Pp"></div>
plot &quot;np.out&quot;
<div style="height: 1.00em;">&#x00A0;</div>
<h1 class="Sh" title="Sh" id="OPTIONS"><a class="selflink" href="#OPTIONS">OPTIONS</a></h1>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-a</b></dt>
  <dd class="It-tag">asynchronous mode: prepost receives (MPI, IB modules)</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-b</b><i>&#x00A0;<i>TCP_buffer_sizes</i></i></dt>
  <dd class="It-tag">Set the send and receive TCP buffer sizes (TCP module
      only).
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-B</b></dt>
  <dd class="It-tag">Burst mode where all receives are preposted at once (MPI,
      IB modules).
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-f</b></dt>
  <dd class="It-tag">Use a fence to block for completion (MPI2 module only).
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-g</b></dt>
  <dd class="It-tag">Use MPI_Get() instead of MPI_Put() (MPI2 module only).
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-h</b><i>&#x00A0;<i>hostname</i></i></dt>
  <dd class="It-tag">Specify the name of the receiver host to connect to (TCP,
      PVM, IB, GM).
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-I</b></dt>
  <dd class="It-tag">Invalidate cache to measure performance without cache
      effects (mostly affects IB and memcpy modules).
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-i</b></dt>
  <dd class="It-tag">Do an integrity check instead of a performance evaluation.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-l</b><i>&#x00A0;<i>starting_msg_size</i></i></dt>
  <dd class="It-tag">Specify the lower bound for the size of messages to be
      tested.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-n</b><i>&#x00A0;<i>nrepeats</i></i></dt>
  <dd class="It-tag">Set the number of repeats for each test to a constant.
      Otherwise, the number of repeats is chosen to provide an accurate timing
      for each test. Be very careful if specifying a low number so that the time
      for the ping-pong test exceeds the timer accuracy.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-O</b><i>&#x00A0;<i>source_offset,dest_offset</i></i></dt>
  <dd class="It-tag">Specify the source and destination offsets of the buffers
      from perfect page alignment.</dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-o</b><i>&#x00A0;<i>output_filename</i></i></dt>
  <dd class="It-tag">Specify the output filename (default is np.out).
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-p</b><i>&#x00A0;<i>perturbation_size</i></i></dt>
  <dd class="It-tag">NetPIPE chooses the message sizes at regular intervals,
      increasing them exponentially from the lower boundary to the upper
      boundary. At each point, it also tests perturbations of 3 bytes above and
      3 bytes below each test point to find idiosyncrasies in the system. This
      perturbation value can be changed using the <i>-p</i> option, or turned
      off using <i>-p</i> <i>0</i> <b>.</b>
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-r</b></dt>
  <dd class="It-tag">This option resets the TCP sockets after every test (TCP
      module only). It is necessary for some streaming tests to get good
      measurements since the socket window size may otherwise collapse.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-s</b></dt>
  <dd class="It-tag">Set streaming mode where data is only transmitted in one
      direction.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-S</b></dt>
  <dd class="It-tag">Use synchronous sends (MPI module only).
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-u</b><i>&#x00A0;<i>upper_bound</i></i></dt>
  <dd class="It-tag">Specify the upper boundary to the size of message being
      tested. By default, NetPIPE will stop when the time to transmit a block
      exceeds one second.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-z</b></dt>
  <dd class="It-tag">Receive messages using MPI_ANY_SOURCE (MPI module only)
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<dl class="Bl-tag">
  <dt class="It-tag"><b>-2</b></dt>
  <dd class="It-tag">Set bi-directional mode where both sides send and receive
      at the same time (supported by most modules). You may need to use
      <i>-a</i> to choose asynchronous communications for MPI to avoid
      freeze-ups. For TCP, the maximum test size will be limited by the TCP
      buffer sizes.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<h1 class="Sh" title="Sh" id="FILES"><a class="selflink" href="#FILES">FILES</a></h1>
<dl class="Bl-tag">
  <dt class="It-tag"><i>np.out</i></dt>
  <dd class="It-tag">Default output file for <b>NetPIPE</b>. Overridden by the
      <b>-o</b> option.
    <div style="height: 1.00em;">&#x00A0;</div>
  </dd>
</dl>
<h1 class="Sh" title="Sh" id="AUTHOR"><a class="selflink" href="#AUTHOR">AUTHOR</a></h1>
The original NetPIPE core plus TCP and MPI modules were written by Quinn Snell,
  Armin Mikler, Guy Helmer, and John Gustafson. NetPIPE is currently being
  developed and maintained by Dave Turner with contributions from many students
  (Bogdan Vasiliu, Adam Oline, Xuehua Chen, and Brian Smith).
<div style="height: 1.00em;">&#x00A0;</div>
<div class="Pp"></div>
Send comments/bug-reports to: <i>&lt;netpipe@scl.ameslab.gov&gt;.</i>
<div class="Pp"></div>
Additional information about <b>NetPIPE</b> can be found on the World Wide Web
  at <i>http://www.scl.ameslab.gov/Projects/NetPIPE/</i>
<div style="height: 1.00em;">&#x00A0;</div>
<h1 class="Sh" title="Sh" id="BUGS"><a class="selflink" href="#BUGS">BUGS</a></h1>
As of version 3.6.1, there is a bug that causes NetPIPE to segfault on RedHat
  Enterprise systems. I will debug this as soon as I get access to a few such
  systems. -Dave Turner (turner@ameslab.gov)</div>
<table class="foot">
  <tr>
    <td class="foot-date">June 1, 2004</td>
    <td class="foot-os">NetPIPE</td>
  </tr>
</table>
</body>
</html>
