<!-- Creator     : groff version 1.22.3 -->
<!-- CreationDate: Sun Aug 27 16:27:07 2017 -->
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta name="generator" content="groff -Thtml, see www.gnu.org">
<meta http-equiv="Content-Type" content="text/html; charset=US-ASCII">
<meta name="Content-Style" content="text/css">
<style type="text/css">
       p       { margin-top: 0; margin-bottom: 0; vertical-align: top }
       pre     { margin-top: 0; margin-bottom: 0; vertical-align: top }
       table   { margin-top: 0; margin-bottom: 0; vertical-align: top }
       h1      { text-align: center }
</style>
<title></title>
</head>
<body>

<hr>


<p>netpipe(1) netpipe netpipe(1)</p>

<p style="margin-top: 1em">NAME <br>
NetPIPE - Network Protocol Independent Performance
Evaluator</p>

<p style="margin-top: 1em">SYNOPSIS <br>
NPtcp [-h receiver_hostname] [-b TCP_buffer_sizes]
[options]</p>

<p style="margin-top: 1em">mpirun [-machinefile hostlist]
-np 2 NPmpi [-a] [-S] [-z] [options]</p>

<p style="margin-top: 1em">mpirun [-machinefile hostlist]
-np 2 NPmpi2 [-f] [-g] [options]</p>

<p style="margin-top: 1em">NPpvm [options]</p>

<p style="margin-top: 1em">See the TESTING sections below
for a more complete description of how to run NetPIPE in
each environment. The OPTIONS section describes the general
options available for all <br>
modules. See the README file from the tar-ball at
http://www.scl.ameslab.gov/Projects/NetPIPE/ for
documentation on the InfiniBand, GM, SHMEM, LAPI, and memcpy
modules.</p>

<p style="margin-top: 1em">DESCRIPTION <br>
NetPIPE uses a simple series of ping-pong tests over a range
of message sizes to provide a complete measure of the
performance of a network. It bounces messages of increasing
<br>
size between two processes, whether across a network or
within an SMP system. Message sizes are chosen at regular
intervals, and with slight perturbations, to provide a
complete <br>
evaluation of the communication system. Each data point
involves many ping-pong tests to provide an accurate timing.
Latencies are calculated by dividing the round trip time in
<br>
half for small messages ( less than 64 Bytes ).</p>

<p style="margin-top: 1em">The communication time for small
messages is dominated by the overhead in the communication
layers, meaning that the transmission is latency bound. For
larger messages, the com&acirc; <br>
munication rate becomes bandwidth limited by some component
in the communication subsystem (PCI bus, network card link,
network switch).</p>

<p style="margin-top: 1em">These measurements can be done
at the message-passing layer (MPI, MPI-2, and PVM) or at the
native communications layers that that run upon (TCP/IP, GM
for Myrinet cards, Infini&acirc; <br>
Band, SHMEM for the Cray T3E systems, and LAPI for IBM SP
systems). Recent work is being aimed at measuring some
internal system properties such as the memcpy module that
mea&acirc; <br>
sures the internal memory copy rates, or a disk module under
development that measures the performance to various I/O
devices.</p>

<p style="margin-top: 1em">Some uses for NetPIPE
include:</p>

<p style="margin-top: 1em">Comparing the latency and
maximum throughput of various network cards.</p>

<p style="margin-top: 1em">Comparing the performance
between different types of networks.</p>

<p style="margin-top: 1em">Looking for inefficiencies in
the message-passing layer by comparing it to the native
communication layer.</p>

<p style="margin-top: 1em">Optimizing the message-passing
layer and tune OS and driver parameters for optimal
performance of the communication subsystem.</p>

<p style="margin-top: 1em">NetPIPE is provided with many
modules allowing it to interface with a wide variety of
communication layers. It is fairly easy to write new
interfaces for other reliable proto&acirc; <br>
cols by using the existing modules as examples.</p>

<p style="margin-top: 1em">TESTING TCP <br>
NPtcp can now be launched in two ways, by manually starting
NPtcp on both systems or by using a nplaunch script. To
manually start NPtcp, the NetPIPE receiver must be started
<br>
first on the remote system using the command:</p>

<p style="margin-top: 1em">NPtcp [options]</p>

<p style="margin-top: 1em">then the primary transmitter is
started on the local system with the command</p>

<p style="margin-top: 1em">NPtcp -h receiver_hostname
[options]</p>

<p style="margin-top: 1em">Any options used must be the
same on both sides. The -P parameter can be used to override
the default port number. This is helpful when running
several streams through a router <br>
to a single endpoint.</p>

<p style="margin-top: 1em">The nplaunch script uses ssh to
launch the remote receiver before starting the local
transmitter. To use rsh, simply change the nplaunch
script.</p>

<p style="margin-top: 1em">nplaunch NPtcp -h
receiver_hostname [options]</p>

<p style="margin-top: 1em">The -b TCP_buffer_sizes option
sets the TCP socket buffer size, which can greatly influence
the maximum throughput on some systems. A throughput graph
that flattens out suddenly <br>
may be a sign of the performance being limited by the socket
buffer sizes.</p>

<p style="margin-top: 1em">Several other protocols are
testable in the same way as TCP. These include TCP6 (TCP
over IPv6), SCTP and IPX. They are started in the same way
but the program names are <br>
NPtcp6, NPsctp, and NPipx respectively.</p>

<p style="margin-top: 1em">TESTING MPI and MPI-2 <br>
Use of the MPI interface for NetPIPE depends on the MPI
implementation being used. All will require the number of
processes to be specified, usually with a -np 2 argument.
<br>
Clusters environments may require a list of the hosts being
used, either during initialization of MPI (during lamboot
for LAM-MPI) or when each job is run (using a -machinefile
<br>
argument for MPICH). For LAM-MPI, for example, put the list
of hosts in hostlist then boot LAM and run NetPIPE
using:</p>

<p style="margin-top: 1em">lamboot -v -b hostlist</p>

<p style="margin-top: 1em">mpirun -np 2 NPmpi [NetPIPE
options]</p>

<p style="margin-top: 1em">For MPICH use a command
like:</p>

<p style="margin-top: 1em">mpirun -machinefile hostlist -np
2 NPmpi [NetPIPE options]</p>

<p style="margin-top: 1em">To test the 1-sided
communications of the MPI-2 standard, compile using:</p>

<p style="margin-top: 1em">make mpi2</p>

<p style="margin-top: 1em">Running as described above and
MPI will use 1-sided MPI_Put() calls in both directions,
with each receiver blocking until the last byte has been
overwritten before bouncing the <br>
message back. Use the -f option to force usage of a fence to
block rather than an overwrite of the last byte. The -g
option will use MP_Get() functions to transfer the data <br>
rather than MP_Put().</p>

<p style="margin-top: 1em">TESTING PVM <br>
Start the pvm system using:</p>

<p style="margin-top: 1em">pvm</p>

<p style="margin-top: 1em">and adding a second machine with
the PVM command</p>

<p style="margin-top: 1em">add receiver_hostname</p>

<p style="margin-top: 1em">Exit the PVM command line
interface using quit, then run the PVM NetPIPE receiver on
one system with the command:</p>

<p style="margin-top: 1em">NPpvm [options]</p>

<p style="margin-top: 1em">and run the TCP NetPIPE
transmitter on the other system with the command:</p>

<p style="margin-top: 1em">NPpvm -h receiver hostname
[options]</p>

<p style="margin-top: 1em">Any options used must be the
same on both sides. The nplaunch script may also be used
with NPpvm as described above for NPtcp.</p>

<p style="margin-top: 1em">TESTING METHODOLOGY <br>
NetPIPE tests network performance by sending a number of
messages at each block size, starting from the lower bound
on the message sizes.</p>

<p style="margin-top: 1em">The message size is incremented
until the upper bound on the message size is reached or the
time to transmit a block exceeds one second, which ever
occurs first. Message sizes <br>
are chosen at regular intervals, and for slight
perturbations from them to provide a more complete
evaluation of the communication subsystem.</p>

<p style="margin-top: 1em">The NetPIPE output file may be
graphed using a program such as gnuplot(1). The output file
contains three columns: the number of bytes in the block,
the transfer rate in bits <br>
per second, and the time to transfer the block (half the
round-trip time). The first two columns are normally used to
graph the throughput vs block size, while the third column
<br>
provides the latency. For example, the throughput versus
block size graph can be created by graphing bytes versus
bits per second. Sample gnuplot(1) commands for such a graph
<br>
would be</p>

<p style="margin-top: 1em">set logscale x</p>

<p style="margin-top: 1em">plot &quot;np.out&quot;</p>

<p style="margin-top: 1em">OPTIONS <br>
-a asynchronous mode: prepost receives (MPI, IB modules)</p>

<p style="margin-top: 1em">-b TCP_buffer_sizes <br>
Set the send and receive TCP buffer sizes (TCP module
only).</p>

<p style="margin-top: 1em">-B Burst mode where all receives
are preposted at once (MPI, IB modules).</p>

<p style="margin-top: 1em">-f Use a fence to block for
completion (MPI2 module only).</p>

<p style="margin-top: 1em">-g Use MPI_Get() instead of
MPI_Put() (MPI2 module only).</p>

<p style="margin-top: 1em">-h hostname <br>
Specify the name of the receiver host to connect to (TCP,
PVM, IB, GM).</p>

<p style="margin-top: 1em">-I Invalidate cache to measure
performance without cache effects (mostly affects IB and
memcpy modules).</p>

<p style="margin-top: 1em">-i Do an integrity check instead
of a performance evaluation.</p>

<p style="margin-top: 1em">-l starting_msg_size <br>
Specify the lower bound for the size of messages to be
tested.</p>

<p style="margin-top: 1em">-n nrepeats <br>
Set the number of repeats for each test to a constant.
Otherwise, the number of repeats is chosen to provide an
accurate timing for each test. Be very careful if
speci&acirc; <br>
fying a low number so that the time for the ping-pong test
exceeds the timer accuracy.</p>

<p style="margin-top: 1em">-O source_offset,dest_offset
<br>
Specify the source and destination offsets of the buffers
from perfect page alignment.</p>

<p style="margin-top: 1em">-o output_filename <br>
Specify the output filename (default is np.out).</p>

<p style="margin-top: 1em">-p perturbation_size <br>
NetPIPE chooses the message sizes at regular intervals,
increasing them exponentially from the lower boundary to the
upper boundary. At each point, it also tests pertur&acirc;
<br>
bations of 3 bytes above and 3 bytes below each test point
to find idiosyncrasies in the system. This perturbation
value can be changed using the -p option, or turned off <br>
using -p 0 .</p>

<p style="margin-top: 1em">-r This option resets the TCP
sockets after every test (TCP module only). It is necessary
for some streaming tests to get good measurements since the
socket window size may <br>
otherwise collapse.</p>

<p style="margin-top: 1em">-s Set streaming mode where data
is only transmitted in one direction.</p>

<p style="margin-top: 1em">-S Use synchronous sends (MPI
module only).</p>

<p style="margin-top: 1em">-u upper_bound <br>
Specify the upper boundary to the size of message being
tested. By default, NetPIPE will stop when the time to
transmit a block exceeds one second.</p>

<p style="margin-top: 1em">-z Receive messages using
MPI_ANY_SOURCE (MPI module only)</p>

<p style="margin-top: 1em">-2 Set bi-directional mode where
both sides send and receive at the same time (supported by
most modules). You may need to use -a to choose asynchronous
communications for <br>
MPI to avoid freeze-ups. For TCP, the maximum test size will
be limited by the TCP buffer sizes.</p>

<p style="margin-top: 1em">FILES <br>
np.out Default output file for NetPIPE. Overridden by the -o
option.</p>

<p style="margin-top: 1em">AUTHOR <br>
The original NetPIPE core plus TCP and MPI modules were
written by Quinn Snell, Armin Mikler, Guy Helmer, and John
Gustafson. NetPIPE is currently being developed and
maintained <br>
by Dave Turner with contributions from many students (Bogdan
Vasiliu, Adam Oline, Xuehua Chen, and Brian Smith).</p>

<p style="margin-top: 1em">Send comments/bug-reports to:
&lt;netpipe@scl.ameslab.gov&gt;.</p>

<p style="margin-top: 1em">Additional information about
NetPIPE can be found on the World Wide Web at
http://www.scl.ameslab.gov/Projects/NetPIPE/</p>

<p style="margin-top: 1em">BUGS <br>
As of version 3.6.1, there is a bug that causes NetPIPE to
segfault on RedHat Enterprise systems. I will debug this as
soon as I get access to a few such systems. -Dave Turner
<br>
(turner@ameslab.gov)</p>

<p style="margin-top: 1em">NetPIPE June 1, 2004
netpipe(1)</p>
<hr>
</body>
</html>
